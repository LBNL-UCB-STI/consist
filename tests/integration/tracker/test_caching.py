import pytest
import pandas as pd
import logging


@pytest.fixture
def dummy_input(tmp_path):
    """Creates a dummy input file and returns its path."""
    f = tmp_path / "source_data.csv"
    f.write_text("id,val\n1,100")
    return f


def test_caching_and_forking(tracker, dummy_input):
    """
    Tests Consist's core caching (cache hit) and run forking (lineage linking) mechanisms.

    This test verifies that Consist correctly identifies identical runs as cache hits
    and, when a run consumes an output from a previous run, automatically establishes
    a lineage link to that parent run. This is fundamental for reproducibility and efficiency.

    What happens:
    1. A `Tracker` and a dummy `input_file` are set up using the `tracker_setup` fixture.
    2. **Run A (Base Run)**: A `tracker.start_run` context ("run_A") is initiated with
       a specific configuration and the `input_file`. An output artifact is created
       and logged.
    3. **Run B (Identical Twin)**: A second `tracker.start_run` context ("run_B") is
       initiated with *identical* configuration and inputs as Run A.
    4. **Run C (Child/Lineage Test)**: A third `tracker.start_run` context ("run_C")
       is initiated. Its input is the *output artifact* generated by Run A.

    What's checked:
    -   **Run A**: The `config_hash` and `input_hash` are correctly calculated.
    -   **Run B**:
        - `t.current_consist.cached_run` is not `None`, indicating a cache hit.
        - The `cached_run.id` is "run_A", confirming it correctly identified the parent.
        - The `cached_run.status` is "completed".
    -   **Run C**:
        - `current_run.parent_run_id` is "run_A", verifying automatic lineage linking.
        - The input `Artifact` object within Run C correctly has its `run_id` set to "run_A",
          confirming that the provenance of the input was correctly tracked from its source.
    """
    config = {"param": 42}
    input_path = str(dummy_input)

    # 1. Run A: The Base Run
    logging.info("\n--- Starting Run A ---")
    with tracker.start_run(
        "run_A", model="test_model", config=config, inputs=[input_path]
    ) as t:
        # Create an output
        df = pd.DataFrame({"a": [1, 2]})
        out_path = t.run_dir / "run_A_out.parquet"
        df.to_parquet(out_path)
        t.log_artifact(out_path, key="intermediate", direction="output")

    # 2. Run B: The Identical Twin (Cache Test)
    logging.info("\n--- Starting Run B (Expect Cache Hit) ---")
    with tracker.start_run(
        "run_B", model="test_model", config=config, inputs=[input_path]
    ) as t:
        cached = t.current_consist.cached_run
        assert cached is not None, "Run B should have found Run A in cache"
        assert cached.id == "run_A"
        assert cached.status == "completed"

    # 3. Run C: The Child (Lineage/Forking Test)
    logging.info("\n--- Starting Run C (Expect Lineage Link) ---")
    prev_output = str(tracker.run_dir / "run_A_out.parquet")

    with tracker.start_run(
        "run_C", model="downstream_model", inputs=[prev_output]
    ) as t:
        current_run = t.current_consist.run
        assert current_run.parent_run_id == "run_A"

        # Verify input artifact lineage
        inp_artifact = t.current_consist.inputs[0]
        assert inp_artifact.run_id == "run_A"


def test_cache_overwrite_mode(tracker, dummy_input):
    """
    Tests the `cache_mode="overwrite"` functionality of `tracker.start_run`.

    This test verifies that when `cache_mode` is set to "overwrite", Consist
    will always execute the run (ignoring any existing cache entries for the
    same signature) and will update the cache with the results of the current run.
    It ensures that new results can forcibly replace older cached ones.

    What happens:
    1. A `Tracker` and a dummy `input_file` are set up.
    2. **Run A**: An initial run ("run_A_overwrite") is executed and completes,
       populating the cache with its results.
    3. **Run B**: An identical run ("run_B_overwrite") is executed, but with
       `cache_mode="overwrite"`.
    4. **Run C**: A third identical run ("run_C_overwrite") is executed with
       the default `cache_mode="reuse"`.

    What's checked:
    - After Run B (overwrite mode), `t.is_cached` is `False`, confirming that
      the cache was not used, and the run was executed.
    - After Run C (reuse mode), `t.is_cached` is `True`, confirming a cache hit.
    - Importantly, the `cached_run.id` for Run C is "run_B_overwrite", proving that
      Run B successfully updated the cache and replaced the entry from Run A.
    """
    config = {"param": 100}
    input_path = str(dummy_input)

    # 1. Run A: Populate cache
    with tracker.start_run(
        "run_A_overwrite", "overwrite_model", config=config, inputs=[input_path]
    ):
        pass

        # 2. Run B: Overwrite the cache
    logging.info("--- Starting Run B (Overwrite Mode) ---")
    with tracker.start_run(
        "run_B_overwrite",
        "overwrite_model",
        config=config,
        inputs=[input_path],
        cache_mode="overwrite",
    ) as t:
        assert not t.is_cached, "Run B in overwrite mode should not be cached."

    # 3. Run C: Check which run is now cached
    logging.info("--- Starting Run C (Expect Cache Hit on Run B) ---")
    with tracker.start_run(
        "run_C_overwrite",
        "overwrite_model",
        config=config,
        inputs=[input_path],
        cache_mode="reuse",
    ) as t:
        assert t.is_cached, "Run C should have a cache hit."
        cached_run = t.current_consist.cached_run
        assert cached_run.id == "run_B_overwrite"


def test_log_artifact_on_cache_hit_returns_cached_output(tracker, dummy_input):
    """
    On cache hits, step bodies still execute. `log_artifact(...)` should therefore be
    safe to call in cache-agnostic code:

    - If the requested output key exists in the hydrated cached outputs, return it.
    - If the key does not exist, raise (to avoid creating confusing "new" outputs on a
      run that did not execute).
    """
    config = {"param": 7}
    input_path = str(dummy_input)

    with tracker.start_run(
        "run_A_log_artifact_cache",
        model="test_model",
        config=config,
        inputs=[input_path],
    ) as t:
        out_path = t.run_dir / "out.parquet"
        pd.DataFrame({"a": [1]}).to_parquet(out_path)
        produced = t.log_artifact(out_path, key="out", direction="output")

    with tracker.start_run(
        "run_B_log_artifact_cache",
        model="test_model",
        config=config,
        inputs=[input_path],
    ) as t:
        assert t.is_cached
        assert len(t.current_consist.outputs) == 1

        returned = t.log_artifact(
            "does-not-matter.parquet", key="out", direction="output"
        )
        assert returned.id == produced.id
        assert len(t.current_consist.outputs) == 1

        # If code attempts to produce a new output on a cache hit, Consist demotes the
        # cache hit to an executing run so provenance remains truthful.
        new_out = t.log_artifact("new.parquet", key="new", direction="output")
        assert new_out.key == "new"
        assert not t.is_cached


def test_log_artifact_on_cache_hit_with_content_hash_returns_cached_output(
    tracker, dummy_input
):
    config = {"param": 7}
    input_path = str(dummy_input)

    with tracker.start_run(
        "run_A_log_artifact_cache_hash",
        model="test_model",
        config=config,
        inputs=[input_path],
    ) as t:
        out_path = t.run_dir / "out.parquet"
        pd.DataFrame({"a": [1]}).to_parquet(out_path)
        produced = t.log_artifact(out_path, key="out", direction="output")

    with tracker.start_run(
        "run_B_log_artifact_cache_hash",
        model="test_model",
        config=config,
        inputs=[input_path],
    ) as t:
        assert t.is_cached
        returned = t.log_artifact(
            "does-not-matter.parquet",
            key="out",
            direction="output",
            content_hash="override_hash",
        )
        assert returned.id == produced.id
        assert t.is_cached


def test_log_input_reuses_existing_artifact_with_force_override(tracker, dummy_input):
    input_path = str(dummy_input)

    with tracker.start_run(
        "run_A_input_override",
        model="test_model",
        inputs=[input_path],
    ) as t:
        out_path = t.run_dir / "out.parquet"
        pd.DataFrame({"a": [1]}).to_parquet(out_path)
        produced = t.log_artifact(out_path, key="out", direction="output")

    with tracker.start_run(
        "run_B_input_override",
        model="test_model",
        inputs=[input_path],
    ) as t:
        reused = t.log_artifact(
            out_path,
            key="out",
            direction="input",
            content_hash="override_hash",
            force_hash_override=True,
        )

    assert reused.id == produced.id
    assert reused.run_id == produced.run_id
    assert reused.hash == "override_hash"


def test_log_input_override_ignored_when_hash_differs(tracker, dummy_input, caplog):
    input_path = str(dummy_input)

    with tracker.start_run(
        "run_A_input_override_guard",
        model="test_model",
        inputs=[input_path],
    ) as t:
        out_path = t.run_dir / "out.parquet"
        pd.DataFrame({"a": [1]}).to_parquet(out_path)
        produced = t.log_artifact(out_path, key="out", direction="output")

    caplog.set_level(logging.WARNING)
    with tracker.start_run(
        "run_B_input_override_guard",
        model="test_model",
        inputs=[input_path],
    ) as t:
        reused = t.log_artifact(
            out_path,
            key="out",
            direction="input",
            content_hash="override_hash",
        )

    assert reused.id == produced.id
    assert reused.hash == produced.hash
    assert any(
        "Ignoring content_hash override" in record.message for record in caplog.records
    )
