{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"","text":"<p>   Track provenance. Skip redundant computation. Query results across runs. </p>","path":[""],"tags":[]},{"location":"#what-is-consist","level":2,"title":"What is Consist?","text":"<p>Consist is a Python library for automatic provenance tracking and intelligent caching in scientific simulation workflows.</p> <p>Problem: Simulation pipelines are expensive (often 24-48 hour runs), produce massive files, and require exact reproducibility. When you re-run with the same inputs, you waste days recomputing identical results. Without provenance tracking, you can't answer \"which configuration produced this forecast?\" or \"did Figure 3 come from v1.2 or v1.3?\"</p> <p>Solution: Consist automatically:</p> <ul> <li>Tracks provenance: what code version, configuration, and inputs produced each result</li> <li>Detects redundant computation: reuses results when code + config + inputs match (cache hits)</li> <li>Enables SQL queries across runs: ingest results into DuckDB for analytics without loading everything into memory</li> <li>Keeps workflows portable: run the same pipeline on different machines and it \"just works\"</li> </ul>","path":[""],"tags":[]},{"location":"#getting-started","level":2,"title":"Getting Started","text":"<p>Choose your path:</p> <p>New to provenance tracking?</p> <ol> <li>Install &amp; Quickstart — 5 minutes</li> <li>Core Concepts — Mental model</li> <li>First Workflow — Multi-step pipeline</li> <li>Examples — Runnable notebooks</li> </ol> <p>Integrating Consist into a tool (ActivitySim, MATSim, BEAM)?</p> <ol> <li>Core Concepts</li> <li>Architecture — System design</li> <li>Config Adapters — Write an adapter</li> <li>Containers — If tracking Docker/Singularity</li> </ol> <p>Optimizing performance or debugging cache issues?</p> <ol> <li>Caching &amp; Hydration</li> <li>Troubleshooting</li> <li>CLI Reference</li> </ol>","path":[""],"tags":[]},{"location":"#by-user-type","level":2,"title":"By User Type","text":"<ul> <li> <p>Simulation developers</p> <p>Building or maintaining tools like ActivitySim, MATSim, BEAM?</p> <p>→ Core Concepts | Architecture | Config Adapters</p> </li> <li> <p>Practitioners &amp; MPO staff</p> <p>Prefer command-line over Python?</p> <p>→ CLI Reference — query and compare results without code</p> </li> <li> <p>Researchers</p> <p>Building reproducible workflows for publication?</p> <p>→ Core Concepts | Data Materialization | Mounts &amp; Portability</p> </li> </ul>","path":[""],"tags":[]},{"location":"#by-topic","level":2,"title":"By Topic","text":"","path":[""],"tags":[]},{"location":"#beginner","level":3,"title":"Beginner","text":"<ul> <li>Core Concepts – Mental models and core ideas</li> <li>Quickstart – 5-minute first run</li> <li>First Workflow – Build a multi-step pipeline</li> <li>Example Notebooks – Runnable examples with explanations</li> </ul>","path":[""],"tags":[]},{"location":"#intermediate","level":3,"title":"Intermediate","text":"<ul> <li>Config Management – Config hashing and queryable metadata (facets)</li> <li>Caching &amp; Hydration – Caching patterns and data recovery</li> <li>Data Materialization Strategy – When to ingest data and use hybrid views</li> <li>DLT Loader Integration – Schema-validated data ingestion</li> <li>Mounts &amp; Portability – Reproducible workflows across machines</li> </ul>","path":[""],"tags":[]},{"location":"#advanced","level":3,"title":"Advanced","text":"<ul> <li>Architecture – System design and implementation details</li> <li>Container Integration – Docker/Singularity support</li> <li>Config Adapters – Building adapters for external tools</li> <li>CLI Reference – All command-line tools</li> <li>Troubleshooting – Common issues and solutions</li> <li>FAQ – Frequently asked questions</li> <li>API Reference – Complete function and class documentation</li> </ul>","path":[""],"tags":[]},{"location":"#common-tasks","level":2,"title":"Common Tasks","text":"I want to... Go to Speed up my pipeline Caching &amp; Hydration Debug a cache miss Troubleshooting Find which config produced a result <code>consist lineage</code> Compare results across scenarios Data Materialization Ingest data for SQL analysis Data Materialization Understand config vs. facets Config Management Share a reproducible study Mounts &amp; Portability Integrate with ActivitySim/BEAM/MATSim Config Adapters or Containers","path":[""],"tags":[]},{"location":"#built-on-open-standards","level":2,"title":"Built on Open Standards","text":"<p>Consist relies on modern, high-performance data engineering tools:</p> <ul> <li>DuckDB: The \"SQLite for Analytics\" powers our lightning-fast provenance queries and data virtualization.</li> <li>SQLModel: Combines SQLAlchemy and Pydantic for robust, type-safe data modeling and schema validation.</li> <li>DLT (Data Load Tool): Handles robust, schema-aware data ingestion from diverse sources into your provenance database.</li> <li>Apache Parquet &amp; Zarr: Industry-standard formats for efficient, compressed storage of tabular and multi-dimensional scientific data.</li> </ul>","path":[""],"tags":[]},{"location":"#learn-more","level":2,"title":"Learn More","text":"<p>See Core Concepts for a complete mental model, or Glossary for quick term definitions.</p>","path":[""],"tags":[]},{"location":"architecture/","level":1,"title":"Architecture","text":"","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#how-caching-works","level":2,"title":"How Caching Works","text":"<p>Consist identifies runs using a three-part signature:</p> <pre><code>signature = SHA256(code_hash || config_hash || input_hash)\n</code></pre> Component Source Notes Code hash Git commit SHA Appends <code>-dirty-&lt;hash&gt;</code> if tracked files are modified; falls back to <code>unknown_code_version</code> without Git Config hash Canonical JSON of config dict Normalized for key order and numeric types; Pydantic models serialize deterministically Input hash SHA256 of input content For Consist artifacts, uses the producing run's signature (Merkle linking); for raw files, hashes bytes or metadata per <code>hashing_strategy</code>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#what-changes-break-cache-hits","level":3,"title":"What Changes Break Cache Hits?","text":"What Changed Cache Hit? Why Input file content ❌ No File hash changes → signature changes Config value ❌ No Config hash changes → signature changes Function code ❌ No Code hash changes → signature changes <code>runtime_kwargs</code> ✅ Yes runtime_kwargs are NOT hashed; don't affect signature Output file names ✅ Yes Output names don't affect signature Comments in code Depends Committed comment changes affect the code hash; uncommitted changes mark the repo dirty and break cache. <p>Merkle DAG structure: Each run's signature incorporates the signatures of its input artifacts' producing runs. This forms a directed acyclic graph where:</p> <ul> <li>Changing a parameter invalidates only downstream runs that depend on it</li> <li>Identical inputs produce cache hits across machines (given the same code version)</li> <li>Provenance validity depends on the lineage graph, not file existence</li> </ul> <p>For detailed terminology, see Core Concepts.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#cache-modes","level":3,"title":"Cache Modes","text":"Mode Behavior <code>reuse</code> (default) Return cached result if signature matches <code>overwrite</code> Always execute, update cache with new result <code>readonly</code> Use cache but don't persist new results (sandbox mode)","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#ghost-mode","level":3,"title":"Ghost Mode","text":"<p>Consist enables \"Ghost Mode\" — the ability to delete intermediate files while preserving provenance and recoverability. As long as the provenance database records lineage and content hashes, Consist can:</p> <ul> <li>Verify that a cached result is valid (signature matches)</li> <li>Identify which upstream run produced a missing artifact</li> <li>Re-execute only the necessary steps to regenerate data (when strict cache validation is enabled)</li> </ul> <p>When You Need This</p> <p>Long-running research workflows accumulate massive intermediate datasets that you want to keep for lineage but eventually need to reclaim space. Consider a 20-year grid planning study with annual dispatch simulations:</p> <pre><code>Year 2025: Compute hourly dispatch → Store (200GB) → Use as input for 2026 capacity expansion\nYear 2026: Compute hourly dispatch → Store (200GB) → Use as input for 2027 capacity expansion\n... (20 years × 200GB = 4TB)\nYear 2045: Final resource plan published\n</code></pre> <p>Once you've published results (2035–2045), you can safely delete 2025–2034 intermediate files. The provenance database still tracks what those files contained (via content hashes). If you later need to re-run 2036, Consist can: 1. Detect that 2035's output is missing from disk (via cache validation) 2. Re-run only the missing upstream steps 3. Materialize just the files needed for the downstream run</p> <p>How It Works</p> <p>When you log an artifact in Consist, three pieces of information are persisted:</p> <ul> <li>Content hash (SHA256 of file bytes) — Stored in the database</li> <li>URI and metadata — Stored as provenance</li> <li>Actual file — On disk (optional in Ghost Mode)</li> </ul> <p>On cache hits, if a file is missing from disk, you have two recovery paths: - Re-execute (recommended for non-ingested outputs) by enabling strict cache validation - DB recovery (for ingested tabular outputs) via <code>consist.load_df(..., db_fallback=\"always\")</code></p> <p>Example: recover a missing, ingested tabular artifact from DuckDB: <pre><code>with tracker.start_run(\"2036_dispatch\", model=\"grid_sim\"):\n    artifact = tracker.log_artifact(\n        \"year_2035_dispatch.parquet\", key=\"dispatch\", direction=\"input\"\n    )\n    df = consist.load_df(artifact, db_fallback=\"always\")  # (1)!\n</code></pre></p> <ol> <li>Retrieves the 2035 dispatch data from DuckDB even though the original Parquet file was deleted. Requires the artifact to have been ingested during the original run.</li> </ol> <p>Re-execution respects the same cache key (code + config + inputs), so if inputs haven't changed, Consist reuses the prior computation and materializes its output on-demand.</p> <p>Best Practices</p> <ul> <li>Use Ghost Mode for intermediate outputs in long-running studies, not critical published results.</li> <li>Keep the provenance database (<code>provenance.duckdb</code>) on reliable storage; it's the source of truth for recovery.</li> <li>Archive deleted files alongside the database for offline recovery if needed.</li> <li>Test recovery workflows (intentional deletion + re-run or DB recovery) before relying on Ghost Mode in production.</li> </ul>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#data-model","level":2,"title":"Data Model","text":"<p>Consist uses two core entities with a many-to-many relationship:</p> Entity Purpose <code>Run</code> Execution context: model name, config, timestamps, status, parent linkage <code>Artifact</code> File metadata: path (as URI), content hash, driver, schema reference <code>RunArtifactLink</code> Connects runs to their input and output artifacts with direction metadata <p>Key fields for workflow tracking: - <code>Run.parent_run_id</code> — Links scenario steps to their parent scenario; used as the scenario identifier in views (see <code>consist_scenario_id</code>) - <code>Run.year</code> — Simulation year for time-series workflows - <code>Run.tags</code> — String labels for filtering (stored as JSON array) - <code>Artifact.hash</code> — SHA256 content hash for deduplication and verification</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#configuration-management","level":3,"title":"Configuration Management","text":"<p>Consist provides three strategies for tracking configuration:</p> Strategy Use case Stored in DB Affects cache Identity config (<code>config=</code>) Standard parameters JSON snapshot only Yes Facet (<code>facet=</code>) Queryable subset DuckDB table No Hash-only inputs (<code>hash_inputs=</code>) Large config files (10MB+) Hash only Yes <p>Most workflows combine all three: identity config for full parameters, facet for filtering, and hash-only for large external files.</p> <p>For detailed usage, see Configuration, Identity, and Facets.</p> <p>Database tables:</p> <ul> <li><code>config_facet</code>: Deduplicated facet JSON, namespaced by model</li> <li><code>run_config_kv</code>: Flattened key/value index for facet filtering</li> </ul>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#dual-write-persistence","level":2,"title":"Dual-Write Persistence","text":"<p>Consist maintains two synchronized records for resilience:</p> <pre><code>graph TD\n    Tracker[Tracker] --&gt; Context[Active Run Context]\n    Context --&gt; Memo[In-Memory Model]\n    Memo --&gt; Snapshot[consist.json Snapshot]\n    Memo --&gt; DB[(DuckDB Database)]\n    Snapshot --- Source[Source of Truth]\n    DB --- Query[Query Engine]</code></pre> <p>Write order (safety guarantee):</p> <ol> <li>Update in-memory model</li> <li>Flush to <code>consist.json</code> (atomic write) ← Source of truth</li> <li>Attempt DB sync (catch errors, log warning, never crash)</li> </ol> <p>JSON snapshots (<code>consist.json</code> per run): Portable, human-readable, version-controllable. Each run directory contains a complete record that survives database corruption.</p> <p>DuckDB database: Enables fast queries across runs, artifacts, and lineage. Can be rebuilt from JSON snapshots if needed. Handles concurrent access with retry logic.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#testing-coverage-focus","level":2,"title":"Testing &amp; Coverage Focus","text":"<p>The test suite prioritizes correctness for production workflows and portability:</p> <ul> <li>Cache hydration across run directories (metadata-only, requested/all outputs, missing-input reconstruction, permission/mount issues).</li> <li>Path virtualization and mount resolution (workspace URIs, symlink handling, stale/moved run directories).</li> <li>Persistence resilience (lock retries, constraint conflict handling, JSON snapshot safety).</li> <li>Ingestion and data virtualization (DLT ingestion paths, strict schema validation, hybrid view behavior).</li> <li>CLI/query helpers for inspection workflows (filters, preview error modes).</li> </ul>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#path-virtualization","level":2,"title":"Path Virtualization","text":"<p>Absolute paths break portability. Consist stores relative URIs and resolves them at runtime. For a focused guide, see Mounts &amp; Portability.</p> <pre><code>User logs: /mnt/data/land_use.csv\n           ↓\nTracker detects mount: mounts={\"inputs\": \"/mnt/data\"}\n           ↓\nStored URI: inputs://land_use.csv\n</code></pre> <p>Workspace URIs: For run-specific output directories, Consist mounts the current run's directory as <code>workspace://</code>. Historical paths are resolved via metadata stored in <code>Run.meta[\"_physical_run_dir\"]</code>.</p> <p>This means: - Provenance stays valid when data moves between machines - Teams can share databases without path conflicts - Cloud and local storage can coexist</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#data-virtualization","level":2,"title":"Data Virtualization","text":"","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#sql-views","level":3,"title":"SQL Views","text":"<p>Register SQLModel schemas to query artifacts across runs as unified tables:</p> <pre><code>class Person(SQLModel, table=True):\n    person_id: int = Field(primary_key=True)\n    age: int\n\ntracker = Tracker(schemas=[Person])\n\n# Access the view\nVPerson = tracker.views.Person\n\n# Query across all runs\nquery = select(VPerson).where(VPerson.consist_year == 2030)\n</code></pre> <p>Views automatically include system columns: - <code>consist_run_id</code> — Which run produced this row - <code>consist_scenario_id</code> — Parent scenario identifier - <code>consist_year</code> — Simulation year - <code>consist_artifact_id</code> — Source artifact</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#hybrid-views","level":3,"title":"Hybrid Views","text":"<p>Consist creates \"hybrid\" views that union: - Hot data: Rows ingested into DuckDB tables (fast queries) - Cold data: Parquet/CSV files on disk (no duplication)</p> <p>This lets you query terabytes of simulation output without loading everything into memory.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#matrix-views-n-dimensional","level":3,"title":"Matrix Views (N-Dimensional)","text":"<p>For Zarr/NetCDF arrays, <code>MatrixViewFactory</code> creates lazy xarray Datasets:</p> <ol> <li>Query the artifact catalog for matching Zarr stores</li> <li>Open each store lazily (no data loaded)</li> <li>Concatenate along <code>run_id</code> dimension with <code>year</code>/<code>iteration</code> as coordinates</li> </ol> <pre><code>ds = tracker.matrix.load(\"skim_matrices\", variables=[\"travel_time\"])\n# ds is an xarray.Dataset with dims: (run_id, origin, destination)\n</code></pre>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#container-integration","level":2,"title":"Container Integration","text":"<p>Containers are treated as pure functions. The cache signature includes:</p> <pre><code>signature = SHA256(image_digest || command || env_hash || mount_hashes || input_signatures)\n</code></pre> Condition Behavior Cache hit Verify outputs exist, relink artifacts, hydrate files to host paths Cache miss Execute container, capture outputs, record image digest <p>Supported backends: Docker, Singularity/Apptainer. For usage details, see Container Integration Guide.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#event-hooks","level":2,"title":"Event Hooks","text":"<p>Register callbacks for run lifecycle events:</p> <pre><code>tracker.events.on_run_complete(lambda run, artifacts: notify_slack(run.id))\ntracker.events.on_run_failed(lambda run, error: log_to_sentry(error))\n</code></pre> Event Callback signature <code>on_run_complete</code> <code>Callable[[Run, List[Artifact]], None]</code> <code>on_run_failed</code> <code>Callable[[Run, Exception], None]</code> <p>Hook failures are logged but do not crash the run.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#context-stack","level":2,"title":"Context Stack","text":"<p>Consist maintains a context-local stack of active trackers, allowing nested contexts and implicit tracker resolution:</p> <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        # consist.log_artifact() finds the active tracker automatically\n        with sc.step(name=\"simulate\"):\n            consist.log_dataframe(df, key=\"results\")  # No tracker= needed\n</code></pre> <p>This enables clean APIs where most functions don't require explicit tracker parameters.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"caching-and-hydration/","level":1,"title":"Moved","text":"<p>This page has been reorganized. See Caching &amp; Hydration for the new location.</p> <p>Redirect</p>","path":["Moved"],"tags":[]},{"location":"caching-fundamentals/","level":1,"title":"Moved","text":"<p>This page has been consolidated. See Caching &amp; Hydration for patterns or Architecture for technical details.</p> <p>Redirect</p>","path":["Moved"],"tags":[]},{"location":"cli-reference/","level":1,"title":"CLI Reference","text":"<p>Consist provides command-line tools to inspect, query, and compare runs and artifacts without writing Python code. Use it to answer “what ran, with what inputs, and what changed?” directly from your provenance database.</p> <p>This is especially useful when you are SSH’d into a remote server or working in a headless environment: you can quickly explore runs, artifacts, and lineage from the shell without starting a Python session. </p> <p>In particular, the <code>consist shell</code> command creates a persistent <code>Tracker</code> object linked to a database and allows you to query run inputs and outputs and artifact metadata, producing nicely formatted tables and summaries in the terminal.</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#database-discovery","level":2,"title":"Database Discovery","text":"<p>The CLI looks for the provenance database in this order: 1. Explicit <code>--db-path</code> argument 2. <code>CONSIST_DB</code> environment variable 3. <code>provenance.duckdb</code> in the current directory 4. Common subdirectories (<code>./data/</code>, <code>./db/</code>, <code>./.consist/</code>)</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#commands","level":2,"title":"Commands","text":"","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-runs","level":3,"title":"consist runs","text":"<p>List recent runs with optional filters.</p> <pre><code>consist runs                          # Last 10 runs\nconsist runs --limit 20               # Last 20 runs\nconsist runs --model travel_demand    # Filter by model name\nconsist runs --status completed       # Filter by status\nconsist runs --tag simulation         # Filter by tag\nconsist runs --json                   # JSON output for scripting\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-show","level":3,"title":"consist show","text":"<p>Display detailed information about a specific run.</p> <pre><code>consist show &lt;run_id&gt;\n</code></pre> <p>Shows run metadata, configuration, status, duration, and any custom metadata fields.</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-artifacts","level":3,"title":"consist artifacts","text":"<p>List input and output artifacts for a run.</p> <pre><code>consist artifacts &lt;run_id&gt;\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-lineage","level":3,"title":"consist lineage","text":"<p>Trace the full provenance chain for an artifact.</p> <pre><code>consist lineage &lt;artifact_key_or_id&gt;\n</code></pre> <p>Displays a tree showing which runs and inputs produced the artifact.</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-scenarios","level":3,"title":"consist scenarios","text":"<p>List all scenarios and their run counts.</p> <pre><code>consist scenarios\nconsist scenarios --limit 50\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-scenario","level":3,"title":"consist scenario","text":"<p>Show all runs within a specific scenario.</p> <pre><code>consist scenario &lt;scenario_id&gt;\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-search","level":3,"title":"consist search","text":"<p>Search runs by ID, model name, or scenario.</p> <pre><code>consist search \"baseline\"\nconsist search \"travel\" --limit 50\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-summary","level":3,"title":"consist summary","text":"<p>Display database statistics: total runs, artifacts, date range, and runs per model.</p> <pre><code>consist summary\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-preview","level":3,"title":"consist preview","text":"<p>Preview tabular artifacts (CSV, Parquet) without loading full data.</p> <pre><code>consist preview &lt;artifact_key&gt;\nconsist preview &lt;artifact_key&gt; --rows 10\nconsist preview &lt;artifact_key&gt; --trust-db  # Allow mount inference from DB metadata\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-validate","level":3,"title":"consist validate","text":"<p>Check that artifacts in the database exist on disk.</p> <pre><code>consist validate\nconsist validate --fix  # Mark missing artifacts\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-shell","level":3,"title":"consist shell","text":"<p>Start an interactive session for exploring provenance.</p> <pre><code>consist shell\nconsist shell --trust-db  # Allow mount inference from DB metadata\n</code></pre> <p>Inside the shell: <pre><code>(consist) runs --limit 5\n(consist) show abc123\n(consist) artifacts abc123\n(consist) scenarios\n(consist) exit\n</code></pre></p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#scripting-with-json-output","level":2,"title":"Scripting with JSON Output","text":"<p>Use <code>--json</code> for machine-readable output:</p> <pre><code>consist runs --json | jq '.[] | select(.status == \"completed\")'\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"concepts/","level":1,"title":"Moved","text":"<p>This page has been reorganized. See Core Concepts for the new location.</p> <p>Redirect</p>","path":["Moved"],"tags":[]},{"location":"configs/","level":1,"title":"Configuration, Identity, and Facets","text":"<p>This guide explains how Consist handles configuration data, how it impacts run identity hashing, and how to make config data queryable in the database.</p>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#overview","level":2,"title":"Overview","text":"<p>Consist supports three complementary configuration channels per run:</p> <ol> <li>Identity config: <code>config=...</code> (hashed into the run signature)</li> <li>Queryable facet: <code>facet=...</code> (persisted in DuckDB and optionally indexed)</li> <li>Hash-only inputs: <code>hash_inputs=[...]</code> (file/dir digests folded into identity)</li> </ol> <p>Use <code>config</code> for the full run configuration, <code>facet</code> for a small subset of fields you want to query, and <code>hash_inputs</code> for external config trees or files that should affect caching without being stored as structured metadata.</p>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#api-summary","level":2,"title":"API Summary","text":"<p>You can pass config parameters via the high-level APIs:</p> <ul> <li><code>consist.run(...)</code> / <code>consist.trace(...)</code></li> <li><code>Tracker.run(...)</code> / <code>Tracker.trace(...)</code> (explicit tracker form)</li> <li><code>ScenarioContext.run(...)</code></li> <li><code>ScenarioContext.trace(...)</code></li> </ul> <p>Relevant arguments:</p> <ul> <li><code>config: dict | BaseModel | None</code></li> <li><code>config_plan: ConfigPlan | None</code></li> <li><code>facet: dict | BaseModel | None</code></li> <li><code>facet_from: list[str] | None</code></li> <li><code>hash_inputs: list[Path | str | (label, Path|str)] | None</code></li> <li><code>facet_schema_version: str|int|None</code></li> <li><code>facet_index: bool</code> (default <code>True</code>)</li> </ul>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#identity-hashing","level":2,"title":"Identity Hashing","text":"<p>A run signature is derived from:</p> <ul> <li>the config hash</li> <li>the input hash (logged inputs)</li> <li>the code hash</li> </ul> <p>The config hash includes:</p> <ul> <li>the <code>config</code> payload (normalized JSON)</li> <li>select run fields (<code>model</code>, <code>year</code>, <code>iteration</code>) under a reserved   <code>__consist_run_fields__</code> key to avoid cache hits across distinct runs</li> </ul> <p>This means changing <code>model</code>, <code>year</code>, or <code>iteration</code> will change the run signature even if <code>config</code> is unchanged.</p>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#identity-config-config","level":2,"title":"Identity Config (<code>config</code>)","text":"<ul> <li>Drives <code>Run.config_hash</code> and cache identity.</li> <li>Stored in the per-run <code>consist.json</code> snapshot under <code>config</code>.</li> <li>Not stored as structured/queryable data in DuckDB.</li> </ul> <p>If you pass a Pydantic model, it is serialized via <code>model_dump()</code> for hashing and JSON snapshotting.</p>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#facets-facet-and-facet_from","level":2,"title":"Facets (<code>facet</code> and <code>facet_from</code>)","text":"<p>Facets are compact, queryable config subsets persisted to DuckDB. They are intended to be small, stable, and useful for filtering.</p> <p>Facet sources:</p> <ul> <li>Explicit <code>facet=...</code></li> <li><code>facet_from=[\"key\", ...]</code> extracts top-level keys from <code>config</code></li> <li><code>to_consist_facet()</code> on a Pydantic config model (if implemented)</li> </ul> <p>Behavior notes:</p> <ul> <li>Facets are only persisted when explicitly provided, extracted via   <code>facet_from</code>, or produced by <code>to_consist_facet()</code>.</li> <li><code>facet_from</code> raises <code>KeyError</code> if any key is missing in <code>config</code>.</li> <li>If both <code>facet</code> and <code>facet_from</code> are provided, the extracted values are   merged into the explicit facet, and explicit keys win.</li> <li>If <code>facet_schema_version</code> is not provided and the config model exposes   <code>facet_schema_version</code>, Consist records that value automatically.</li> </ul> <p>Guardrails (enforced silently; no error raised):</p> Limit Behavior when exceeded Facet &gt; 16 KB (canonical JSON) Facet not persisted; run continues Facet &gt; 500 KV rows Facet not indexed; run continues","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#hash-only-inputs-hash_inputs","level":2,"title":"Hash-Only Inputs (<code>hash_inputs</code>)","text":"<p>Use <code>hash_inputs</code> to include file or directory content in identity hashing without logging inputs in provenance.</p> <ul> <li>Files are hashed via the configured <code>hashing_strategy</code>.</li> <li>Directories are hashed deterministically via a sorted walk.</li> <li>Dotfiles are ignored by default.</li> </ul> <p><code>hash_inputs</code> is path-only (files or directories). Artifacts are intentionally excluded to keep identity hashing deterministic and avoid implicit hydration.</p> <p>Digests are recorded in:</p> <ul> <li><code>config[\"__consist_hash_inputs__\"]</code> (identity-only payload)</li> <li><code>run.meta[\"consist_hash_inputs\"]</code> (audit/debugging)</li> </ul>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#when-to-use-hash-only-inputs","level":3,"title":"When to Use Hash-Only Inputs","text":"<p><code>hash_inputs</code> solves a specific problem: Configuration files that must affect the cache key but are too large or unstructured to query in the database.</p> <p>The Trade-off</p> <p>Normally, you pass configuration two ways:</p> <ol> <li>Via <code>config=</code> — Stored in the JSON run snapshot and hashed into identity, but not queryable by default.</li> <li>Not tracked — Smaller footprint, but changes to external files don't invalidate the cache, risking incorrect cache hits.</li> </ol> <p><code>hash_inputs</code> is the middle ground: Hash the files so cache keys change when they do, but don't store the content.</p> <p>Example: ActivitySim Configuration</p> <p>ActivitySim projects use a 5MB+ directory of YAML and csv configuration files. The config affects simulation output, so it must change the cache signature. But the YAML is: - Too large to store as a queryable facet (exceeds the 16 KB limit) - Unstructured (hundreds of files with complex nested keys) - Not useful for filtering runs (you don't query \"find runs where beam.memory=180\")</p> <p>Instead, hash the config directory:</p> <pre><code>import consist\nfrom pathlib import Path\nfrom consist import use_tracker\n\nasim_config_dir = Path(\"./configs/activitysim\")\n\nwith use_tracker(tracker):\n    # First run with baseline config\n    result1 = consist.run(\n        fn=run_activitysim,\n        name=\"asim_baseline\",\n        config={\"scenario\": \"baseline\"},\n        hash_inputs=[(\"asim_config\", asim_config_dir)],\n    )\n\n# Later: You edit the YAML config (change a parameter)\n# The directory hash changes → signature changes → cache miss\n# ActivitySim re-runs with the new config\n\n    # Second run with updated config\n    result2 = consist.run(\n        fn=run_activitysim,\n        name=\"asim_baseline\",\n        config={\"scenario\": \"baseline\"},  # Same config dict\n        hash_inputs=[(\"asim_config\", asim_config_dir)],  # Different hash\n    )\n# Different signature (hash_inputs changed) → fresh run\n</code></pre> <p>Comparison: config vs hash_inputs</p> Approach Space Queryable Cache Behavior <code>config={\"yaml\": large_dict}</code> JSON snapshot only No (by default) Cache respects changes <code>hash_inputs=[path]</code> Minimal No Cache respects changes Ignore files Minimal N/A Cache miss if files change (BAD) <p>For large, unstructured configs like ActivitySim YAML, use <code>hash_inputs</code>.</p> <p>Audit Trail</p> <p>Even though the file content isn't stored, Consist records the hash for debugging:</p> <pre><code>run = tracker.get_run(\"asim_baseline_001\")\nprint(run.meta[\"consist_hash_inputs\"])\n# Output: {\"asim_config\": \"sha256:a1b2c3...\"}\n</code></pre> <p>This lets you correlate runs with specific configuration states without storing the full config.</p>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#querying-facets-in-duckdb","level":2,"title":"Querying Facets in DuckDB","text":"<p>Facet persistence creates two tables:</p> <ul> <li><code>config_facet</code>: deduplicated facet JSON blobs</li> <li><code>run_config_kv</code>: flattened key/value index for filtering</li> </ul> <p>Flattened keys use dot-notation, with dots in raw keys escaped as <code>\\.</code>.</p> <p>Examples:</p> <pre><code>tracker.find_runs_by_facet_kv(\n    namespace=\"beam\",\n    key=\"memory_gb\",\n    value_num=180,\n)\n</code></pre> <p>For range queries, filter results in Python or use direct SQL on the <code>run_config_kv</code> table.</p> <p>You can also fetch facets directly:</p> <pre><code>facet = tracker.get_config_facet(facet_id)\n</code></pre>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#model-specific-config-adapters","level":2,"title":"Model-Specific Config Adapters","text":"<p>For complex, file-based configurations (such as ActivitySim YAML/CSV configs), Consist provides config adapters—model-specific modules that discover, canonicalize, and ingest configuration data into queryable tables.</p> <p>Unlike in-memory configs (dicts/Pydantic models), adapters handle: - Discovery: Locating config files and computing content hashes - Canonicalization: Converting config metadata into artifacts and ingestable schemas - Ingestion: Persisting calibration-sensitive parameters as queryable tables</p> <p>This decouples model-specific parsing logic from Consist core, making it easy to add new model types without coupling them to the framework.</p> <p>Available adapters:</p> <ul> <li>ActivitySim Config Adapter — Discover, canonicalize, and query ActivitySim YAML/CSV configurations</li> <li>BEAM Config Adapter — Canonicalize HOCON configs and query key/value parameters</li> </ul> <p>For detailed usage and API reference, see the Config Adapters Integration Guide.</p>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#examples","level":2,"title":"Examples","text":"","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#consistrun-with-config-facet","level":3,"title":"consist.run with config + facet","text":"<pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=run_beam_step,\n        name=\"beam\",\n        config=beam_cfg,\n        facet={\"memory_gb\": beam_cfg.memory_gb},\n        hash_inputs=[(\"beam_hocon\", beam_cfg_path)],\n    )\n</code></pre>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#consistrun-with-config_plan-config-adapters","level":3,"title":"consist.run with config_plan (config adapters)","text":"<pre><code>from consist.integrations.activitysim import ActivitySimConfigAdapter\n\nimport consist\nfrom consist import use_tracker\n\nadapter = ActivitySimConfigAdapter()\nplan = tracker.prepare_config(adapter, [overlay_dir, base_dir])\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=run_activitysim,\n        name=\"activitysim\",\n        config={\"scenario\": \"baseline\"},\n        config_plan=plan,\n        cache_mode=\"reuse\",\n    )\n</code></pre>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#scenariocontextrun-with-facet_from","level":3,"title":"ScenarioContext.run with facet_from","text":"<pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    with consist.scenario(\"demo\", tags=[\"travel\"]) as sc:\n        sc.run(\n            fn=run_activitysim,\n            name=\"activitysim\",\n            config=asim_cfg,\n            facet_from=[\"scenario\", \"sample_rate\"],\n            hash_inputs=[(\"asim_yaml\", asim_config_dir)],\n        )\n</code></pre>","path":["Guides","Configuration, Identity, and Facets"],"tags":[]},{"location":"containers-guide/","level":1,"title":"Container Integration Guide","text":"<p>Consist executes containerized tools (Docker, Singularity) with automatic provenance tracking and caching based on image digest and parameters.</p> <p>Image Digest</p> <p>An image digest is the SHA256 hash of a container image's content (e.g., <code>sha256:a1b2c3...</code>). Unlike tags (<code>:latest</code>, <code>:v1.0</code>), digests are immutable—the same digest always refers to the same image bytes.</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#when-to-use-containers","level":2,"title":"When to Use Containers","text":"Use case Recommendation Wrapping existing tools (ActivitySim, SUMO, BEAM, R scripts) Container Complex dependencies easier to package than install Container Black-box or non-deterministic tools Container Tool expects specific file paths Container Simple Python functions <code>consist.run()</code> Fine-grained step caching <code>scenario()</code> + <code>consist.run()</code> Development/debugging with fast iteration Native execution","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#how-caching-works","level":2,"title":"How Caching Works","text":"<p>Consist computes a container signature from:</p> Component Included in signature Notes Image digest Yes Resolved from registry if <code>pull_latest=True</code> Command Yes Exact command string and arguments Environment variables Yes Deterministic hash Container mount paths Yes e.g., <code>/inputs</code>, <code>/outputs</code> Host paths No Run-specific; not part of cache key Input file content Yes SHA256 hashes <p>If all components match a prior run, Consist returns cached outputs without executing the container.</p> <p>Host paths are not cached</p> <p>Volume host paths (e.g., <code>./data:/inputs</code>) are excluded from the cache key because they vary between runs. To ensure config changes invalidate cache, pass config files via <code>inputs=</code>.</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#basic-example-single-container-run","level":2,"title":"Basic Example: Single Container Run","text":"<pre><code>from consist import Tracker\nfrom consist.integrations.containers import run_container\nfrom pathlib import Path\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\n# Create input data\ninput_path = Path(\"./data/input.csv\")\ninput_path.parent.mkdir(exist_ok=True)\ninput_path.write_text(\"x,y\\n1,2\\n3,4\\n\")\n\n# Execute container\nresult = run_container(\n    tracker=tracker,\n    run_id=\"my_model_run\",\n    image=\"my-org/my-model:v1.0\",\n    command=[\"python\", \"process.py\", \"--input\", \"/inputs/input.csv\"],\n    volumes={\n        \"./data\": \"/inputs\",      # Host path → Container path\n        \"./outputs\": \"/outputs\",\n    },\n    inputs=[input_path],          # Files to hash (for cache key)\n    outputs=[\"./outputs/result.csv\"],  # Files to capture as artifacts\n    backend_type=\"docker\",\n)\n\n# Access results\nif result.cache_hit:\n    print(f\"Cache hit from {result.cache_source}\")\nelse:\n    print(\"Container executed\")\n\nfor key, artifact in result.artifacts.items():\n    print(f\"Output: {key} → {artifact.path}\")\n</code></pre> <p>What happened: 1. Consist created a signature from image + command + inputs 2. Checked if this signature exists in the database 3. If no prior run: executed the container, scanned outputs, logged them as artifacts 4. If prior run exists: copied cached artifacts to <code>./outputs/result.csv</code> (no container execution)</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#activitysim-integration","level":2,"title":"ActivitySim Integration","text":"<p>ActivitySim is a commonly used transportation demand modeling tool. Here's how to integrate it with Consist:</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#setup","level":3,"title":"Setup","text":"<ol> <li>Create a Docker image with ActivitySim installed:</li> </ol> <pre><code>FROM python:3.11\nRUN pip install activitysim numpy pandas\nCOPY . /workspace\nWORKDIR /workspace\n</code></pre> <p>Build and push to registry: <pre><code>docker build -t my-org/activitysim:v1.0 .\ndocker push my-org/activitysim:v1.0\n</code></pre></p> <ol> <li>Create ActivitySim config files (as usual) in a local directory: <pre><code>./configs/\n├── settings.yaml\n├── accessibility_coefficients.csv\n├── mode_choice_coefficients.csv\n└── ...\n</code></pre></li> </ol>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#running-a-scenario","level":3,"title":"Running a Scenario","text":"<pre><code>from consist import Tracker\nfrom consist.integrations.containers import run_container\nfrom pathlib import Path\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\ndef run_activitysim_scenario(scenario_name: str, configs_dir: Path):\n    \"\"\"Execute ActivitySim with Consist provenance.\"\"\"\n\n    # Ensure output directory exists\n    output_dir = Path(f\"./outputs/{scenario_name}\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    result = run_container(\n        tracker=tracker,\n        run_id=f\"activitysim_{scenario_name}\",\n        image=\"my-org/activitysim:v1.0\",\n        command=[\n            \"python\", \"-m\", \"activitysim.core.workflow\",\n            \"-c\", \"/configs\",\n            \"-o\", f\"/outputs/{scenario_name}\",\n        ],\n        volumes={\n            str(configs_dir): \"/configs\",\n            \"./outputs\": \"/outputs\",\n        },\n        inputs=[configs_dir],  # Hash all configs\n        outputs=[f\"./outputs/{scenario_name}\"],  # Capture all outputs\n        environment={\n            \"ACTIVITYSIM_CHUNK_SIZE\": \"100000\",  # Optional: tune performance\n        },\n        backend_type=\"docker\",\n    )\n\n    return result\n\n# Run baseline scenario\nresult_baseline = run_activitysim_scenario(\n    \"baseline\",\n    configs_dir=Path(\"./configs\"),\n)\n\n# Run scenario with modified coefficients\n# (New output directory and modified config) → new cache entry\nresult_modified = run_activitysim_scenario(\n    \"high_income_sensitivity\",\n    configs_dir=Path(\"./configs_modified\"),\n)\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#querying-results","level":3,"title":"Querying Results","text":"<p>After running scenarios, query outputs across runs:</p> <pre><code>import pandas as pd\nfrom pathlib import Path\n\n# Find all ActivitySim outputs\nfor run_subdir in Path(\"./runs\").glob(\"activitysim_*/\"):\n    output_artifacts = tracker.get_artifacts_for_run(run_subdir.name)\n    for key, artifact in output_artifacts.outputs.items():\n        if \"summary\" in key:\n            df = pd.read_csv(artifact.path)\n            print(f\"Scenario: {run_subdir.name}\")\n            print(df.head())\n</code></pre> <p>Or use Consist's database queries:</p> <pre><code>from sqlmodel import Session, select\n\nwith Session(tracker.engine) as session:\n    # Find all ActivitySim runs\n    runs = session.exec(\n        select(Run).where(Run.model == \"activitysim\")\n    ).all()\n\n    for run in runs:\n        artifacts = tracker.get_artifacts_for_run(run.id)\n        print(f\"Run: {run.id}, Outputs: {list(artifacts.outputs.keys())}\")\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#singularity-apptainer-support","level":2,"title":"Singularity / Apptainer Support","text":"<p>If using Singularity (common on HPC clusters):</p> <pre><code>from consist.integrations.containers import run_container\n\nresult = run_container(\n    tracker=tracker,\n    run_id=\"hpc_job\",\n    image=\"/path/to/my_model.sif\",  # Local .sif file path\n    command=[\"python\", \"model.py\", \"--param\", \"value\"],\n    volumes={\n        \"/scratch/inputs\": \"/inputs\",\n        \"/scratch/outputs\": \"/outputs\",\n    },\n    inputs=[Path(\"/scratch/inputs/data.csv\")],\n    outputs=[\"/scratch/outputs\"],\n    backend_type=\"singularity\",  # Use Singularity instead of Docker\n)\n</code></pre> <p>Key differences: - Image is typically a local file path (.sif) not a registry URL - Volume syntax is the same - Caching behavior is identical (based on image path digest)</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#volume-mounting-best-practices","level":2,"title":"Volume Mounting Best Practices","text":"","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#mount-paths","level":3,"title":"Mount Paths","text":"<p>Map host directories to container paths consistently:</p> <pre><code>volumes = {\n    \"/host/absolute/path\": \"/container/path\",  # Use absolute paths\n    \"./relative\": \"/container/relative\",        # Or relative (resolved against cwd)\n}\n</code></pre> <p>By default, Consist only allows host paths that live under configured mounts. If you need to allow arbitrary absolute host paths, pass <code>strict_mounts=False</code> to <code>run_container()</code>.</p> <p>To ensure config changes invalidate the cache, pass config files via <code>inputs</code>:</p> <pre><code>result = run_container(\n    ...\n    volumes={\n        \"./config_dir\": \"/configs\",\n        \"./data\": \"/data\",\n    },\n    inputs=[Path(\"./config_dir/settings.yaml\"), Path(\"./data/input.csv\")],\n    ...\n)\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#handling-outputs","level":3,"title":"Handling Outputs","text":"<p>Outputs can be specified as:</p> <ul> <li> <p>List of paths (logged with filename as key):   <pre><code>outputs=[\"./outputs/result.csv\", \"./outputs/logs.txt\"]\n# Artifact keys: \"result.csv\", \"logs.txt\"\n</code></pre></p> </li> <li> <p>Dict mapping keys to paths (custom artifact keys):   <pre><code>outputs={\n    \"main_result\": \"./outputs/result.csv\",\n    \"diagnostics\": \"./outputs/logs.txt\",\n}\n# Artifact keys: \"main_result\", \"diagnostics\"\n</code></pre></p> </li> </ul> <p>Consist scans output directories on the host after container exits. Files created inside the container at mounted paths are detected automatically.</p> <p>Outputs must live under <code>tracker.run_dir</code> or a configured mount unless <code>allow_external_paths=True</code> (or <code>CONSIST_ALLOW_EXTERNAL_PATHS=1</code>) is set on the tracker.</p> <p>Warning: If the container doesn't create files at the expected paths, Consist logs a warning but doesn't fail (use <code>lineage_mode=\"full\"</code> to capture what was created).</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#environment-variables-configuration","level":2,"title":"Environment Variables &amp; Configuration","text":"<p>Pass environment variables to the container:</p> <pre><code>result = run_container(\n    ...\n    environment={\n        \"MODEL_PARAM_1\": \"value1\",\n        \"MODEL_PARAM_2\": \"value2\",\n        \"DEBUG\": \"true\",\n    },\n    ...\n)\n</code></pre> <p>These variables are part of the cache key. Changing them invalidates the cache.</p> <p>Consist does not persist raw environment values in run metadata or container manifests. It stores a deterministic hash instead. If you need those values for reproducibility, include them in config files and add those files to <code>inputs</code>.</p> <p>For large configurations, mount config files instead of passing via environment:</p> <pre><code># DON'T do this for large configs:\nenvironment={\"CONFIG\": json.dumps(huge_config)}  # Bad: unreadable, cache-unfriendly\n\n# DO this:\nvolumes={\"./config.json\": \"/app/config.json\"}\ninputs=[Path(\"./config.json\")]\ncommand=[\"python\", \"app.py\", \"--config\", \"/app/config.json\"]\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#cache-behavior-hydration","level":2,"title":"Cache Behavior &amp; Hydration","text":"","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#cache-hits","level":3,"title":"Cache Hits","text":"<p>On a cache hit:</p> <ol> <li>Consist finds a prior run with the same signature</li> <li>No container execution occurs</li> <li>Cached output files are copied to host paths</li> <li><code>result.cache_hit == True</code>, <code>result.cache_source</code> = prior run ID</li> </ol> <pre><code>result = run_container(...)\nif result.cache_hit:\n    print(f\"Cache hit from {result.cache_source}\")\n    # result.artifacts are materialized (files exist on disk)\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#cache-invalidation","level":3,"title":"Cache Invalidation","text":"<p>Cache is invalidated (new run executed) if any of these change:</p> <ul> <li>Image: <code>my-model:v1</code> → <code>my-model:v2</code> (or image pulled with different digest if <code>pull_latest=True</code>)</li> <li>Command: Arguments to the tool</li> <li>Environment: Any env var changes</li> <li>Inputs: Hash of input files changes</li> <li>Volumes: Container mount paths change</li> </ul>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#manual-cache-bypass","level":3,"title":"Manual Cache Bypass","text":"<p>To force re-execution even with matching signature:</p> <pre><code># No built-in flag, but you can:\n# 1. Change a trivial env var to force cache miss:\nenvironment={\"CACHE_BYPASS\": str(time.time())}\n\n# 2. Or use a cache_mode on the enclosing run:\ntracker.begin_run(..., cache_mode=\"overwrite\")\nresult = run_container(...)\ntracker.end_run()\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#nested-containers-inside-scenarios","level":2,"title":"Nested Containers (Inside Scenarios)","text":"<p>Use <code>run_container()</code> inside <code>scenario()</code> for multi-step workflows:</p> <pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom consist.integrations.containers import run_container\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\nwith use_tracker(tracker):\n    with consist.scenario(\"multi_model_workflow\") as sc:\n\n        # Step 1: Data preprocessing (Python)\n        preprocess_result = run_container(\n            tracker=tracker,\n            run_id=\"preprocess\",\n            image=\"my-org/preprocess:v1\",\n            command=[\"python\", \"preprocess.py\"],\n            volumes={\"./raw_data\": \"/data\"},\n            inputs=[Path(\"./raw_data\")],\n            outputs=[\"./preprocessed\"],\n        )\n        sc.coupler.set(\"preprocessed_data\", preprocess_result.output)\n\n        # Step 2: Model execution (ActivitySim)\n        model_result = run_container(\n            tracker=tracker,\n            run_id=\"activitysim\",\n            image=\"my-org/activitysim:v1\",\n            command=[\"python\", \"-m\", \"activitysim.core.workflow\"],\n            volumes={\n                \"./configs\": \"/configs\",\n                \"./model_outputs\": \"/outputs\",\n            },\n            inputs=[Path(\"./configs\")],\n            outputs=[\"./model_outputs\"],\n        )\n        sc.coupler.set(\"model_outputs\", model_result.output)\n\n        # Step 3: Analysis (Python)\n        with sc.trace(name=\"analysis\"):\n            # Load previous results\n            preprocessed = consist.load_df(sc.coupler.require(\"preprocessed_data\"))\n            model_output = consist.load_df(sc.coupler.require(\"model_outputs\"))\n            # ... analysis code ...\n</code></pre> <p>Each container step caches independently. If preprocessing input hasn't changed, skip it. If model config hasn't changed, skip it.</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#error-handling","level":2,"title":"Error Handling","text":"","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#container-execution-fails","level":3,"title":"Container Execution Fails","text":"<p>If the container exits with a non-zero code:</p> <pre><code>try:\n    result = run_container(...)\nexcept RuntimeError as e:\n    print(f\"Container failed: {e}\")\n    # Debug: check container logs, input paths, volume mounts\n</code></pre> <p>Debugging steps: 1. Run the container manually to check for errors:    <pre><code>docker run -it -v ./data:/inputs my-org/my-model:v1 python process.py\n</code></pre> 2. Check that input paths exist and are readable by the container 3. Ensure output directories are writable (containers often run as root) 4. Check volume mount paths are absolute or correctly resolved</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#output-files-not-found","level":3,"title":"Output Files Not Found","text":"<p>If Consist doesn't find expected outputs:</p> <pre><code>result = run_container(...)\n# Warning logged: \"Expected output not found: ./outputs/result.csv\"\n\n# Check what was actually created:\nimport subprocess\nsubprocess.run([\"docker\", \"run\", \"--rm\",\n                \"-v\", \"./outputs:/outputs\",\n                \"my-org/my-model:v1\",\n                \"ls\", \"-la\", \"/outputs\"])\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#image-pull-errors","level":3,"title":"Image Pull Errors","text":"<p>If the image cannot be pulled:</p> <pre><code>result = run_container(\n    ...\n    image=\"my-org/my-model:v1.0\",\n    pull_latest=False,  # Avoid registry roundtrip if not needed\n    backend_type=\"docker\",\n)\n# Check docker auth:\n# docker login\n# docker pull my-org/my-model:v1.0\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#performance-tuning","level":2,"title":"Performance Tuning","text":"","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#container-startup-overhead","level":3,"title":"Container Startup Overhead","text":"<p>Container creation/startup is ~1-2 seconds. For workflows with many short-lived steps, batch them:</p> <pre><code># DON'T do this (N containers, N startups):\nfor i in range(100):\n    run_container(...)\n\n# DO this (1 container, batch processing inside):\nrun_container(\n    command=[\"python\", \"batch_process.py\", \"--n\", \"100\"],\n    ...\n)\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#image-size-registry","level":3,"title":"Image Size &amp; Registry","text":"<p>Large images slow down pulls. Optimize: - Use slim base images (<code>python:3.11-slim</code> not <code>python:3.11</code>) - Only install required dependencies - Use multi-stage builds</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#persistent-caching","level":3,"title":"Persistent Caching","text":"<p>If you re-run the same container frequently, Consist's cache avoids re-execution. But if cache is disabled or cleared:</p> <pre><code># Cache is per (image_digest, command, inputs) signature\n# To maximize cache reuse:\n# 1. Pin image versions (don't use :latest)\n# 2. Use `pull_latest=False` unless you need latest code\n# 3. Log inputs consistently (same file paths, same hashes)\n</code></pre>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#lineage-mode-full-vs-none","level":2,"title":"Lineage Mode: \"full\" vs \"none\"","text":"<p>By default, <code>lineage_mode=\"full\"</code> performs provenance tracking. If you need to execute containers without Consist logging (for external tools), use <code>lineage_mode=\"none\"</code>:</p> <pre><code>result = run_container(\n    ...\n    lineage_mode=\"none\",  # Don't create a Consist run\n)\n# No provenance logged, but you still get manifest_hash for external tracking\n</code></pre> <p>This is useful if you're embedding Consist-executed containers inside a non-Consist workflow.</p>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#see-also","level":2,"title":"See Also","text":"<ul> <li>Usage Guide: Pattern 3 (Container Integration)</li> <li>Architecture: Container Integration</li> <li>Troubleshooting: Container Execution</li> </ul>","path":["Guides","Container Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/","level":1,"title":"DLT Loader Integration Guide","text":"<p>The DLT (Data Load Tool) integration enables robust, schema-validated ingestion of data into DuckDB with automatic provenance column injection. This guide covers when to use DLT, how to configure schemas, and best practices for data quality.</p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#what-is-dlt","level":2,"title":"What is DLT?","text":"<p><code>dlt</code> is an open-source library for extracting and loading data. Consist uses it to:</p> <ul> <li>Ingest diverse formats (Parquet, CSV, JSON, Python objects) into DuckDB</li> <li>Auto-detect and enforce schemas (schema enforcement when provided)</li> <li>Handle data quality issues (type mismatches, missing values, duplicates)</li> <li>Inject Consist provenance columns (<code>consist_run_id</code>, <code>consist_artifact_id</code>, <code>consist_year</code>, etc.)</li> <li>Scale efficiently with streaming and batching</li> </ul>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#when-to-use-dlt-vs-direct-logging","level":2,"title":"When to Use DLT vs Direct Logging","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#when-to-use-each-api","level":3,"title":"When to Use Each API","text":"Criterion Use DLT Use Direct Logging Dataset size 100K+ rows &lt;10K rows Cross-run SQL queries Required Not needed Schema validation Required Not needed Data structure Complex, nested, evolving Simple flat files Source Your workflow outputs External data you don't control <p>DLT example: <pre><code>import consist\nfrom sqlmodel import select\n\nVPerson = tracker.views.Person\nrows = consist.run_query(\n    select(VPerson.consist_run_id, VPerson.age),\n    tracker=tracker,\n)\n</code></pre></p> <p>Direct logging example: <pre><code>with tracker.start_run(\"log_result\", model=\"demo\"):\n    consist.log_artifact(result_path, key=\"my_result\")\n</code></pre></p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#decision-tree","level":3,"title":"Decision Tree","text":"<pre><code>Need cross-run SQL queries?\n├─ YES → Use DLT (register a schema)\n└─ NO  → Need schema validation?\n         ├─ YES → Use DLT with a schema\n         └─ NO  → Use direct logging\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#basic-dlt-workflow","level":2,"title":"Basic DLT Workflow","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-1-define-a-schema-sqlmodel","level":3,"title":"Step 1: Define a Schema (SQLModel)","text":"<pre><code>from sqlmodel import SQLModel, Field\nfrom typing import Optional\n\nclass Person(SQLModel, table=True):\n    \"\"\"Schema for person records.\"\"\"\n    person_id: int = Field(primary_key=True)\n    age: int\n    income: Optional[float]\n    name: str\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-2-create-tracker-with-schema","level":3,"title":"Step 2: Create Tracker with Schema","text":"<pre><code>from consist import Tracker\nfrom pathlib import Path\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person],  # Register schema\n)\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-3-log-data-with-schema","level":3,"title":"Step 3: Log Data with Schema","text":"<pre><code>import pandas as pd\n\n# Create data\ndf = pd.DataFrame({\n    \"person_id\": [1, 2, 3],\n    \"age\": [25, 30, 35],\n    \"income\": [50000.0, 60000.0, 70000.0],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\"],\n})\n\n# Log with schema (DLT ingestion)\nwith tracker.start_run(\"ingest_people\", model=\"demo\"):\n    tracker.log_dataframe(df, key=\"persons\", schema=Person)\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-4-query-across-runs","level":3,"title":"Step 4: Query Across Runs","text":"<pre><code>from sqlmodel import Session, select, func\n\n# Compute an aggregate over a single run_id (via the hybrid view).\nVPerson = tracker.views.Person\nwith Session(tracker.engine) as session:\n    avg_age = session.exec(\n        # Filter to one Consist run and take an average over the ingested rows.\n        select(func.avg(VPerson.age)).where(VPerson.consist_run_id == \"run_123\")\n    ).first()\n    print(f\"Average age in run_123: {avg_age}\")\n</code></pre> <p>Expected output:</p> <pre><code>Average age in run_123: 30.0\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#schema-definition-validation","level":2,"title":"Schema Definition &amp; Validation","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#basic-schema","level":3,"title":"Basic Schema","text":"<pre><code>from sqlmodel import SQLModel, Field\nfrom typing import Optional\n\nclass Trip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True)\n    person_id: int\n    origin: str\n    destination: str\n    distance_miles: float\n    mode: str  # \"car\", \"transit\", \"bike\"\n    departure_hour: Optional[int]\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#validation-required-vs-optional","level":3,"title":"Validation: Required vs Optional","text":"<pre><code>class StrictTrip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True, description=\"Unique trip ID\")\n    person_id: int = Field(gt=0)  # Must be positive\n    mode: str = Field(min_length=1)  # Non-empty\n    departure_hour: Optional[int] = Field(ge=0, le=23)  # 0-23 if present\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#foreign-keys-relationships","level":3,"title":"Foreign Keys (Relationships)","text":"<pre><code>from sqlmodel import SQLModel, Field, Relationship\n\nclass Person(SQLModel, table=True):\n    person_id: int = Field(primary_key=True)\n    name: str\n    # trips: List[\"Trip\"] = Relationship(back_populates=\"person\")\n\nclass Trip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True)\n    person_id: int = Field(foreign_key=\"person.person_id\")\n    distance_miles: float\n    # person: Person = Relationship(back_populates=\"trips\")\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#type-mapping","level":3,"title":"Type Mapping","text":"Python Type DuckDB Type Notes <code>int</code> <code>INTEGER</code> <code>float</code> <code>DOUBLE</code> <code>str</code> <code>VARCHAR</code> <code>bool</code> <code>BOOLEAN</code> <code>date</code> <code>DATE</code> Use <code>datetime.date</code> <code>datetime</code> <code>TIMESTAMP</code> Use <code>datetime.datetime</code> <code>Optional[T]</code> <code>T NULL</code> Allows NULL <code>List[T]</code> <code>T[]</code> Arrays (advanced)","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#logging-patterns","level":2,"title":"Logging Patterns","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#single-dataframe","level":3,"title":"Single DataFrame","text":"<pre><code>import consist\n\ndf = pd.read_csv(\"results.csv\")\nwith tracker.start_run(\"log_results\", model=\"demo\"):\n    consist.log_dataframe(\n        df,\n        key=\"results\",\n        schema=MySchema,  # Validate against schema\n    )\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#parquet-file","level":3,"title":"Parquet File","text":"<pre><code>with tracker.start_run(\"log_parquet\", model=\"demo\"):\n    tracker.log_artifact(\n        Path(\"results.parquet\"),\n        key=\"raw_results\",\n        schema=MySchema,  # Schema is stored; call tracker.ingest(...) to ingest\n    )\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#csv-file","level":3,"title":"CSV File","text":"<pre><code>with tracker.start_run(\"log_csv\", model=\"demo\"):\n    tracker.log_artifact(\n        Path(\"results.csv\"),\n        key=\"csv_results\",\n        schema=MySchema,\n    )\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#zarr-netcdf-matrix-data","level":3,"title":"Zarr / NetCDF (Matrix Data)","text":"<pre><code># Zarr metadata is ingested as catalog (not raw data)\nwith tracker.start_run(\"log_zarr\", model=\"demo\"):\n    tracker.log_artifact(\n        Path(\"simulation_output.zarr\"),\n        key=\"gridded_results\",\n        driver=\"zarr\",\n    )\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#data-quality-error-handling","level":2,"title":"Data Quality &amp; Error Handling","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#schema-enforcement-fail-on-issues","level":3,"title":"Schema Enforcement (Fail on Issues)","text":"<p>When you provide a schema, Consist enforces the column set and types. Extra columns raise a <code>ValueError</code>, and type mismatches are surfaced during ingestion.</p> <pre><code>with tracker.start_run(\"strict_ingest\", model=\"demo\"):\n    tracker.log_dataframe(\n        df,\n        key=\"strict_results\",\n        schema=MySchema,\n    )\n</code></pre> <p>If you want best-effort ingestion (no strict schema), omit the schema and let DLT infer the structure.</p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#type-coercion","level":3,"title":"Type Coercion","text":"<p>DLT attempts to coerce types:</p> <pre><code>df = pd.DataFrame({\n    \"trip_id\": [\"1\", \"2\", \"3\"],      # Strings\n    \"distance\": [1.5, 2.2, 3.1],     # Floats\n})\n\n# Ingested as:\n# trip_id: [1, 2, 3]  (coerced to int)\n# distance: [1.5, 2.2, 3.1]  (floats)\n</code></pre> <p>To avoid surprises, ensure input DataFrame matches schema types:</p> <pre><code>df = df.astype({\n    \"trip_id\": \"int64\",\n    \"distance\": \"float64\",\n})\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#handling-missing-data","level":3,"title":"Handling Missing Data","text":"<pre><code>class Trip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True)\n    person_id: int\n    departure_hour: Optional[int]  # Can be NULL\n    arrival_hour: Optional[int]\n</code></pre> <p>Missing values in Optional fields → NULL in DB. Missing in required fields → error or default depending on the schema and ingestion behavior.</p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#duplicate-handling","level":3,"title":"Duplicate Handling","text":"<p>Primary keys are not enforced automatically in all cases. If you need uniqueness, deduplicate before ingestion:</p> <pre><code>class Household(SQLModel, table=True):\n    household_id: int = Field(primary_key=True)\n    size: int\n\n# If DataFrame has duplicate household_id:\ndf = pd.DataFrame({\n    \"household_id\": [1, 2, 1],  # Duplicate!\n    \"size\": [4, 3, 5],\n})\n</code></pre> <p>To handle duplicates, deduplicate before ingestion:</p> <pre><code>df = df.drop_duplicates(subset=[\"household_id\"], keep=\"last\")\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#provenance-columns","level":2,"title":"Provenance Columns","text":"<p>Consist automatically injects system columns during ingestion:</p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#available-columns","level":3,"title":"Available Columns","text":"Column Type Description <code>consist_run_id</code> str ID of the run that created this data <code>consist_artifact_id</code> str Artifact ID of the source file <code>consist_scenario_id</code> str Scenario ID (available in hybrid views) <code>consist_year</code> int Year (if provided to run context) <code>consist_iteration</code> int Iteration count (if provided)","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#example-query","level":3,"title":"Example Query","text":"<pre><code>from sqlmodel import Session, select, func\n\nVPerson = tracker.views.Person\nwith Session(tracker.engine) as session:\n    # Count persons per run\n    results = session.exec(\n        select(\n            VPerson.consist_run_id,\n            func.count(VPerson.person_id).label(\"count\")\n        ).group_by(VPerson.consist_run_id)\n    ).all()\n\n    for run_id, count in results:\n        print(f\"Run {run_id}: {count} persons\")\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#filtering-by-provenance","level":3,"title":"Filtering by Provenance","text":"<pre><code># Get persons from a specific scenario year\nwith Session(tracker.engine) as session:\n    persons_2030 = session.exec(\n        select(VPerson).where(\n            VPerson.consist_year == 2030,\n            VPerson.consist_scenario_id == \"baseline\"\n        )\n    ).all()\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#advanced-patterns","level":2,"title":"Advanced Patterns","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#multi-step-ingestion-scenario","level":3,"title":"Multi-Step Ingestion (Scenario)","text":"<pre><code>import consist\nfrom consist import Tracker, use_tracker\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person, Trip, Household],\n)\n\nwith use_tracker(tracker):\n    with consist.scenario(\"model_run_2030\", year=2030) as sc:\n\n        # Step 1: Load persons\n        with sc.trace(name=\"load_persons\"):\n            df_persons = load_population_data()\n            consist.log_dataframe(\n                df_persons,\n                key=\"population\",\n                schema=Person,\n            )\n\n        # Step 2: Simulate trips\n        with sc.trace(name=\"simulate_trips\"):\n            df_trips = run_trip_simulation(df_persons)\n            consist.log_dataframe(\n                df_trips,\n                key=\"trips\",\n                schema=Trip,\n            )\n\n        # Step 3: Aggregate\n        with sc.trace(name=\"aggregate\"):\n            # Query previous step\n            with Session(tracker.engine) as session:\n                total_trips = session.exec(\n                    select(func.count(Trip.trip_id)).where(\n                        Trip.consist_scenario_id == \"model_run_2030\"\n                    )\n                ).first()\n            print(f\"Total trips: {total_trips}\")\n</code></pre> <p>All data from steps 1-3 is queryable together:</p> <pre><code># Cross-step query\nwith Session(tracker.engine) as session:\n    results = session.exec(\n        select(Person, Trip).join(\n            Trip, Person.person_id == Trip.person_id\n        ).where(Trip.consist_scenario_id == \"model_run_2030\")\n    ).all()\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#incremental-ingestion-batches","level":3,"title":"Incremental Ingestion (Batches)","text":"<p>For very large datasets, ingest in batches:</p> <pre><code>from pathlib import Path\n\n# Split large file into chunks\nimport pandas as pd\n\nchunk_size = 100000\nfor i, chunk in enumerate(pd.read_csv(\"large_file.csv\", chunksize=chunk_size)):\n    consist.log_dataframe(\n        chunk,\n        key=f\"data_chunk_{i}\",\n        schema=MySchema,\n    )\n</code></pre> <p>DuckDB automatically unions these into a single table when the schema/table name is the same (for example, when you pass <code>schema=MySchema</code> each time):</p> <pre><code># Query all chunks together\nwith Session(tracker.engine) as session:\n    count = session.exec(select(func.count(MySchema.id))).first()\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#late-arriving-data","level":3,"title":"Late-Arriving Data","text":"<p>If you ingest data, then later find more data to add:</p> <pre><code># Run 1: Initial data\nconsist.log_dataframe(df1, key=\"data\", schema=MySchema)\n\n# Run 2: Additional data (same key, same schema)\nconsist.log_dataframe(df2, key=\"data\", schema=MySchema)\n</code></pre> <p>Both sets are ingested and queryable:</p> <pre><code>with Session(tracker.engine) as session:\n    # Both df1 and df2 are included\n    results = session.exec(select(MySchema)).all()\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#performance-tuning","level":2,"title":"Performance Tuning","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#batch-size","level":3,"title":"Batch Size","text":"<p>DLT loads data in batches. For large files, tune batch size:</p> <pre><code># In tracker initialization (future feature):\n# tracker = Tracker(..., dlt_batch_size=50000)\n\n# Or split manually:\nfor chunk in pd.read_csv(\"file.csv\", chunksize=50000):\n    consist.log_dataframe(chunk, key=\"data\", schema=MySchema)\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#indexing","level":3,"title":"Indexing","text":"<p>After ingestion, create indexes for frequently queried columns:</p> <pre><code># Manual index (in DuckDB):\nwith tracker.engine.begin() as conn:\n    conn.exec_driver_sql(\n        \"CREATE INDEX idx_person_run ON global_tables.person(consist_run_id)\"\n    )\n</code></pre>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#file-format-choice","level":3,"title":"File Format Choice","text":"<ul> <li>Parquet: Faster loading, better compression, typed columns → preferred</li> <li>CSV: Human-readable, larger files, slower parsing → use for interchange</li> </ul>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#deduplication","level":3,"title":"Deduplication","text":"<p>If your pipeline generates duplicate records, deduplicate before ingestion:</p> <pre><code>df = df.drop_duplicates(subset=[\"id\"])\n</code></pre> <p>Consist does not currently deduplicate automatically.</p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#common-errors","level":2,"title":"Common Errors","text":"","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#schema-mismatch","level":3,"title":"Schema Mismatch","text":"<pre><code>Error: Column 'age' expected int, got str\n</code></pre> <p>Fix: Ensure DataFrame types match schema: <pre><code>df[\"age\"] = df[\"age\"].astype(\"int64\")\n</code></pre></p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#missing-required-column","level":3,"title":"Missing Required Column","text":"<pre><code>Error: Column 'person_id' required but missing\n</code></pre> <p>Fix: Add column or make it optional: <pre><code>df[\"person_id\"] = df.index + 1  # Add column\n# OR\nclass MySchema(SQLModel, table=True):\n    person_id: Optional[int]  # Make optional\n</code></pre></p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#primary-key-violation","level":3,"title":"Primary Key Violation","text":"<pre><code>Error: Duplicate primary key value\n</code></pre> <p>Fix: Deduplicate before ingestion: <pre><code>df = df.drop_duplicates(subset=[\"id\"])\n</code></pre></p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#dlt-not-installed","level":3,"title":"DLT Not Installed","text":"<pre><code>ImportError: No module named 'dlt'\n</code></pre> <p>Fix: Install the ingest extras: <pre><code>pip install \"consist[ingest]\"\n</code></pre></p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#null-in-non-optional-field","level":3,"title":"Null in Non-Optional Field","text":"<pre><code>Warning: Null value in non-optional field 'age'\n</code></pre> <p>Fix (with schema enforcement): Ensure no nulls: <pre><code>df = df.dropna(subset=[\"age\"])\n\n# OR make optional:\nclass MySchema(SQLModel, table=True):\n    age: Optional[int]\n</code></pre></p>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#comparison-dlt-vs-direct-logging","level":2,"title":"Comparison: DLT vs Direct Logging","text":"Feature DLT Direct Logging Schema validation ✅ Yes ❌ No Cross-run SQL queries ✅ Yes ❌ No Type enforcement ✅ Yes ❌ No Setup overhead ⚠️ Moderate ✅ Minimal Best for Analytics, large data Simple results Example use case Population, trips table Single analysis output","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#see-also","level":2,"title":"See Also","text":"<ul> <li>Data Materialization Strategy</li> <li>Schema Export</li> <li>Architecture: Data Virtualization</li> <li>dlt Documentation</li> </ul>","path":["Guides","DLT Loader Integration Guide"],"tags":[]},{"location":"examples/","level":1,"title":"Examples","text":"<p>Each notebook is runnable top-to-bottom and writes outputs under <code>examples/runs/</code>.</p>","path":["Examples"],"tags":[]},{"location":"examples/#quickstart","level":2,"title":"Quickstart","text":"<p>00_quickstart.ipynb</p> Learning outcome Create a tracked run Log artifacts Query run history","path":["Examples"],"tags":[]},{"location":"examples/#tutorial-series","level":2,"title":"Tutorial Series","text":"<p>01_parameter_sweep_monte_carlo.ipynb</p> Learning outcome Run parameter sweeps with provenance Export schemas to SQLModel stubs Query across runs with typed views <p>02_iterative_workflows.ipynb</p> Learning outcome Build scenario workflows with feedback loops Query provenance across iterations Use couplers to pass artifacts between steps <p>03_transportation_demand_modeling.ipynb</p> Learning outcome Build end-to-end modeling pipelines Compare scenarios with cached intermediate steps Trace lineage through multi-step workflows","path":["Examples"],"tags":[]},{"location":"examples/#prerequisites","level":2,"title":"Prerequisites","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>For notebook structure and helper modules, see examples/README.md.</p>","path":["Examples"],"tags":[]},{"location":"faq/","level":1,"title":"Frequently Asked Questions (FAQ)","text":"","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#caching-performance","level":2,"title":"Caching &amp; Performance","text":"","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#why-is-my-cache-not-hitting","level":3,"title":"Why is my cache not hitting?","text":"<p>Cache misses occur when the Signature changes. Check if: - You have uncommitted code changes (your Git repo is \"dirty\"). - The input file content has changed (even a tiny metadata change can trigger a re-hash). - You are passing a non-deterministic config (e.g., a dict with a timestamp or a random seed). - Your function code changed.</p> <p>Use <code>CONSIST_CACHE_DEBUG=1</code> to log detailed signature components and identify exactly what changed.</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#can-i-use-consist-without-git","level":3,"title":"Can I use Consist without Git?","text":"<p>Yes. If Git is not found, Consist falls back to a static <code>unknown_code_version</code>. However, this means code changes won't automatically invalidate your cache. We recommend using Git for full reproducibility.</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#does-consist-work-with-massive-files","level":3,"title":"Does Consist work with massive files?","text":"<p>Yes. Consist tracks files as Artifacts. It stores the path and hash but doesn't necessarily copy the bytes into the database unless you call <code>tracker.ingest()</code>. For multi-GB files, we recommend keeping them as \"Cold Data\" (on disk) and using <code>cache_hydration=\"metadata\"</code> (the default).</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#data-storage","level":2,"title":"Data &amp; Storage","text":"","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#where-are-my-results-stored","level":3,"title":"Where are my results stored?","text":"<p>Results are stored in the <code>run_dir</code> you provided when initializing the <code>Tracker</code>. Each run gets its own subdirectory containing a <code>consist.json</code> snapshot and any output files you logged.</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#can-i-share-my-provenance-database-with-a-colleague","level":3,"title":"Can I share my provenance database with a colleague?","text":"<p>Yes. If you share your DuckDB file and your <code>run_dir</code>, your colleague can query your results. If you use Mounts, they can resolve the same portable URIs to their local filesystem paths. See Mounts &amp; Portability.</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#how-do-i-reclaim-disk-space","level":3,"title":"How do I reclaim disk space?","text":"<p>You can safely delete intermediate output files if they have been ingested into DuckDB. Consist's Ghost Mode allows you to recover the data from the database or re-run the pipeline to regenerate the files.</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#integration","level":2,"title":"Integration","text":"","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#does-consist-support-r-or-java","level":3,"title":"Does Consist support R or Java?","text":"<p>Consist is a Python library, but you can track any external tool using the Container Integration or by wrapping a subprocess call in a Python function. As long as you can define the inputs and capture the outputs, Consist can track it.</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"faq/#how-does-this-compare-to-dvc-or-mlflow","level":3,"title":"How does this compare to DVC or MLflow?","text":"<ul> <li>DVC: Focuses on large file versioning and pipeline orchestration. Consist focuses on fine-grained provenance and SQL-native analytics via DuckDB.</li> <li>MLflow: Focuses on experiment tracking and model deployment. Consist focuses on scientific simulation lineage and data handoffs between complex model steps.</li> </ul> <p>Consist is designed specifically for \"heavy\" scientific workflows where data integrity and SQL queryability are paramount.</p>","path":["Frequently Asked Questions (FAQ)"],"tags":[]},{"location":"glossary/","level":1,"title":"Glossary","text":"<p>This page defines key terms used in Consist documentation. For a conceptual introduction with dependency ordering, see Core Concepts Overview.</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#artifact","level":2,"title":"Artifact","text":"<p>A file (CSV, Parquet, HDF5, etc.) produced or used by a run, with metadata attached to track its origin and integrity. Think of it as a \"named, tracked file\" that Consist remembers who created it and can verify hasn't been corrupted.</p> <p>Artifacts record: - The file path and format (CSV, Parquet, HDF5, etc.) - Which run created or used it (for inputs) - A content hash (SHA256) for integrity checking - Optional ingestion status (whether it was stored in DuckDB)</p> <p>Research example: When you publish results, each output file is an Artifact. You can trace any file back to the exact code version and config that produced it using the CLI. See Usage Guide for more.</p> <p>See also: Run, Provenance, Ingestion</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#cache-hit","level":2,"title":"Cache Hit","text":"<p>When Consist skips execution because it finds a previous run with an identical signature. The cached results (artifact metadata and optionally file copies) are returned instantly.</p> <p>Example: You run a function with config <code>{\"threshold\": 0.5}</code> on input <code>data.csv</code>, then re-run with the exact same config and input. Second execution is a cache hit—no computation happens.</p> <p>Research example: In a parameter sweep, Consist skips preprocessing steps that haven't changed, saving hours of compute time. See Usage Guide for realistic time-saving scenarios.</p> <p>Opposite: Cache miss (run must execute)</p> <p>See also: Signature, Cache Miss</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#cache-miss","level":2,"title":"Cache Miss","text":"<p>When Consist cannot find a matching cached result, so the function must execute. The outputs are recorded as a new run and new artifacts are created.</p> <p>See also: Cache Hit, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#canonical-hashing","level":2,"title":"Canonical Hashing","text":"<p>Converting configuration data (dicts, YAML, etc.) into a single, consistent fingerprint, regardless of field order or how numbers are formatted. This ensures <code>{\"a\": 1, \"b\": 2}</code> and <code>{\"b\": 2, \"a\": 1}</code> produce the same hash, so Consist treats them as identical configurations.</p> <p>Why it matters: Without canonical hashing, the same config in different orders would produce different cache keys, breaking reproducibility.</p> <p>See also: Signature, Config</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#config","level":2,"title":"Config","text":"<p>Dictionary of parameters that affect computation, and are included in the cache signature. Example:</p> <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=my_model,\n        config={\"year\": 2030, \"scenario\": \"baseline\"},  # Part of signature\n        inputs={...},\n        outputs=[...]\n    )\n</code></pre> <p>Changing config invalidates cache and triggers re-execution. Config is hashed (not stored as-is) to allow large nested dictionaries.</p> <p>Research example: If a colleague changes a mode choice coefficient in a 50MB parameter file, Consist detects the change and automatically knows to re-run the affected demand model.</p> <p>See also: Facet, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#coupler","level":2,"title":"Coupler","text":"<p>A Consist pattern for orchestrating multi-step workflows. A coupler passes artifacts from one step's outputs to the next step's inputs, linking them through scenario trees. Useful for complex loops and iterative computations.</p> <p>Example: In a transportation model with feedback loops (trip distribution → mode choice → assignment → congestion update), couplers link each step's outputs to the next step's inputs.</p> <p>See also: Scenario, Run, Trace</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#dlt-data-load-tool","level":2,"title":"DLT (Data Load Tool)","text":"<p>An optional Python library for loading data into data warehouses. Consist integrates with DLT to materialize artifacts into DuckDB with provenance columns (run_id, artifact_id, etc.). Using DLT is optional; you can ingest manually or load files directly.</p> <p>See also: Ingestion, Materialization</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#facet","level":2,"title":"Facet","text":"<p>A small, queryable subset of configuration (e.g., <code>{\"year\": 2030, \"scenario\": \"baseline\"}</code>) that's indexed in DuckDB so you can filter runs. Use facets when you want to ask \"show me all runs where year=2030\" without storing the entire 50MB config file.</p> <p>Unlike identity config (which is hashed into the cache key), facets are stored queryably in the database for filtering and analysis.</p> <p>Use case: Your config is a 10 MB YAML file (too large to store). You extract <code>{\"year\": 2030, \"parking_cost\": 5.0}</code> as facets and query runs by year/parking_cost.</p> <p>Example: <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=my_model,\n        config={\"huge_model_config\": ...},  # Not queryable directly (too large)\n        facet={\"year\": 2030, \"scenario\": \"baseline\"},  # Indexed and queryable\n        inputs={...},\n        outputs=[...]\n    )\n\n# Later: query all 2030 runs\ndf = tracker.find_runs(facet_year=2030)\n</code></pre></p> <p>Research example: In a multi-year study, you can set <code>facet={\"year\": 2030, \"scenario\": \"transit-friendly\"}</code>. This allows you to instantly find all 2030 sensitivity tests without searching through directories.</p> <p>See also: Config, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#ghost-mode","level":2,"title":"Ghost Mode","text":"<p>The ability to recover artifacts that exist only in the provenance database, not as physical files. If an artifact was ingested, Consist can recover the data from DuckDB even if the original file was deleted. See Architecture: Ghost Mode for details.</p> <p>See also: Ingestion, Materialization</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#hydration","level":2,"title":"Hydration","text":"<p>Recovering the metadata and location information about a previous run's output without copying the file bytes. On a cache hit, Consist \"hydrates\" the output artifact so you know where it came from and can access it, but doesn't necessarily copy it to your current run directory.</p> <p>Hydration ≠ copying files. A hydrated artifact has provenance metadata but may not have file bytes copied to the new run's directory. By default, Consist recovers the information but doesn't copy files (saves disk space). You opt in to copying files when needed.</p> <p>See also: Materialization, Cache Hit</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#identity-config","level":2,"title":"Identity Config","text":"<p>The full set of configuration parameters that affect a run's cache signature; if identity config changes, the run must re-execute. Unlike facets (which are just for querying), identity config is hashed into the cache key to ensure cache invalidation when parameters change.</p> <p>Example: If your <code>config</code> dict contains <code>{\"year\": 2030, \"mode_choice_coefficient\": 0.5}</code>, both values are hashed into the signature. Changing either value invalidates cache.</p> <p>See also: Config, Facet, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#ingestion","level":2,"title":"Ingestion","text":"<p>Loading artifact data into DuckDB for SQL-native analysis. Optional; you can use Consist without ingesting (just track files).</p> <p>Use case: You want to query 50 Parquet files across 50 runs in SQL without loading them all into memory.</p> <p>Process: 1. Artifact is created/logged by a run 2. <code>tracker.ingest(artifact, data=df)</code> stores the data in DuckDB 3. Later: query in SQL across all ingested data</p> <p>See also: Materialization, Ghost Mode, Hybrid View</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#lineage","level":2,"title":"Lineage","text":"<p>The complete dependency chain showing where a result came from. Lineage tracks: which run created an artifact, which inputs that run used, which runs created those inputs, etc.</p> <p>Example: <code>consist lineage traffic_volumes</code> shows: <pre><code>traffic_volumes (artifact)\n├── created by: traffic_simulation run\n│   ├── input: assigned_trips\n│   │   └── created by: assignment run\n│   │       └── input: trip_tables\n│   │           └── created by: mode_choice run\n│   │               └── ...\n</code></pre></p> <p>See also: Provenance, Run, Artifact</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#merkle-dag","level":2,"title":"Merkle DAG","text":"<p>A chain of computations where each step's inputs are linked to the outputs of previous steps, creating an unbreakable record of data lineage. Like a railroad consist (the specific order of locomotives and cars), each simulation year depends on the previous year's output.</p> <p>Why it matters: This structure enables Consist to detect when cached results are valid (all upstream inputs haven't changed) and to recover missing data if needed.</p> <p>See also: Signature, Lineage, Artifact</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#materialization","level":2,"title":"Materialization","text":"<p>Saving the actual bytes of a data file into DuckDB (the provenance database) so it's recoverable even if the original file gets deleted. If you ingest a 10GB result file, DuckDB stores a copy so you can retrieve it later without the original file.</p> <p>Materialization = copying bytes into the database Hydration = recovering metadata without bytes</p> <p>See also: Hydration, Ingestion, Ghost Mode</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#provenance","level":2,"title":"Provenance","text":"<p>Complete history of where a result came from: code version, configuration, input data, and compute environment. Consist records provenance automatically for every run.</p> <p>Why it matters: Reproducibility (\"Can I re-run this exactly?\"), Accountability (\"Which config made this figure?\"), Debugging (\"Why did this change?\")</p> <p>Research example: If a policy maker asks which assumptions led to a specific forecast, you can use provenance to identify the exact code version, zoning policy, and parcel data used. You can then reproduce it exactly or change one parameter to show the impact.</p> <p>See also: Artifact, Lineage, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#run","level":2,"title":"Run","text":"<p>A single execution of a tracked function or workflow step. A run records: - Input artifacts and configuration - Execution status (completed, failed, cached) - Output artifacts - Timing (start/end times) - Tags and metadata</p> <p>Example: <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=prepare_load_shapes,\n        inputs={\"raw_path\": \"hourly_demand.csv\"},\n        config={\"peak_shave_threshold\": 0.95},\n        outputs=[\"processed_load\"],\n    )\n</code></pre></p> <p>This creates a Run with one input artifact, one config dict, and one output artifact.</p> <p>Research example: In grid modeling, each annual simulation is a Run. You can query \"what was the total compute time across all high-load scenarios?\" or trace a reliability violation back to original forecast assumptions.</p> <p>See also: Artifact, Scenario</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#scenario","level":2,"title":"Scenario","text":"<p>A grouping of related runs. Scenarios are useful for organizing multi-variant studies or iterative workflows.</p> <p>Example: \"baseline_2030\" scenario contains 5 related runs: - Year 2030, baseline policy, iteration 0 - Year 2030, baseline policy, iteration 1 - Year 2030, baseline policy, iteration 2 - ...</p> <p>Important: Consist uses \"scenario\" differently from policy modeling jargon. In Consist, a scenario is a parent run grouping; in transportation modeling, \"baseline scenario\" and \"growth scenario\" are policy variants. Don't confuse the two.</p> <p>See also: Run, Coupler, Trace</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#signature","level":2,"title":"Signature","text":"<p>A unique fingerprint of a run created by hashing together your code version, configuration parameters, and input data. It's like a \"run ID\" that Consist compares across executions to detect when the same inputs, code, and config have been run before.</p> <p>How it works: 1. Function code is hashed (git commit SHA + modified files) 2. Config dict is hashed deterministically (canonical hashing) 3. Input file hashes are computed 4. All three are combined: <code>signature = SHA256(code + config + inputs)</code> 5. This signature is the cache key</p> <p>Why: If you re-run with the same signature, Consist knows the result will be the same, so it returns the cached version instantly. Identical signatures = identical outputs (assuming deterministic functions).</p> <p>See also: Cache Hit, Config, Artifact, Canonical Hashing</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#trace","level":2,"title":"Trace","text":"<p>The execution path through a multi-step workflow, showing which runs were executed, which were cache hits, and what artifacts were passed between them.</p> <p>See also: Scenario, Coupler, Lineage</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#virtualization-data-virtualization","level":2,"title":"Virtualization (Data Virtualization)","text":"<p>Querying multiple artifacts as if they were a single table, without loading all data into memory. DuckDB handles data movement lazily.</p> <p>Example: Query 50 Parquet files across 50 runs: <pre><code>SELECT year, mode, COUNT(*) as trips\nFROM consist_view_trips\nWHERE scenario IN ('baseline', 'high_growth')\nGROUP BY year, mode\n</code></pre></p> <p>Consist creates a virtual SQL view that queries each file as needed, not loading all at once.</p> <p>See also: Hybrid View, Ingestion</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#hybrid-view","level":2,"title":"Hybrid View","text":"<p>A SQL view that combines: 1. Hot data: Ingested artifacts stored in DuckDB 2. Cold data: Raw files (Parquet, CSV) queried on-the-fly</p> <p>Hybrid views let you query across runs without requiring all data to be ingested, reducing storage overhead.</p> <p>See also: Ingestion, Virtualization</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"ingestion-and-hybrid-views/","level":1,"title":"Moved","text":"<p>This page has been reorganized. See Data Materialization Strategy for the new location.</p> <p>Redirect</p>","path":["Moved"],"tags":[]},{"location":"mounts-and-portability/","level":1,"title":"Mounts &amp; Portability","text":"<p>Consist stores portable URIs instead of absolute filesystem paths so runs can move between machines without breaking lineage. This page explains how mounts, workspace URIs, and historical path resolution work.</p>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#getting-started-with-mounts","level":2,"title":"Getting Started with Mounts","text":"<p>When you set up Consist for your research project, you'll define mounts — mappings from short names to directories on your filesystem. These mounts let each team member keep their data in different locations while sharing a common provenance database.</p> <p>The Workflow</p> <ol> <li>Identify your data directories: Where are your inputs? Where do you write outputs? Where is temporary scratch space?</li> <li>Assign mount names: Choose memorable names like <code>inputs</code>, <code>outputs</code>, <code>scratch</code>, <code>shared</code>.</li> <li>Define mounts in your Tracker: Map those names to real paths on each person's machine.</li> <li>Log artifacts under mounts: Instead of absolute paths, use URIs like <code>inputs://land_use.csv</code>.</li> </ol> <p>Example for a Research Team</p> <p>Suppose your team shares an ActivitySim project:</p> <pre><code>from consist import Tracker\nfrom pathlib import Path\n\n# Shared setup (agreed upon by the team)\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/shared/data/activitysim_inputs\",      # Shared NFS mount\n        \"outputs\": \"/local/activitysim_outputs\",          # Local SSD for speed\n        \"scratch\": \"/scratch/users/YOUR_USERNAME\",        # Temporary workspace\n    },\n)\n</code></pre> <p>On each team member's machine, the paths differ but mount names stay the same:</p> <pre><code># Alice's setup\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/mnt/nfs/activitysim_inputs\",          # Alice's NFS mount point\n        \"outputs\": \"/home/alice/activitysim_outputs\",\n        \"scratch\": \"/scratch/alice\",\n    },\n)\n\n# Bob's setup\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/data/nfs/inputs\",                     # Bob's mount point\n        \"outputs\": \"/var/cache/bob/outputs\",\n        \"scratch\": \"/tmp/bob_scratch\",\n    },\n)\n</code></pre> <p>When Alice runs a simulation and logs an output (inside a run context):</p> <pre><code>with tracker.start_run(\"asim_baseline\", model=\"activitysim\"):\n    consist.log_artifact(\n        Path(\"/home/alice/activitysim_outputs/results.parquet\"),\n        key=\"results\",\n        direction=\"output\",\n    )\n</code></pre> <p>Consist detects the mount and stores a portable URI:</p> <pre><code>outputs://results.parquet\n</code></pre> <p>When Bob retrieves this artifact, Consist resolves it using his mount configuration:</p> <pre><code>outputs:// → /var/cache/bob/outputs/ → /var/cache/bob/outputs/results.parquet\n</code></pre> <p>Benefits</p> <ul> <li>Portability: Runs move between machines without breaking lineage.</li> <li>Flexibility: Each user can customize their local paths for their hardware (local SSD, NFS, cloud storage).</li> <li>Shared provenance: A single <code>provenance.duckdb</code> file shared across the team still works even if underlying filesystems differ.</li> <li>Isolation: Temporary outputs stay local; shared data stays on shared infrastructure.</li> </ul>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#mounts-at-a-glance","level":2,"title":"Mounts at a glance","text":"<p>Mounts map a short scheme name to a real path on disk:</p> <pre><code>from consist import Tracker\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/shared/inputs\",\n        \"scratch\": \"/scratch/users/MY_USERNAME\",\n    },\n)\n</code></pre> <p>When you log a path under a mount, Consist stores a URI such as:</p> <pre><code>inputs://land_use.csv\nscratch://temp/output.parquet\n</code></pre> <p>This keeps provenance portable and lets each user remap mounts on their machine.</p>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#workspace-uris-run-local-outputs","level":2,"title":"Workspace URIs (run-local outputs)","text":"<p>Paths under the run directory are stored relative to the active run:</p> <pre><code>./outputs/&lt;run_id&gt;/model.csv\n</code></pre> <p>Consist resolves these using the run's <code>_physical_run_dir</code> metadata field, which records the absolute run directory at execution time.</p> Scenario Behavior Current run directory matches original Files accessible Run directory moved Metadata-only cache hits work; file access fails Run directory deleted Metadata-only cache hits work; file access fails <p><code>_physical_run_dir</code></p> <p>Stored in <code>run.meta[\"_physical_run_dir\"]</code>. Used for historical path resolution when hydrating artifacts from prior runs.</p>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#historical-path-resolution","level":2,"title":"Historical path resolution","text":"<p>When Consist needs bytes from a historical run (e.g., cache hydration or <code>inputs-missing</code>), it resolves paths in this order:</p> <p>1) If the URI uses <code>workspace://</code> or <code>./</code>, resolve relative to the original run’s    <code>_physical_run_dir</code>. 2) If the URI uses a mount scheme (e.g., <code>inputs://</code>), resolve using the current    tracker mounts. 3) Otherwise, treat the URI as an absolute path.</p> <p>If a mount is missing or points somewhere else, materialization will warn and skip missing files rather than crashing (unless explicitly set to raise).</p>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#sharing-a-database-across-machines","level":2,"title":"Sharing a database across machines","text":"<p>Sharing a DuckDB provenance file across a team is supported, but you must keep mounts consistent in intent even if the physical paths differ.</p> <p>Recommended practice: - Agree on mount names (<code>inputs</code>, <code>outputs</code>, <code>scratch</code>, <code>shared</code>). - Each user maps those names to their local filesystem. - Store the DB in a shared location with write access controls.</p> <p>If a user hits a cache hit but cannot access the source filesystem, Consist will log a warning and proceed without materializing the files.</p>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#best-practices","level":2,"title":"Best practices","text":"<ul> <li>Prefer mounts for shared data directories; avoid absolute paths in artifacts.</li> <li>Keep run directories local and disposable; treat cached outputs as rehydratable.</li> <li>Use <code>cache_hydration=\"outputs-requested\"</code> for only the outputs you need.</li> <li>Use <code>cache_hydration=\"inputs-missing\"</code> to backfill inputs when a run moves   across machines or directories.</li> </ul>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#troubleshooting","level":2,"title":"Troubleshooting","text":"<ul> <li>Missing file on cache hit: Check that mounts map to the correct root and the   original run directory still exists for workspace URIs.</li> <li>Moved run directory: Cache metadata is still valid, but byte materialization   will warn because <code>_physical_run_dir</code> no longer points to the original location.</li> <li>Permission denied: Consist warns and continues; adjust mount permissions or   use a shared accessible path for cached outputs you need to materialize.</li> </ul>","path":["Guides","Mounts &amp; Portability"],"tags":[]},{"location":"schema-export/","level":1,"title":"Schema Export (SQLModel Stubs)","text":"<p>Consist can capture the observed (post-ingest) schema of tabular artifacts and export that schema as a static SQLModel class stub you can commit and edit. This reduces “retype the table definition” friction while keeping Consist honest about what it can and cannot infer.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#what-you-get","level":2,"title":"What You Get","text":"<ul> <li>A Python 3.11 file containing one SQLModel class stub (exported as <code>table=True</code>).</li> <li>Columns with:</li> <li>a best-effort type mapping from DuckDB logical types</li> <li>nullability (<code>Optional[...] = None</code> only when nullable)</li> <li>deterministic ordering (prefers <code>ordinal_position</code>)</li> <li>“Hints” as comments (enums/stats) when available (on by default).</li> <li>Messy column names are handled: the generated attribute name is sanitized, but the original DB column name is preserved via <code>Field(sa_column=Column(\"original\", ...))</code> when needed.</li> <li>Foreign keys are preserved if they were provided in a curated SQLModel schema (rendered as <code>Field(foreign_key=\"table.column\")</code>).</li> </ul>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#abstract-vs-concrete-exports","level":2,"title":"Abstract vs Concrete Exports","text":"<p>By default, Consist exports SQLModel classes as abstract (<code>__abstract__ = True</code>). This keeps the stub importable immediately, because SQLAlchemy requires a primary key to map a concrete table/view, and many analysis tables don’t have an obvious primary key.</p> <ul> <li>Default (recommended): abstract export, safe to import, great for view schemas.</li> <li>Concrete export (advanced): export with <code>--concrete</code> (or <code>abstract=False</code> in Python) and then add a primary key before importing, otherwise SQLAlchemy will raise an error like “could not assemble any primary key columns”.</li> </ul>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#what-you-still-do-manually","level":2,"title":"What You Still Do Manually","text":"<p>Consist does not infer semantic intent. You typically edit the stub to add:</p> <ul> <li>primary keys / foreign keys</li> <li>indexes / uniqueness constraints</li> <li>relationships</li> <li>renames / normalization decisions</li> </ul> <p>If you pass a curated SQLModel schema to <code>log_artifact(..., schema=YourModel)</code> and it includes <code>foreign_key=...</code>, those FKs are persisted and will be re-emitted during export.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#prerequisites","level":2,"title":"Prerequisites","text":"<p>Schema export uses the schema captured from a DuckDB-ingested table. In practice:</p> <ol> <li>You run with a database configured (<code>db_path=...</code>).</li> <li>You ingest tabular data into DuckDB (hot data).</li> <li>Consist profiles the ingested table and stores a deduped schema referenced by <code>artifact.meta[\"schema_id\"]</code>.</li> </ol> <p>If you already see <code>schema_id</code> in an artifact’s <code>meta</code>, you’re ready to export.</p> <p>You can also capture schemas without ingestion for CSV/Parquet artifacts by enabling lightweight file profiling at log time (<code>profile_file_schema=True</code>, optional <code>file_schema_sample_rows=</code>). This writes the same <code>schema_id</code> pointer into <code>artifact.meta</code>, allowing schema export even if the original file is later missing or moved.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#exporting-from-the-cli","level":2,"title":"Exporting from the CLI","text":"<p>Export by artifact UUID (recommended UX for now):</p> <pre><code>python -m consist.cli schema export \\\n  --artifact-id 00000000-0000-0000-0000-000000000000 \\\n  --out your_pkg/models/persons.py\n</code></pre> <p>Or export by schema id (hash):</p> <pre><code>python -m consist.cli schema export \\\n  --schema-id &lt;schema-hash&gt; \\\n  --out your_pkg/models/persons.py\n</code></pre> <p>Useful flags:</p> <ul> <li><code>--class-name Persons</code> to override the generated class name</li> <li><code>--table-name persons</code> to override <code>__tablename__</code></li> <li><code>--include-system-cols</code> to include system/ingestion columns like <code>consist_*</code> and <code>_dlt_*</code></li> <li><code>--no-stats-comments</code> to omit stats/enum hint comments</li> <li><code>--concrete</code> to export a non-abstract mapped class (you must add a primary key)</li> </ul> <p>If <code>--out</code> is omitted, the stub is printed to stdout (so it can be piped or redirected).</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#exporting-from-python","level":2,"title":"Exporting from Python","text":"<p>You can generate the code (and optionally write it) from a <code>Tracker</code>:</p> <pre><code>from pathlib import Path\nfrom consist import Tracker\n\ntracker = Tracker(run_dir=\".\", db_path=\"provenance.duckdb\")\n\ncode = tracker.export_schema_sqlmodel(\n    artifact_id=\"00000000-0000-0000-0000-000000000000\",\n    out_path=Path(\"your_pkg/models/persons.py\"),\n    abstract=True,\n)\n</code></pre> <p>The method returns the generated string regardless of whether you write it.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#besteffort-foreign-key-enforcement","level":2,"title":"Best‑Effort Foreign Key Enforcement","text":"<p>If you want DuckDB to attempt to enforce persisted foreign keys, run:</p> <pre><code>python -m consist.cli schema apply-fks\n</code></pre> <p>This step is best‑effort: if a constraint can’t be added (e.g., due to missing parent rows), Consist logs a warning and continues.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#where-to-put-the-generated-file","level":2,"title":"Where to Put the Generated File","text":"<p>Place it anywhere importable by your project (on <code>PYTHONPATH</code>), for example:</p> <ul> <li><code>your_pkg/models/</code></li> <li><code>your_pkg/schemas/</code></li> </ul> <p>If it’s a package directory, include an <code>__init__.py</code> so you can <code>import your_pkg.models.persons</code>.</p> <p>Consist does not require a special directory layout; it only needs you to import the class and register it for views.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#using-the-generated-model-with-views","level":2,"title":"Using the Generated Model with Views","text":"<p>Consist’s hybrid views are created from your SQLModel schema. To activate a view:</p> <p>1) Import the model class. 2) Register it with the <code>Tracker</code> (at init time or after).</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#register-at-tracker-initialization","level":3,"title":"Register at Tracker initialization","text":"<pre><code>from consist import Tracker\nfrom your_pkg.models.persons import Persons\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\", schemas=[Persons])\nVPersons = tracker.views.Persons\n</code></pre>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#register-later","level":3,"title":"Register later","text":"<pre><code>from your_pkg.models.persons import Persons\n\nVPersons = tracker.view(Persons)  # registers + creates/refreshes the hybrid view\n</code></pre>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#querying","level":3,"title":"Querying","text":"<pre><code>from sqlmodel import select\nimport consist\n\nVPersons = tracker.views.Persons\nrows = consist.run_query(select(VPersons).limit(5), tracker=tracker)\n</code></pre> <p>Views automatically include Consist metadata columns (e.g. <code>consist_run_id</code>, <code>consist_year</code>, <code>consist_scenario_id</code>) for filtering and grouping.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#column-name-rules-important","level":2,"title":"Column Name Rules (Important)","text":"<p>The generated stub aggressively normalizes attribute names to reduce runtime errors:</p> <ul> <li>invalid characters become <code>_</code></li> <li>leading digits are prefixed with <code>_</code></li> <li>Python keywords get a trailing <code>_</code></li> <li>collisions become <code>__2</code>, <code>__3</code>, ...</li> </ul> <p>When the attribute name differs from the real column name, the stub uses:</p> <pre><code>mass_kg: float | None = Field(default=None, sa_column=Column(\"Mass(kg)\", ...))\n</code></pre> <p>This matters for views and empty-view typing: Consist now uses the original DB column name when creating typed empty views, so schemas with renamed attributes still behave correctly.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#matching-artifacts-tables-and-__tablename__","level":2,"title":"Matching Artifacts, Tables, and <code>__tablename__</code>","text":"<p>For views to “pick up” your data, the model’s <code>__tablename__</code> should match the concept key you are querying:</p> <ul> <li>for ingested (“hot”) data, this is typically the DuckDB table name used at ingest time</li> <li>for file-based (“cold”) data, this is typically the <code>Artifact.key</code></li> </ul> <p>If you want your curated model name to differ from the ingestion key, you can:</p> <ul> <li>override <code>--table-name</code> / <code>table_name=...</code> during export, or</li> <li>set <code>__tablename__ = \"...\"</code> manually after editing</li> </ul>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#system-and-ingestion-columns","level":2,"title":"System and Ingestion Columns","text":"<p>By default, exported stubs omit system/ingestion columns:</p> <ul> <li><code>consist_*</code> (Consist provenance columns)</li> <li><code>_dlt_*</code> (common ingestion/system columns)</li> </ul> <p>Use <code>--include-system-cols</code> if you need a faithful “as-ingested” representation.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#notes-on-wide-sparse-tables","level":2,"title":"Notes on Wide / Sparse Tables","text":"<p>Very wide tables can cause the stored JSON profile to truncate. Consist still persists per-field rows so schema export continues to work, and emits a warning when truncation occurs.</p> <p>If your upstream workflow produces sparse wide tables, consider reshaping to a long format before ingestion (or as a dedicated transformation step). Future Consist work may add first-class “pre/post ingestion transforms”, but schema export is designed to work even when the JSON blob is truncated.</p>","path":["Guides","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"troubleshooting/","level":1,"title":"Troubleshooting Guide","text":"<p>This guide organizes issues by symptom. For concept definitions, see Core Concepts. For topic-specific guides, see:</p> <ul> <li>Container Integration</li> <li>DLT Loader</li> <li>Mounts &amp; Portability</li> </ul>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#cache-provenance-issues","level":2,"title":"Cache &amp; Provenance Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#relation-leak-warnings","level":3,"title":"\"Relation leak warnings\"","text":"<p>Symptom: Warning: <code>Consist has N active DuckDB relations...</code></p> <p>Root Cause: Relations returned by <code>consist.load(...)</code> (tabular artifacts) keep a DuckDB connection open until you close them.</p> <p>Solution:</p> <ul> <li>Prefer <code>consist.load_df(...)</code> if you only need a pandas DataFrame.</li> <li>Use <code>consist.load_relation(...)</code> as a context manager to ensure connections are closed.</li> <li>If you're intentionally holding many Relations, increase the warning threshold:   <code>CONSIST_RELATION_WARN_THRESHOLD=500</code>.</li> </ul>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#old-dbs-no-longer-load-after-the-relation-first-refactor","level":3,"title":"\"Old DBs no longer load after the Relation-first refactor\"","text":"<p>Symptom: Errors when reading artifacts or querying the DB after upgrading.</p> <p>Root Cause: The artifact schema changed: - <code>Artifact.uri</code> → <code>Artifact.container_uri</code> - <code>Artifact.table_path</code> added (nullable) for container formats (HDF5 tables) - <code>Artifact.array_path</code> added (nullable) for array formats - <code>meta[\"table_path\"]</code> is no longer used</p> <p>Solution:</p> <p>Reset your Consist database(s) and re-run workflows:</p> <pre><code>rm ./provenance.duckdb\nrm ./test_db.duckdb\n</code></pre> <p>Then update any code that referenced <code>artifact.uri</code> or <code>artifact.meta[\"table_path\"]</code>:</p> <pre><code># Before\nartifact.uri\nartifact.meta.get(\"table_path\")\n\n# After\nartifact.container_uri\nartifact.table_path\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#cache-hit-but-output-files-are-missing","level":3,"title":"\"Cache hit but output files are missing\"","text":"<p>Symptom: <code>cache_hit=True</code> but <code>artifact.path</code> doesn't exist on disk.</p> <p>Root Cause: Consist returned a cache hit but didn't materialize the files to disk.</p> <p>Why this happens: Consist defaults to metadata-only cache hits to keep cache checks fast and avoid duplicating large files. You explicitly opt in to file copying via hydration/materialization when you need bytes on disk.</p> <p>Solution:</p> <p>Use cache hydration to copy files:</p> <pre><code>result = consist.run(\n    fn=my_function,\n    inputs={...},\n    cache_hydration=\"outputs-all\",  # Copy all cached outputs\n    ...\n)\n</code></pre> <p>Or materialize manually:</p> <pre><code>from pathlib import Path\n\nresult = consist.run(...)\nif result.cache_hit:\n    from consist.core.materialize import materialize_artifacts\n    artifacts_to_load = [\n        (artifact, Path(tracker.run_dir) / \"rehydrated\" / artifact.path.name)\n        for artifact in result.outputs.values()\n    ]\n    materialize_artifacts(tracker, artifacts_to_load)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#same-inputsconfig-but-cache-not-found","level":3,"title":"\"Same inputs/config but cache not found\"","text":"<p>Symptom: Code hasn't changed, inputs haven't changed, but run re-executes instead of hitting cache.</p> <p>Root Cause: Signature mismatch. Something in the cache key changed.</p> <p>Solution:</p> <p>Debug the signature:</p> <pre><code>from pathlib import Path\n\nidentity = tracker.identity\ncode_hash = identity.get_code_version()\n# If you want to match Consist's exact run hash, include model/year/iteration:\n# config_hash = identity.compute_run_config_hash(config={\"param\": value}, model=\"my_model\", year=2030)\nconfig_hash = identity.compute_config_hash({\"param\": value})\ninput_hash = identity.compute_file_checksum(Path(\"input.csv\"))\n\nprint(f\"Code: {code_hash}\")\nprint(f\"Config: {config_hash}\")\nprint(f\"Inputs: {input_hash}\")\n\n# Check if these match a prior run\nprior_runs = tracker.find_runs()\nfor run in prior_runs:\n    print(f\"Run {run.id}: signature={run.signature}\")\n</code></pre> <p>Common causes: - Code changed: Check git status, function definitions - Config changed: Check parameter types (0 vs 0.0, \"0\" vs 0) - Input file changed: Check file modification time, content hash - Run fields changed: <code>model</code>, <code>year</code>, or <code>iteration</code> are folded into the config hash - Dependencies changed: Installed package versions can affect behavior</p>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#how-do-i-clearreset-cache","level":3,"title":"\"How do I clear/reset cache?\"","text":"<p>Solution:</p> <p>Delete the database file:</p> <pre><code>rm ./provenance.duckdb\n</code></pre> <p>This clears all run history and cache. Next run will re-execute everything.</p> <p>To keep history but force re-execution:</p> <pre><code>result = consist.run(\n    fn=your_fn,\n    inputs={...},\n    outputs=[...],\n    cache_mode=\"overwrite\",\n)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#database-locked-error","level":3,"title":"\"Database locked\" error","text":"<p>Symptom: <code>database is locked</code> or similar error when running multiple Consist processes.</p> <p>Root Cause: DuckDB locks the database during writes. Concurrent write attempts fail.</p> <p>Solution:</p> <ol> <li> <p>Run sequentially (recommended):    <pre><code>python workflow1.py\npython workflow2.py\n</code></pre></p> </li> <li> <p>Use separate databases per process: <pre><code>tracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=f\"./provenance_{process_id}.duckdb\",  # Unique per process\n)\n</code></pre></p> </li> <li> <p>Increase lock timeout: <pre><code>import duckdb\nconn = duckdb.connect(\"provenance.duckdb\", timeout=30)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#mount-path-issues","level":2,"title":"Mount &amp; Path Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#mount-not-resolving-container-integration","level":3,"title":"\"Mount not resolving\" (Container integration)","text":"<p>Symptom: Container runs but <code>/inputs</code> is empty or doesn't exist.</p> <p>Root Cause: Volume mount paths don't exist or are incorrect.</p> <p>Solution:</p> <ol> <li> <p>Check paths exist on host: <pre><code>from pathlib import Path\nfor host_path in volumes.keys():\n    assert Path(host_path).exists(), f\"Missing: {host_path}\"\n</code></pre></p> </li> <li> <p>Use absolute paths: <pre><code># DON'T:\nvolumes={\"./data\": \"/inputs\"}\n\n# DO:\nvolumes={str(Path(\"./data\").resolve()): \"/inputs\"}\n</code></pre></p> </li> <li> <p>Check permissions: <pre><code>ls -la ./data\n# Must be readable by your user (and by Docker if using docker-in-docker)\n</code></pre></p> </li> <li> <p>Debug mount: <pre><code>docker run -it -v ./data:/inputs my-image ls -la /inputs\n</code></pre></p> </li> </ol> <p>If Consist errors about host paths not living under configured mounts, either add the mount in your <code>Tracker</code> or pass <code>strict_mounts=False</code> to <code>run_container()</code>.</p>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#uri-resolution-failed","level":3,"title":"\"URI resolution failed\"","text":"<p>Symptom: Error like <code>Cannot resolve URI: outputs://key/file.csv</code></p> <p>Root Cause: URI scheme not recognized or mount not registered.</p> <p>Solution:</p> <p>Use absolute paths instead of URI schemes for file operations:</p> <pre><code># DON'T:\nartifact_uri = \"outputs://key/result.csv\"\ndf = pd.read_csv(artifact_uri)  # Fails\n\n# DO:\nwith tracker.start_run(\"resolve_uri\", model=\"example\"):\n    artifact = tracker.log_artifact(result_path, key=\"key\", direction=\"output\")\n    df = pd.read_csv(artifact.path)  # Use .path property\n</code></pre> <p>Or resolve URI explicitly:</p> <pre><code>resolved_path = tracker.resolve_uri(\"outputs://key/result.csv\")\ndf = pd.read_csv(resolved_path)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#working-directory-changed-between-runs","level":3,"title":"\"Working directory changed between runs\"","text":"<p>Symptom: File paths work in first run but fail in second run (re-run from different directory).</p> <p>Root Cause: Relative paths depend on current working directory.</p> <p>Solution:</p> <p>Use absolute paths everywhere:</p> <pre><code># DON'T:\noutput_file = \"results.csv\"  # Relative to cwd\n\n# DO:\noutput_file = Path(tracker.run_dir) / \"results.csv\"  # Absolute\n</code></pre> <p>Or use artifact URIs:</p> <pre><code>with tracker.start_run(\"log_output\", model=\"example\"):\n    tracker.log_artifact(result, key=\"output\", direction=\"output\")\n# Later, access via:\nartifact = tracker.get_artifacts_for_run(\"run_id\").outputs[\"output\"]\nprint(artifact.path)  # Absolute path\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#data-schema-issues","level":2,"title":"Data &amp; Schema Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#schema-mismatch-during-ingestion","level":3,"title":"\"Schema mismatch during ingestion\"","text":"<p>Symptom: Error like <code>Column 'age' expected int, got str</code></p> <p>Root Cause: DataFrame column type doesn't match schema definition.</p> <p>Solution:</p> <p>Convert DataFrame types before ingestion:</p> <pre><code>from your_pkg.models import MySchema\n\n# Check types\nprint(df.dtypes)\n\n# Convert if needed\ndf = df.astype({\n    \"age\": \"int64\",\n    \"income\": \"float64\",\n    \"name\": \"object\",\n})\n\nwith tracker.start_run(\"ingest_data\", model=\"example\"):\n    tracker.log_dataframe(df, key=\"data\", schema=MySchema)\n</code></pre> <p>Or use Pandas casting:</p> <pre><code>df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")  # Convert with fallback\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#null-in-non-optional-field","level":3,"title":"\"Null in non-optional field\"","text":"<p>Symptom: Warning like <code>Null value in non-optional field 'age'</code></p> <p>Root Cause: DataFrame has NaN/None in a field that schema requires non-null.</p> <p>Solution:</p> <ol> <li> <p>Drop nulls: <pre><code>df = df.dropna(subset=[\"age\"])\n</code></pre></p> </li> <li> <p>Fill nulls: <pre><code>df[\"age\"] = df[\"age\"].fillna(0)  # Default value\n</code></pre></p> </li> <li> <p>Make field optional: <pre><code>from typing import Optional\n\nclass MySchema(SQLModel, table=True):\n    age: Optional[int]  # Can be None\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#duplicate-primary-keys","level":3,"title":"\"Duplicate primary keys\"","text":"<p>Symptom: Error like <code>Primary key violation: duplicate ID</code></p> <p>Root Cause: DataFrame has duplicate values in the primary key column.</p> <p>Solution:</p> <p>Deduplicate before ingestion:</p> <pre><code># Keep last occurrence (or \"first\")\ndf = df.drop_duplicates(subset=[\"id\"], keep=\"last\")\n\n# Or remove all duplicates\ndf = df[~df.duplicated(subset=[\"id\"], keep=False)]\n\nwith tracker.start_run(\"ingest_deduped\", model=\"example\"):\n    tracker.log_dataframe(df, key=\"data\", schema=MySchema)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#cant-query-across-runs","level":3,"title":"\"Can't query across runs\"","text":"<p>Symptom: <code>tracker.views.MySchema</code> doesn't exist or returns empty results.</p> <p>Root Cause: Schema not registered or data not ingested with schema.</p> <p>Solution:</p> <ol> <li> <p>Register schema on Tracker creation: <pre><code>tracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person, Trip],  # Register here\n)\n</code></pre></p> </li> <li> <p>Ingest with schema: <code>python with tracker.start_run(\"ingest_persons\", model=\"example\"):     tracker.log_dataframe(df, key=\"persons\", schema=Person)</code></p> </li> <li> <p>Verify schema exists: <pre><code>print(tracker.views.Person)  # Should not raise AttributeError\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#container-execution-issues","level":2,"title":"Container Execution Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#container-execution-failed","level":3,"title":"\"Container execution failed\"","text":"<p>Symptom: Error: <code>RuntimeError: Container execution failed</code></p> <p>Root Cause: Container exited with non-zero code.</p> <p>Solution:</p> <ol> <li> <p>Test container manually: <pre><code>docker run -it -v ./data:/inputs my-image python script.py\n</code></pre></p> </li> <li> <p>Check logs: <pre><code>docker logs &lt;container_id&gt;\n</code></pre></p> </li> <li> <p>Add verbose output: <pre><code>result = run_container(\n    ...\n    environment={\"DEBUG\": \"1\"},  # Enable debug output in container\n)\n</code></pre></p> </li> <li> <p>Verify input paths: <pre><code>from pathlib import Path\nfor input_path in inputs:\n    assert Path(input_path).exists(), f\"Missing: {input_path}\"\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#output-files-not-found-after-container","level":3,"title":"\"Output files not found after container\"","text":"<p>Symptom: Warning: <code>Expected output not found: ./outputs/result.csv</code></p> <p>Root Cause: Container didn't create output at expected location.</p> <p>Solution:</p> <ol> <li> <p>Verify container creates outputs: <pre><code>docker run -it -v ./outputs:/outputs my-image sh -c \"ls -la /outputs &amp;&amp; echo 'done'\"\n</code></pre></p> </li> <li> <p>Check output paths in container: <pre><code>run_container(\n    ...\n    command=[\"python\", \"script.py\"],  # Ensure script creates output\n)\n</code></pre></p> </li> <li> <p>Use correct host paths: <pre><code>output_dir = Path(\"./outputs\").mkdir(parents=True, exist_ok=True)\nresult = run_container(\n    ...\n    outputs=[str(output_dir / \"result.csv\")],\n)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#image-pull-failed","level":3,"title":"\"Image pull failed\"","text":"<p>Symptom: Error: <code>Error pulling image: authentication required</code></p> <p>Root Cause: Docker can't access the image registry.</p> <p>Solution:</p> <ol> <li> <p>Authenticate: <pre><code>docker login\n</code></pre></p> </li> <li> <p>Use public images: <pre><code>run_container(\n    image=\"ubuntu:latest\",  # Public image\n    ...\n)\n</code></pre></p> </li> <li> <p>Check image exists locally: <pre><code>docker images | grep my-image\n</code></pre></p> </li> <li> <p>Disable pull: <pre><code>run_container(\n    ...\n    pull_latest=False,  # Use local image if available\n)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#permission-denied-in-container","level":3,"title":"\"Permission denied in container\"","text":"<p>Symptom: <code>Permission denied</code> when container writes to mounted volume.</p> <p>Root Cause: Container user doesn't have write permission on host mount.</p> <p>Solution:</p> <ol> <li> <p>Make directory writable: <pre><code>chmod 777 ./outputs\n</code></pre></p> </li> <li> <p>Run container as current user: <pre><code># (Requires custom Dockerfile or user configuration)\n# docker run --user $(id -u):$(id -g) ...\n</code></pre></p> </li> <li> <p>Create output directory with correct permissions: <pre><code>output_dir = Path(\"./outputs\")\noutput_dir.mkdir(parents=True, exist_ok=True, mode=0o777)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#performance-issues","level":2,"title":"Performance Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#runs-are-very-slow","level":3,"title":"\"Runs are very slow\"","text":"<p>Symptom: Each run takes much longer than expected.</p> <p>Root Cause: Several possibilities:</p> <ol> <li>No cache hits: Check if signature is changing unexpectedly.</li> <li>File I/O bottleneck: Large artifact materialization.</li> <li>Database queries slow: Too many cross-run queries.</li> <li>Container startup overhead: Each container run adds 1-2 seconds.</li> </ol> <p>Solution:</p> <ol> <li> <p>Profile execution: <pre><code>import time\nstart = time.time()\nresult = consist.run(...)\nprint(f\"Elapsed: {time.time() - start}s\")\nprint(f\"Cache hit: {result.cache_hit}\")\n</code></pre></p> </li> <li> <p>Avoid unnecessary materialization: <pre><code># Don't materialize if you don't need it\nresult = consist.run(..., cache_hydration=\"none\")\n</code></pre></p> </li> <li> <p>Use Parquet instead of CSV (faster parsing):    <pre><code>df.to_parquet(\"output.parquet\")  # Instead of .to_csv()\n</code></pre></p> </li> <li> <p>Batch containers to reduce startup overhead: <pre><code># DON'T:\nfor i in range(100):\n    run_container(...)  # 100 containers, 100 startups\n\n# DO:\nrun_container(\n    command=[\"python\", \"process_batch.py\", \"--n\", \"100\"],\n    ...\n)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#database-is-huge-and-slow","level":3,"title":"\"Database is huge and slow\"","text":"<p>Symptom: Queries are slow, database file is large.</p> <p>Root Cause: Too much data ingested or too many runs.</p> <p>Solution:</p> <ol> <li> <p>Vacuum database: <pre><code>with tracker.engine.begin() as conn:\n    conn.exec_driver_sql(\"VACUUM\")\n</code></pre></p> </li> <li> <p>Archive old runs: <pre><code># Move old database\nmv provenance.duckdb provenance.backup.duckdb\n# Start fresh\n</code></pre></p> </li> <li> <p>Use selective ingestion: <code>python    # Don't ingest everything, just what you need with tracker.start_run(\"sample_ingest\", model=\"example\"):     tracker.log_dataframe(df.head(1000), key=\"sample\")  # Sample instead of all</code></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#debugging-tools","level":2,"title":"Debugging Tools","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#enable-logging","level":3,"title":"Enable Logging","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(\"consist\")\nlogger.setLevel(logging.DEBUG)\n</code></pre> <p>This prints detailed provenance tracking, signature computation, and cache decisions.</p>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#inspect-run-metadata","level":3,"title":"Inspect Run Metadata","text":"<pre><code>run = tracker.get_run(\"run_id\")\nprint(f\"Signature: {run.signature}\")\nprint(f\"Code hash: {run.git_hash}\")\nprint(f\"Meta: {run.meta}\")\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#inspect-database","level":3,"title":"Inspect Database","text":"<pre><code>import duckdb\n\nconn = duckdb.connect(\"provenance.duckdb\")\nprint(conn.query(\"SELECT * FROM run LIMIT 5\").df())\nprint(conn.query(\"SELECT * FROM artifact LIMIT 5\").df())\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#check-file-hashes","level":3,"title":"Check File Hashes","text":"<pre><code>from pathlib import Path\n\nwith tracker.start_run(\"hash_input\", model=\"example\"):\n    artifact = tracker.log_artifact(Path(\"input.csv\"), key=\"input\", direction=\"input\")\n    print(f\"Path: {artifact.path}\")\n    print(f\"Hash: {artifact.hash}\")\n    print(f\"Size: {artifact.path.stat().st_size}\")\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#getting-help","level":2,"title":"Getting Help","text":"<p>If you hit an issue not covered here:</p> <ol> <li> <p>Check the logs: <pre><code>logging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> <li> <p>Inspect database: <pre><code>duckdb provenance.duckdb \".schema\"\n</code></pre></p> </li> <li> <p>File an issue on GitHub with:</p> </li> <li>Error message and traceback</li> <li>Minimal reproducible example</li> <li>Output of <code>consist runs</code> (recent run history)</li> <li>Output of logging (with DEBUG enabled)</li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#see-also","level":2,"title":"See Also","text":"<ul> <li>Container Integration</li> <li>DLT Loader</li> <li>Architecture (for implementation details)</li> <li>CLI Reference (for debugging commands)</li> </ul>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"usage-guide/","level":1,"title":"Usage Guide","text":"<p>Consist provides flexible patterns for tracking provenance in scientific workflows. This guide walks you through the main usage patterns, from simple single-step runs to complex multi-year simulations. Each section is written to help:</p> <ul> <li>Developers integrating Consist into a simulation tool</li> <li>Practitioners running tools and wanting clearer inputs/outputs</li> <li>Researchers managing multi-stage pipelines and reproducibility</li> </ul> <p>New to Consist? Start with the quickstart notebook, then work through the examples below in order.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#choosing-your-pattern","level":2,"title":"Choosing Your Pattern","text":"<p>Choose based on what you're building and how much structure you need:</p> Your Workflow Pattern Why Single data processing step (clean, transform, aggregate) <code>run</code> Simple: caches the entire function call with low overhead. Use for self-contained operations. Multi-step workflow (preprocessing → simulation → analysis) <code>scenario</code> Groups related steps, shares state via coupler, per-step caching. Use when steps have dependencies or shared configuration. Existing tool/model (subprocess, legacy code, container) <code>container</code> or <code>depends_on</code> Wraps external executables, tracks container digest as cache key. Use for black-box tools. Parameter sweep / sensitivity analysis <code>scenario</code> + loop Run the same step with different configs, compare results. Multi-year simulation <code>scenario</code> + loop Runs in years, each year caches independently, all years share scenario context.","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#pattern-1-single-step-runs-run","level":2,"title":"Pattern 1: Single-Step Runs (<code>run()</code>)","text":"<p>Use <code>run()</code> when you have a self-contained operation: data cleaning, transformation, aggregation, or any callable that takes inputs and produces outputs. Consist caches the entire function call based on code version + config + input data.</p> <p>When to use:</p> <ul> <li>Simple data transformations (filter, aggregate, merge)</li> <li>Expensive computations that don't depend on other runs</li> <li>One-off analyses</li> <li>You want the simplest mental model</li> </ul> <p>When NOT to use:</p> <ul> <li>Multi-step workflows with dependencies between steps (use <code>scenario()</code> instead)</li> <li>Existing tools that write to directories (use <code>depends_on</code> or <code>container</code>)</li> </ul>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#simple-example-data-cleaning","level":3,"title":"Simple Example: Data Cleaning","text":"<p>Here's the basic pattern:</p> <pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom pathlib import Path\nimport pandas as pd\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")  # (1)!\n\ndef clean_data(raw: pd.DataFrame, threshold: float = 0.5) -&gt; pd.DataFrame:  # (2)!\n    df = raw[raw[\"value\"] &gt; threshold]  # (3)!\n    return df\n\nwith use_tracker(tracker):  # (4)!\n    result = consist.run(\n        fn=clean_data,\n        inputs={\"raw\": Path(\"raw.csv\")},\n        config={\"threshold\": 0.5},  # (5)!\n        outputs=[\"cleaned\"],\n    )\n\ncleaned_artifact = result.outputs[\"cleaned\"]  # (6)!\ncleaned_df = consist.load_df(cleaned_artifact)\nprint(f\"Output: {cleaned_artifact.path}\")\n</code></pre> <ol> <li>Create the tracker for run metadata and caching.</li> <li>Define the function Consist will cache.</li> <li>Filter rows above the threshold.</li> <li>Run inside the tracker context so Consist can log provenance.</li> <li><code>config</code> participates in the cache key.</li> <li>Access the cached output artifact and load data.</li> </ol> <p>If you run it again with the same inputs and config, you should get a cache hit and no re-execution.</p> Alternative: keep raw file paths (no auto-load)  Use this when your function needs a `Path` and manages I/O directly (common for legacy tools or file-based APIs).  <pre><code>def clean_data(raw_file: Path, _consist_ctx) -&gt; None:\n    df = pd.read_csv(raw_file)\n    out_path = _consist_ctx.run_dir / \"cleaned.parquet\"\n    df.to_parquet(out_path)\n    _consist_ctx.log_output(out_path, key=\"cleaned\")\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean_data,\n        inputs={\"raw_file\": Path(\"raw.csv\")},  # (1)!\n        load_inputs=False,\n        runtime_kwargs={\"raw_file\": Path(\"raw.csv\")},\n        inject_context=True,\n    )\n</code></pre>  1. Hash the input file as part of the cache key.","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#example-with-config","level":3,"title":"Example with Config","text":"<p>Use Pydantic models for structured configs:</p> <pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom pydantic import BaseModel\nfrom pathlib import Path\nimport pandas as pd\n\nclass CleaningConfig(BaseModel):\n    threshold: float = 0.5\n    remove_outliers: bool = True\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\ndef clean_data(raw: pd.DataFrame, config: CleaningConfig) -&gt; pd.DataFrame:\n    df = raw\n    df = df[df[\"value\"] &gt;= config.threshold]\n    if config.remove_outliers:\n        df = df[df[\"value\"] &lt; df[\"value\"].quantile(0.95)]\n    return df\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean_data,\n        inputs={\"raw\": Path(\"raw.csv\")},\n        config=CleaningConfig(threshold=0.5, remove_outliers=True),\n        outputs=[\"cleaned\"],\n    )\n</code></pre> <p>Each distinct config → separate cache entries. Change the threshold? Only that run re-executes.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#wrapping-legacy-or-black-box-tools","level":3,"title":"Wrapping Legacy or Black-Box Tools","text":"<p>If you have existing code that writes files to a directory, use the injected run context to capture outputs:</p> <pre><code>from pathlib import Path\n\ndef run_legacy_model(upstream, _consist_ctx) -&gt; None:\n    import legacy_model\n\n    output_dir = _consist_ctx.run_dir / \"legacy_outputs\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    with _consist_ctx.capture_outputs(output_dir, pattern=\"*.csv\"):\n        legacy_model.run(upstream, output_dir=output_dir)\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=run_legacy_model,\n        inputs={\"upstream\": Path(\"input.csv\")},\n        depends_on=[Path(\"config.yaml\"), Path(\"parameters.json\")],  # (1)!\n        load_inputs=False,\n        runtime_kwargs={\"upstream\": Path(\"input.csv\")},  # (2)!\n        inject_context=True,\n    )\n</code></pre> <ol> <li>Hash these files too so config changes invalidate the cache.</li> <li>Pass raw paths at runtime when <code>load_inputs=False</code>.</li> </ol> <p>Captured outputs are keyed by filename stem (for example, <code>results.csv</code> -&gt; <code>results</code>).</p> Alternative: capture a fixed output directory  If the tool always writes to a known folder and returns `None`, you can capture it directly:  <pre><code>with use_tracker(tracker):\n    def run_legacy_model(upstream) -&gt; None:\n        import legacy_model\n\n        legacy_model.run(upstream, output_dir=\"outputs\")\n\n    result = consist.run(\n        fn=run_legacy_model,\n        inputs={\"upstream\": Path(\"input.csv\")},  # (1)!\n        depends_on=[Path(\"config.yaml\")],\n        load_inputs=False,\n        runtime_kwargs={\"upstream\": Path(\"input.csv\")},\n        capture_dir=Path(\"outputs\"),\n        capture_pattern=\"*.csv\",\n    )\n</code></pre>  1. Hash the input file as part of the cache key.  How input mappings become function arguments  When `inputs` is a mapping, Consist matches keys to function parameters and auto-loads those artifacts (for example, a CSV becomes a DataFrame) into the call by default. If you keep `load_inputs=True`, `inputs={\"upstream\": ...}` becomes the `upstream` argument automatically.  If you want raw paths instead of auto-loaded data, set `load_inputs=False` and pass paths explicitly via `runtime_kwargs`.  <p>The <code>depends_on</code> files are hashed as part of the cache key, so changing config.yaml invalidates the cache.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#pattern-2-multi-step-workflows-scenario","level":2,"title":"Pattern 2: Multi-Step Workflows (<code>scenario()</code>)","text":"<p>Use <code>scenario()</code> when you have multiple interdependent steps that share state or configuration. Scenarios group steps into a coherent unit (a \"run scenario\"), while each step caches independently.</p> <p>When to use:</p> <ul> <li>Multi-step pipelines (preprocess → simulate → analyze)</li> <li>Steps have data dependencies (output of step 1 is input to step 2)</li> <li>Multi-year simulations (year 2020 → 2030 → 2040)</li> <li>Parameter sweeps where you want to compare across variants</li> <li>Shared configuration across multiple steps</li> </ul> <p>Benefits over <code>run()</code>:</p> <ul> <li>Steps cache independently—skip re-executing steps whose inputs haven't changed</li> <li>Use the coupler to pass data between steps with automatic provenance tracking</li> <li>Group runs into scenarios for easy cross-scenario queries</li> </ul>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#understanding-the-coupler","level":3,"title":"Understanding the Coupler","text":"<p>The coupler is your scenario-scoped artifact registry. When you log an artifact with a key, it's automatically stored in the coupler, making data flow between steps explicit and traceable. This is especially useful when: - You need clean handoffs between tools (developer workflows) - You want a clear list of outputs by name (practitioner workflows) - You want auditable step-to-step lineage (research workflows)</p> <p>Etymology: A coupler (like the library name \"consist\" from railroad terminology) is the mechanism that links train cars together. In Consist, the coupler links your workflow steps by storing and threading their outputs.</p> <p>Key behaviors:</p> <ul> <li>When you log an artifact with a <code>key</code>, it's automatically synced to the coupler</li> <li>You retrieve artifacts with <code>coupler.require(key)</code> or via <code>inputs=</code> declarations</li> <li>The coupler persists across all steps in a scenario</li> <li>Each scenario has its own coupler; they don't share data</li> <li>On cache hits, cached outputs are pre-synced to the coupler before your step runs</li> </ul> <p>You interact with the coupler when:</p> <ul> <li>Accessing inputs in trace blocks: <code>coupler.require(\"population\")</code></li> <li>Declaring inputs to <code>sc.run()</code>: <code>inputs=[\"population\"]</code></li> <li>Validating that outputs were produced: <code>sc.require_outputs(...)</code></li> </ul> <p>Live-sync (automatic): When you log an artifact, it's immediately available in the coupler—you don't need to manually call <code>coupler.set()</code>.</p> <p>For optional-Consist workflows: If you're using Consist in optional mode (with fallback to Path objects or artifact-like objects), use <code>coupler.set_from_artifact(key, value)</code> instead of <code>coupler.set()</code>. It handles both real Artifacts and artifact-like objects (Paths, strings, noop artifacts) transparently.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#simple-example-two-step-workflow","level":3,"title":"Simple Example: Two-Step Workflow","text":"<pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom pathlib import Path\nimport pandas as pd\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\ndef preprocess_data(raw: pd.DataFrame) -&gt; pd.DataFrame:  # (1)!\n    df = raw[raw[\"value\"] &gt; 0.5]\n    return df\n\ndef analyze_data(preprocessed: pd.DataFrame) -&gt; pd.DataFrame:\n    summary = preprocessed.groupby(\"category\", as_index=False)[\"value\"].mean()\n    return summary\n\nwith use_tracker(tracker):  # (2)!\n    with consist.scenario(\"my_analysis\") as sc:\n        sc.run(\n            name=\"preprocess\",\n            fn=preprocess_data,\n            inputs={\"raw\": Path(\"raw.csv\")},\n            outputs=[\"preprocessed\"],\n        )\n\n        sc.run(\n            name=\"analyze\",\n            fn=analyze_data,\n            inputs=[\"preprocessed\"],\n            load_inputs=True,\n            outputs=[\"analysis\"],\n        )\n</code></pre> <ol> <li>Define steps as regular functions for <code>sc.run</code>.</li> <li>Execute the steps inside a scenario context.</li> </ol> <p>All steps have <code>scenario_id=\"my_analysis\"</code>, making it easy to query together. Change preprocess logic? The preprocess step re-runs; the analyze step re-runs only if the preprocessed artifact changes. Change raw.csv? Both re-execute.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#declaring-inputs-mapping-form-vs-list-form","level":3,"title":"Declaring Inputs: Mapping Form vs List Form","text":"<p>The <code>inputs=</code> parameter supports two different forms for different situations:</p> <p>Mapping form (explicit paths from disk): <pre><code>sc.run(\n    name=\"preprocess\",\n    fn=preprocess_data,\n    inputs={\"raw\": Path(\"raw.csv\")},  # (1)!\n    outputs=[\"preprocessed\"],\n)\n</code></pre></p> <ol> <li>Load from disk to create the initial artifact. Use this when you're loading data from disk for the first time.</li> </ol> <p>List form (resolve from coupler): <pre><code>sc.run(\n    name=\"analyze\",\n    fn=analyze_data,\n    inputs=[\"preprocessed\"],          # (1)!\n    load_inputs=True,                 # (2)!\n    outputs=[\"analysis\"],\n)\n</code></pre></p> <ol> <li>Resolve inputs from the coupler.</li> <li>Auto-load inputs as function parameters. Use this when the artifact is already in the coupler from a prior step. With <code>load_inputs=True</code>, each key becomes a function parameter with the loaded data. For example, <code>inputs=[\"preprocessed\"]</code> means your function receives a <code>preprocessed</code> parameter with the loaded DataFrame.</li> </ol> <p>Note: If you have existing code using <code>input_keys=</code>, it continues to work for backward compatibility. <code>inputs=</code> is the preferred name for new code.</p> Alternative: inline steps with sc.trace  Use this when you want inline code blocks (they always execute, even on cache hits). This can be useful for lightweight validation or logging.  <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"my_analysis\") as sc:\n        with sc.trace(name=\"preprocess\", inputs={\"raw\": Path(\"raw.csv\")}):\n            df = pd.read_csv(\"raw.csv\")\n            consist.log_dataframe(df, key=\"preprocessed\")\n\n        with sc.trace(name=\"analyze\", inputs=[\"preprocessed\"]):\n            df = consist.load_df(sc.coupler.require(\"preprocessed\"))\n            summary = df.groupby(\"category\", as_index=False)[\"value\"].mean()\n            consist.log_dataframe(summary, key=\"analysis\")\n</code></pre>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#passing-data-between-steps-with-the-coupler","level":3,"title":"Passing Data Between Steps with the Coupler","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\", model=\"travel_demand\") as sc:\n        coupler = sc.coupler\n\n        with sc.trace(name=\"initialize\", run_id=\"baseline_init\"):  # (1)!\n            df_pop = load_population()\n            consist.log_dataframe(df_pop, key=\"population\")\n\n        for year in [2020, 2030, 2040]:  # (2)!\n            with sc.trace(\n                name=\"simulate\",\n                run_id=f\"baseline_{year}\",\n                year=year,\n                inputs=[\"population\"],  # (3)!\n            ):\n                df_pop = consist.load_df(coupler.require(\"population\"))  # (4)!\n                df_result = run_model(year, df_pop)\n\n                consist.log_dataframe(df_result, key=\"persons\")  # (5)!\n</code></pre> <ol> <li>Step 1: load and prepare the population artifact.</li> <li>Step 2: simulate for each year.</li> <li>Declare the dependency on the population artifact.</li> <li>Get data from the coupler with automatic cache detection.</li> <li>Log output and store it for downstream steps.</li> </ol> <p>What <code>inputs</code> (list form) does:</p> <ul> <li>Declares that this step depends on \"population\" artifact</li> <li>Consist tracks this as part of the cache key</li> <li>If the population artifact hasn't changed, this step's cache is still valid</li> </ul> <p>Simpler alternative (auto-load inputs with <code>sc.run</code>)</p> <p>If your step can be expressed as a function, <code>sc.run</code> can auto-load inputs into function parameters so you don't need to call <code>coupler.require(...)</code> manually:</p> <pre><code>def simulate_year(population: pd.DataFrame, config: dict) -&gt; pd.DataFrame:\n    year = config[\"year\"]\n    return run_model(year, population)\n\nwith use_tracker(tracker):\n    with consist.scenario(\"baseline\", model=\"travel_demand\") as sc:\n        with sc.trace(name=\"initialize\", run_id=\"baseline_init\"):\n            df_pop = load_population()\n            consist.log_dataframe(df_pop, key=\"population\")\n\n        for year in [2020, 2030, 2040]:\n            sc.run(\n                name=f\"simulate_{year}\",\n                fn=simulate_year,\n                inputs=[\"population\"],\n                outputs=[\"persons\"],\n                load_inputs=True,\n                config={\"year\": year},\n            )\n</code></pre>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#output-validation","level":3,"title":"Output Validation","text":"<p>Consist can validate that your workflow produces expected outputs, catching typos or missing data early. You have three patterns to choose from based on your workflow. Most users just need Pattern A.</p> <p>Pattern A: Declare required outputs (Static workflows) — START HERE</p> <p>Use this when you know all your workflow outputs upfront.</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"workflow\") as sc:\n        sc.require_outputs(\n            \"zarr_skims\",\n            \"synthetic_population\",\n        )\n\n        compile_result = sc.run(\"compile\", fn=asim_compile_runner.run)  # (1)!\n</code></pre> <ol> <li>Run steps; at scenario exit, missing outputs raise <code>RuntimeError</code>.</li> </ol> <p>When to use: Static workflows where steps always produce the same outputs (most common).</p> <p>Benefits: Simple, clear contract. Missing outputs are caught at scenario exit. Typos are caught immediately.</p> <p>Optional: Add guardrails for typos <pre><code>sc.require_outputs(\n    \"zarr_skims\",\n    \"synthetic_population\",\n    warn_undocumented=True,  # (1)!\n    description={\n        \"zarr_skims\": \"Zone-to-zone travel times in Zarr format\",\n        \"synthetic_population\": \"Synthetic population with activity schedules\",\n    }\n)\n</code></pre></p> <ol> <li>Warn if you set other keys by mistake.</li> </ol> <p>Shortcut: Pass required outputs directly to <code>scenario()</code>: <pre><code>with consist.scenario(\n    \"workflow\",\n    require_outputs=[\"zarr_skims\", \"synthetic_population\"],\n) as sc:\n    ...\n</code></pre></p> <p>Pattern B: Runtime-declared validation (Dynamic/optional outputs)</p> <p>Use this when outputs are dynamic or optional (e.g., optional debug outputs, or conditional branching).</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"workflow\") as sc:\n        sc.declare_outputs(\n            \"zarr_skims\", \"synthetic_population\",\n            required={\"zarr_skims\": True, \"synthetic_population\": True}\n        )\n\n        if debugging:  # (1)!\n            sc.declare_outputs(\"debug_report\", required={\"debug_report\": False})  # (2)!\n\n        compile_result = sc.run(\"compile\", fn=asim_compile_runner.run)  # (3)!\n</code></pre> <ol> <li>Add optional outputs later if needed.</li> <li>Declare an optional debug output.</li> <li>Missing required outputs raise <code>RuntimeError</code> at exit.</li> </ol> <p>When to use: Workflows with per-key control over required vs optional, or conditional outputs.</p> <p>Benefits: Granular per-key control. Mix required and optional outputs. Add outputs dynamically.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#selective-output-collection-with-collect_by_keys","level":3,"title":"Selective Output Collection with <code>collect_by_keys()</code>","text":"<p>By default, when a step produces multiple outputs, all are automatically synced to the coupler. Use <code>collect_by_keys()</code> when you need to:</p> <ul> <li>Select only specific outputs from many (ignore others)</li> <li>Namespace outputs by year or scenario (prefix them)</li> </ul> <pre><code>result = sc.run(\"step\", fn=some_func)  # (1)!\nsc.collect_by_keys(result.outputs, \"persons\", \"households\")  # (2)!\n\nfor year in [2020, 2030, 2040]:\n    result = sc.run(f\"forecast_{year}\", fn=forecast_fn)\n    sc.collect_by_keys(result.outputs, \"population\", \"skims\", prefix=f\"{year}_\")  # (3)!\n</code></pre> <ol> <li>Produces <code>persons</code>, <code>households</code>, <code>jobs</code>.</li> <li>Keep only selected outputs; others are ignored.</li> <li>Namespace outputs by year (e.g., <code>2020_population</code>).</li> </ol> <p>Bulk logging with metadata: <pre><code>with consist.scenario(\"outputs\") as sc:\n    with sc.trace(name=\"export_outputs\"):\n        outputs = consist.log_artifacts(  # (1)!\n            {\n                \"persons\": \"results/persons.parquet\",\n                \"households\": \"results/households.parquet\",\n                \"jobs\": \"results/jobs.parquet\"\n            },\n            metadata_by_key={\n                \"households\": {\"role\": \"primary_unit\"},\n                \"jobs\": {\"role\": \"employment_proxy\"}\n            },\n            year=2030,\n            scenario_name=\"base\"  # (2)!\n        )\n</code></pre></p> <ol> <li>Log multiple files at once with explicit keys.</li> <li>All outputs get <code>year=2030</code>, <code>scenario_name=\"base\"</code>; <code>households</code> also gets <code>role=\"primary_unit\"</code>.</li> </ol>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#example-parameter-sweep-in-a-scenario","level":3,"title":"Example: Parameter Sweep in a Scenario","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"sensitivity_analysis\") as sc:\n        coupler = sc.coupler\n\n        with sc.trace(name=\"setup\"):  # (1)!\n            df = pd.read_csv(\"data.csv\")\n            consist.log_dataframe(df, key=\"data\")\n\n        for threshold in [0.3, 0.5, 0.7]:  # (2)!\n            with sc.trace(\n                name=\"analyze\",\n                run_id=f\"threshold_{threshold}\",\n                threshold=threshold,\n                inputs=[\"data\"],\n            ):\n                df = consist.load_df(coupler.require(\"data\"))\n                filtered = df[df[\"value\"] &gt; threshold]\n                consist.log_dataframe(filtered, key=\"filtered\")\n</code></pre> <ol> <li>Load once, reuse for all variants.</li> <li>Test different thresholds as separate runs.</li> </ol> <p>Each threshold creates a separate run with its own cache entry. Re-run later? Consist returns cached results for matching thresholds, skips setup (since input unchanged).</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#pattern-3-container-integration","level":2,"title":"Pattern 3: Container Integration","text":"<p>Use containers when you have existing tools, models, or legacy code that runs as a subprocess or Docker container. The image digest becomes part of the cache key.</p> <p>When to use:</p> <ul> <li>Running ActivitySim, SUMO, BEAM, or other external models</li> <li>Legacy code you don't want to refactor</li> <li>Python/R/Java executables that you invoke as subprocesses</li> <li>Tools that expect specific file paths or output directories</li> </ul> <pre><code>from consist.integrations.containers import run_container\nfrom pathlib import Path\n\nhost_inputs = Path(\"data/inputs\").resolve()\nhost_outputs = Path(\"data/outputs\").resolve()\n\nresult = run_container(\n    tracker=tracker,\n    run_id=\"model_2030\",\n    image=\"travel-model:v2.1\",  # (1)!\n    command=[\"python\", \"run.py\", \"--year\", \"2030\"],\n    volumes={\n        str(host_inputs): \"/inputs\",\n        str(host_outputs): \"/outputs\",\n    },\n    inputs=[host_inputs / \"input.csv\"],\n    outputs={\"results\": host_outputs / \"results.parquet\"},\n)\n\noutput_artifact = result.artifacts[\"results\"]\n</code></pre> <ol> <li>The image digest becomes part of the cache key.</li> </ol> <p>Change the image version? Cache invalidates. Same image + inputs? Returns cached results.</p> Alternative: use consist.run with executor=\"container\"  If you prefer the same API as `consist.run`, you can use the container executor:  <pre><code>import consist\n\nwith consist.use_tracker(tracker):\n    result = consist.run(\n        name=\"model_2030\",\n        executor=\"container\",\n        inputs=[host_inputs / \"input.csv\"],\n        output_paths={\"results\": host_outputs / \"results.parquet\"},\n        container={\n            \"image\": \"travel-model:v2.1\",\n            \"command\": [\"python\", \"run.py\", \"--year\", \"2030\"],\n            \"volumes\": {str(host_inputs): \"/inputs\", str(host_outputs): \"/outputs\"},\n        },\n    )\n</code></pre>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#advanced-patterns","level":2,"title":"Advanced Patterns","text":"","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#cache-hits-and-the-coupler","level":3,"title":"Cache Hits and the Coupler","text":"<p>When Consist detects a cache hit (same step, same code version, same config and inputs):</p> <ol> <li>In <code>sc.run()</code>: Your function is skipped entirely. Cached outputs are returned and automatically synced to the coupler.</li> <li>In <code>sc.trace()</code>: Cached outputs are pre-synced to the coupler BEFORE your trace body runs, so you can access them with <code>coupler.require()</code> immediately. Your code still executes (unlike <code>sc.run()</code>).</li> </ol> <p>This means your code doesn't need to handle cache hits differently—the coupler is populated automatically in both cases.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#cache-hydration","level":3,"title":"Cache Hydration","text":"<p>By default, Consist returns metadata-only cache hits (no file copies). You can opt in to materializing cached files when needed:</p> <ul> <li><code>outputs-requested</code>: Copy only specific cached outputs to paths you provide</li> <li><code>outputs-all</code>: Copy all cached outputs into a target directory</li> <li><code>inputs-missing</code>: When a cache miss occurs, backfill missing inputs from prior runs before executing</li> </ul> <p>Note: <code>outputs-requested</code> requires <code>output_paths=...</code> so Consist knows where to write the files. <code>inputs-missing</code> only works for inputs that are tracked artifacts (not raw paths), so Consist can find the prior run's files or reconstruct ingested tables.</p> <p>Set per-run via <code>cache_hydration=...</code> or for scenario steps via <code>step_cache_hydration=...</code>:</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\", step_cache_hydration=\"inputs-missing\") as sc:\n        coupler = sc.coupler\n        with sc.trace(name=\"simulate\", inputs=[\"population\"]):  # (1)!\n            df_pop = consist.load_df(coupler.require(\"population\"))\n            ...\n</code></pre> <ol> <li>This step backfills missing inputs from prior runs before executing.</li> </ol>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#mixing-runs-and-scenarios","level":3,"title":"Mixing Runs and Scenarios","text":"<p>Call <code>consist.run(...)</code> inside a scenario when a step should cache independently:</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        coupler = sc.coupler\n        preprocess = consist.run(\n            fn=expensive_preprocessing,\n            inputs={\"network_file\": Path(\"network.geojson\")},\n            outputs=[\"processed\"],\n        )  # (1)!\n        coupler.update(preprocess.outputs)  # (2)!\n\n        with sc.trace(name=\"simulate\", inputs=[\"processed\"]):  # (3)!\n            network = consist.load_df(coupler.require(\"processed\"))\n            ...\n</code></pre> <ol> <li>Run expensive preprocessing independently with its own cache key.</li> <li>Add outputs to the coupler for downstream steps.</li> <li>Later steps consume the preprocessed output.</li> </ol>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#when-does-code-execute-understanding-scrun-vs-sctrace","level":2,"title":"When Does Code Execute? Understanding <code>sc.run()</code> vs <code>sc.trace()</code>","text":"<p>When building multi-step scientific workflows, a critical question arises: Does my Python code run every time, or only when inputs change? This section clarifies the difference between <code>sc.run()</code> and <code>sc.trace()</code>, two fundamental Consist patterns that differ in execution behavior on cache hits.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#the-core-distinction","level":3,"title":"The Core Distinction","text":"<p>On a cache hit (when Consist finds previously-cached results for this step with the same inputs):</p> <ul> <li> <p><code>sc.trace(...)</code> — Your Python block always executes. Consist returns cached outputs, but your code still runs. Use this for logging, diagnostics, or steps that must track intermediate state every run.</p> </li> <li> <p><code>sc.run(...)</code> — Your Python function only executes on cache miss. On a cache hit, Consist skips calling your function entirely and returns the cached output. Use this for expensive operations like scientific simulations, data processing, or model fitting.</p> </li> </ul>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#why-this-matters-performance-side-effects","level":3,"title":"Why This Matters: Performance &amp; Side Effects","text":"<p>Consider an expensive simulation that takes 2 hours. Running it 100 times with the same inputs would normally take 200 hours of compute. With Consist:</p> <ul> <li>If you use <code>sc.trace()</code>: code runs 100 times (200 hours) — caching provides metadata only</li> <li>If you use <code>sc.run()</code>: code runs once (2 hours), then 99 cache hits retrieve results instantly</li> </ul> <p>Side effects also differ. If your step writes temporary files, updates external systems, or has other side effects, <code>sc.trace()</code> repeats them on every run, while <code>sc.run()</code> skips them on cache hits.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#example-activitysim-style-land-use-simulation","level":3,"title":"Example: ActivitySim-Style Land Use Simulation","text":"<p>Here's a realistic example showing the difference:</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#using-sctrace-always-runs","level":4,"title":"Using <code>sc.trace()</code> (Always Runs)","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        year = 2030\n        with sc.trace(  # (1)!\n            name=\"prepare_land_use\",\n            inputs={\"geojson\": Path(\"land_use.geojson\")},\n            year=year\n        ):\n            print(f\"Processing land use for year {year}\")  # (2)!\n            zones = load_zones(\"land_use.geojson\")\n\n            df_zones = pd.DataFrame(zones)  # (3)!\n            consist.log_dataframe(df_zones, key=\"zones\")\n</code></pre> <ol> <li>This block executes every time, even on cache hits.</li> <li>Code always runs—useful for logging or status.</li> <li>Consist still returns cached outputs if they exist.</li> </ol>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#using-scrun-skips-on-cache-hit","level":4,"title":"Using <code>sc.run()</code> (Skips on Cache Hit)","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        def prepare_land_use(geojson_path: Path) -&gt; pd.DataFrame:\n            print(f\"Processing land use\")  # (1)!\n            zones = load_zones(geojson_path)\n            return pd.DataFrame(zones)\n\n        result = sc.run(  # (3)!\n            name=\"prepare_land_use\",\n            fn=prepare_land_use,\n            inputs={\"geojson_path\": Path(\"land_use.geojson\")},\n            outputs=[\"zones\"],\n            load_inputs=True,  # (2)!\n        )\n</code></pre> <ol> <li>This function only runs on cache miss; it prints on the first run.</li> <li>Auto-load <code>Path</code> into the function argument.</li> <li>Outputs are synced to the coupler automatically.</li> </ol>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#which-should-you-use","level":3,"title":"Which Should You Use?","text":"<p>Choose based on your workflow needs:</p> Scenario Use Why Expensive simulation, model fitting, or large data transformation <code>sc.run()</code> Skip re-execution on cache hits; critical for 2+ hour runtimes or iterative analysis Steps that log, print diagnostics, or validate state on every run <code>sc.trace()</code> Need to see side effects repeated; cheaper operations that re-run quickly Multi-year simulation where early years are cached, new years execute <code>sc.run()</code> Each year has independent cache entry; skip re-running 2020 when computing 2030 Mixed: some expensive, some diagnostic Both in same scenario Use <code>sc.run()</code> for expensive steps, <code>sc.trace()</code> for cheap validation","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#practical-guidance","level":3,"title":"Practical Guidance","text":"<p>For most scientific workflows, prefer <code>sc.run()</code> when: - Your function is deterministic (no randomness unless seeded in config) - It doesn't have important side effects outside the outputs you log - You can structure it as a pure function (inputs → outputs)</p> <p>Use <code>sc.trace()</code> when: - You need to run initialization or setup code that triggers external systems - You want explicit control over what happens every run vs. only on cache miss - Your step is fast enough that re-execution overhead doesn't matter</p> <p>On large file inputs: If your function receives multi-GB files, set <code>load_inputs=False</code> and use <code>cache_hydration=\"inputs-missing\"</code> to ensure input files are available on cache misses without re-loading on every run.</p> Alternative: log file outputs inside the step  If your function writes files, log them with the injected context (and read inputs yourself if needed):  <pre><code>def beam_preprocess(data_file, _consist_ctx) -&gt; None:\n    out_path = _consist_ctx.run_dir / \"beam_inputs.parquet\"\n    ...\n    _consist_ctx.log_output(out_path, key=\"beam_inputs\")\n\nwith use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        sc.run(\n            name=\"beam_preprocess\",\n            fn=beam_preprocess,\n            inputs={\"data_file\": Path(\"data.parquet\")},\n            load_inputs=False,\n            runtime_kwargs={\"data_file\": Path(\"data.parquet\")},\n            inject_context=True,\n        )\n</code></pre>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#query-facets-with-pivot_facets","level":3,"title":"Query Facets with <code>pivot_facets</code>","text":"<p>Log small, queryable config values (facets) and pivot them into a wide table for analysis. This is a simple way to compare many runs side‑by‑side (for example, a sensitivity analysis across parameters):</p> <pre><code>from sqlmodel import select\nimport consist\n\nparams = consist.pivot_facets(\n    namespace=\"simulate\",\n    keys=[\"alpha\", \"beta\", \"mode\"],\n    value_columns={\"mode\": \"value_str\"},\n)  # (1)!\n\nrows = consist.run_query(\n    select(params.c.run_id, params.c.alpha, params.c.beta, params.c.mode),\n    tracker=tracker,\n)\n</code></pre> <ol> <li>Pivot config facets into columns for analysis.</li> </ol> <p>See Concepts for when to use <code>config</code> vs <code>facet</code>.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#motivation-when-caching-saves-time","level":2,"title":"Motivation: When Caching Saves Time","text":"<p>Caching is most valuable in workflows with many runs and expensive computation. Here are realistic scenarios from scientific domains:</p> <p>Example 1: Land-Use Model Sensitivity Analysis</p> <p>A sensitivity sweep tests 40 parameter combinations (toll levels, parking costs, transit subsidies). - Each ActivitySim run: 30 minutes - Without caching: 40 runs × 30 min = 20 hours - With caching: Base population synthesis (30 min, once) + 39 parameter tweaks with cache hits (5 min each) = 3.75 hours - Time saved: 81% reduction in modeling time</p> <p>Example 2: ActivitySim Calibration Iteration</p> <p>Mode choice coefficients need iterative calibration against observed transit ridership. - Without caching: Repeat all 3 steps = 75 minutes per iteration × 5 iterations = 375 minutes total - With caching: Step 1–2 are cache hits, only step 3 re-executes = 115 minutes - Time saved: 69% reduction; frees analyst time for interpretation</p> <p>Example 3: Grid Dispatch Multi-Scenario Ensemble</p> <p>A baseline scenario and 8 future scenarios all share the same network preprocessing pipeline. - Preprocessing: 3 hours; Each scenario dispatch: 20 minutes - Without caching: 9 × (3 hours + 20 min) = 29.85 hours - With caching: Preprocessing once (3 hours), then 8 scenario runs hit cache on preprocessing = 5.67 hours - Time saved: 81% reduction; enables rapid scenario exploration</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#querying-results","level":2,"title":"Querying Results","text":"","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#finding-runs","level":3,"title":"Finding Runs","text":"<p>See: Example notebooks.</p> <pre><code>import consist\n\nrun = consist.find_run(\n    tracker=tracker,\n    parent_id=\"baseline\",  # (1)!\n    year=2030,\n    model=\"simulate\"\n)\n\nruns_by_year = consist.find_runs(\n    tracker=tracker,\n    parent_id=\"baseline\",\n    model=\"simulate\",\n    index_by=\"year\"\n)\nresult_2030 = runs_by_year[2030]\n</code></pre> <ol> <li>Filter by scenario ID.</li> </ol>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#loading-artifacts","level":3,"title":"Loading Artifacts","text":"<p>See: Example notebooks.</p> <pre><code>artifacts = tracker.get_artifacts_for_run(run.id)\npersons_artifact = artifacts.outputs[\"persons\"]\n\ndf = consist.load_df(persons_artifact)  # (1)!\n</code></pre> <ol> <li>Load the artifact data into a DataFrame.</li> </ol>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#cross-run-queries-with-views","level":3,"title":"Cross-Run Queries with Views","text":"<p>See: Example notebooks.</p> <p>Register schemas to enable SQL queries across all runs:</p> <pre><code>import consist\nfrom sqlmodel import SQLModel, Field, select, func\nfrom consist import Tracker\n\nclass Person(SQLModel, table=True):\n    person_id: int = Field(primary_key=True)\n    age: int\n    number_of_trips: int\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person]\n)\n\nVPerson = tracker.views.Person  # (1)!\n\nquery = (\n    select(\n        VPerson.consist_scenario_id,\n        VPerson.consist_year,\n        func.avg(VPerson.number_of_trips).label(\"avg_trips\")\n    )\n    .where(VPerson.consist_scenario_id.in_([\"baseline\", \"high_gas\"]))\n    .group_by(VPerson.consist_scenario_id, VPerson.consist_year)\n)\n\nresults = consist.run_query(query, tracker=tracker)\n</code></pre> <ol> <li>Views are available after running scenarios and registering schemas.</li> </ol> <p>Views automatically include <code>consist_scenario_id</code>, <code>consist_year</code>, and other metadata columns for filtering and grouping. For more on ingestion and hybrid views, see Data Materialization Strategy.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"usage-guide/#generating-schemas-from-captured-data","level":3,"title":"Generating Schemas from Captured Data","text":"<p>If you ingest tabular data into DuckDB, Consist can capture the observed schema and export an editable SQLModel stub so you can curate PK/FK constraints and then register the model for views. This is useful when you want a stable, documented schema for downstream analysis or audits.</p> <p>You can also opt into lightweight file schema capture when logging CSV/Parquet artifacts by passing <code>profile_file_schema=True</code> (and optionally <code>file_schema_sample_rows=</code>) to <code>log_artifact</code>. These captured schemas are stored in the provenance DB and remain available even if the original files move or are deleted. If you already have a content hash (e.g., after copying or moving a file), pass <code>content_hash=</code> to <code>log_artifact</code> to reuse it without re-hashing the file. For safety, Consist will not overwrite an existing, different hash unless you pass <code>force_hash_override=True</code>. To verify the hash against disk, use <code>validate_content_hash=True</code>.</p> <p>See <code>docs/schema-export.md</code> for the full workflow (CLI + Python) and column-name/<code>__tablename__</code> guidelines. See Data Materialization Strategy for ingestion tradeoffs and DB fallback behavior.</p>","path":["Guides","Usage Guide"],"tags":[]},{"location":"api/","level":1,"title":"API reference","text":"<p>This section documents Consist's public API and key helpers. Start with the public surface area, then drill into the core classes and supporting utilities.</p>","path":["API Reference","API reference"],"tags":[]},{"location":"api/#start-here","level":2,"title":"Start here","text":"<ul> <li>Public API</li> <li>Helpers</li> </ul>","path":["API Reference","API reference"],"tags":[]},{"location":"api/#core-classes","level":2,"title":"Core classes","text":"<ul> <li>Tracker</li> <li>Artifact</li> <li>Run</li> <li>Indexing</li> <li>Identity Manager</li> <li>Workflow Contexts</li> <li>Views</li> <li>Matrix Views</li> <li>Materialization</li> </ul>","path":["API Reference","API reference"],"tags":[]},{"location":"api/api_helpers/","level":1,"title":"API Helpers","text":"","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.RelationConnectionLeakWarning","level":2,"title":"<code>RelationConnectionLeakWarning</code>","text":"<p>               Bases: <code>RuntimeWarning</code></p> <p>Warning emitted when relation connections appear to accumulate.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.view","level":2,"title":"<code>view(model, name=None)</code>","text":"<p>Create a SQLModel class backed by a Consist hybrid view.</p> <p>This is a convenience wrapper around the view factory that lets you define a SQLModel schema for a concept (e.g., a canonicalized config table, an ingested dataset, or a computed artifact view) and then query it as a normal SQLModel table. The returned class is a dynamic subclass with <code>table=True</code> that points at a database view, so you can use it in <code>sqlmodel.Session</code> queries without creating a physical table.</p> <p>If you need explicit control over view naming or want to create multiple named views for the same concept, use <code>Tracker.create_view(...)</code> or <code>Tracker.view(...)</code> directly.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[T]</code> <p>Base SQLModel describing the schema (columns and types).</p> required <code>name</code> <code>Optional[str]</code> <p>Optional override for the generated view name (defaults to the model's table name).</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[T]</code> <p>SQLModel subclass with <code>table=True</code> pointing at the hybrid view.</p> <p>Examples:</p> <pre><code>from sqlmodel import Session, select\nfrom consist import view\nfrom consist.models.activitysim import ActivitySimConstantsCache\n\n# Create a dynamic view class for querying constants\nConstantsView = view(ActivitySimConstantsCache)\n\nwith Session(tracker.engine) as session:\n    rows = session.exec(\n        select(ConstantsView)\n        .where(ConstantsView.key == \"sample_rate\")\n    ).all()\n</code></pre>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.use_tracker","level":2,"title":"<code>use_tracker(tracker)</code>","text":"<p>Set a fallback (default) tracker for Consist API entrypoints.</p> <p>This configures which tracker is used by consist.run(), consist.start_run(), etc. when called outside an active run context (i.e., when the tracker stack is empty). Once inside a run, the tracker becomes \"active\" via push_tracker() and is accessed by logging functions like consist.log_artifact().</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.run","level":2,"title":"<code>run(fn=None, name=None, *, tracker=None, **kwargs)</code>","text":"<p>Execute a function as a tracked Consist run.</p> <p>This is a high-level entrypoint that wraps a function call in a Consist run. It automatically handles run start/end and result capturing.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Optional[Callable]</code> <p>The function to execute.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>A semantic name for the run. Defaults to the function name.</p> <code>None</code> <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to use.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>Tracker.run</code>, such as <code>inputs</code>, <code>tags</code>, or <code>runtime_kwargs</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>RunResult</code> <p>An object containing the function's return value and the recorded <code>Run</code> record.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.trace","level":2,"title":"<code>trace(name, *, tracker=None, **kwargs)</code>","text":"<p>Create a nested tracing context within an active run.</p> <p>Use <code>trace</code> to break down a large run into smaller, logical steps. Each traced step is recorded as a sub-run with its own inputs and outputs.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Semantic name for this step/trace.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to use.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata or parameters for this trace.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The active tracker instance.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.start_run","level":2,"title":"<code>start_run(run_id, model, tracker=None, **kwargs)</code>","text":"<p>Initiate and manage a Consist run.</p> <p>This context manager marks the beginning of a discrete unit of work (a \"run\"). All artifacts logged within this context will be associated with this run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>A unique identifier for this run.</p> required <code>model</code> <code>str</code> <p>The name of the model or system being run.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to use. Defaults to the active global tracker.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for the run, such as <code>tags</code>, <code>config</code>, or <code>inputs</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The active tracker instance.</p> Example <p>with consist.start_run(\"run_123\", \"my_model\"): ...     consist.log_artifact(\"data.csv\", \"input_data\")</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.define_step","level":2,"title":"<code>define_step(*, outputs=None, tags=None, description=None)</code>","text":"<p>Attach metadata to a function without changing execution behavior.</p> <p>This decorator lets you attach defaults such as <code>outputs</code> or <code>tags</code> to a function. <code>Tracker.run</code> and <code>ScenarioContext.run</code> read this metadata.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.scenario","level":2,"title":"<code>scenario(name, tracker=None, *, enabled=True, **kwargs)</code>","text":"<p>Proxy for <code>Tracker.scenario</code> to avoid importing the tracker directly.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the scenario (used for the header run ID).</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to use; defaults to the active global tracker.</p> <code>None</code> <code>enabled</code> <code>bool</code> <p>If False, returns a noop scenario context that executes without provenance tracking while preserving Coupler/RunResult ergonomics.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments forwarded to <code>Tracker.scenario</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>ScenarioContext</code> <p>Scenario context manager.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.single_step_scenario","level":2,"title":"<code>single_step_scenario(name, step_name=None, tracker=None, **kwargs)</code>","text":"<p>Convenience wrapper that exposes a single step scenario.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the scenario header.</p> required <code>step_name</code> <code>Optional[str]</code> <p>Name for the single step; defaults to <code>name</code>.</p> <code>None</code> <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker to execute the scenario; defaults to the active tracker.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments forwarded to <code>Tracker.scenario</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>ScenarioContext</code> <p>Scenario context manager for the single step.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.current_tracker","level":2,"title":"<code>current_tracker()</code>","text":"<p>Retrieves the active <code>Tracker</code> instance from the global context.</p> <p>If no run is active, this function falls back to the default tracker (if set via <code>consist.use_tracker</code> or <code>consist.set_current_tracker</code>).</p> <p>Returns:</p> Type Description <code>Tracker</code> <p>The <code>Tracker</code> instance currently active in the execution context.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no <code>Tracker</code> is active in the current context and no default tracker has been configured.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.current_run","level":2,"title":"<code>current_run()</code>","text":"<p>Return the active <code>Run</code> record if one is in progress, otherwise <code>None</code>.</p> <p>A <code>Run</code> record contains metadata about the current execution, such as its unique ID, model name, and start time.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.current_consist","level":2,"title":"<code>current_consist()</code>","text":"<p>Return the active <code>ConsistRecord</code> if one is in progress, otherwise <code>None</code>.</p> <p>The <code>ConsistRecord</code> is the internal state object that tracks the active run's inputs, outputs, and metadata during execution.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.cached_artifacts","level":2,"title":"<code>cached_artifacts(direction='output')</code>","text":"<p>Returns hydrated cached artifacts for the active run, if any.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>str</code> <p>\"output\" or \"input\".</p> <code>\"output\"</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping from artifact key to Artifact, or empty dict if no cache hit.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.cached_output","level":2,"title":"<code>cached_output(key=None)</code>","text":"<p>Fetch a hydrated cached output artifact for the active run.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[str]</code> <p>Specific artifact key to look up; defaults to the first available artifact.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>Cached artifact instance or <code>None</code> if no cache hit exists.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.get_artifact","level":2,"title":"<code>get_artifact(run_id, key=None, key_contains=None, direction='output')</code>","text":"<p>Retrieve a single artifact from a historical run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Identifier of the run that produced the artifact.</p> required <code>key</code> <code>Optional[str]</code> <p>Exact artifact key to match.</p> <code>None</code> <code>key_contains</code> <code>Optional[str]</code> <p>Substring filter for artifact keys.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Either \"input\" or \"output\".</p> <code>\"output\"</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>Matching artifact or <code>None</code> if not found.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_artifact","level":2,"title":"<code>log_artifact(path, key=None, direction='output', schema=None, driver=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', *, enabled=True, **meta)</code>","text":"<p>Logs an artifact (file or data reference) to the currently active run.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.log_artifact</code>. It automatically links the artifact to the current run context, handles path virtualization, and performs lineage discovery.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact (e.g., \"households\"). Required if <code>path</code> is a path-like (str/Path).</p> <code>None</code> <code>direction</code> <code>str</code> <p>Specifies whether the artifact is an \"input\" or \"output\" for the current run. Defaults to \"output\".</p> <code>\"output\"</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the artifact's data. Its name will be stored in artifact metadata.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify the driver (e.g., 'h5_table'). If None, the driver is inferred from the file extension.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>reuse_if_unchanged</code> <code>bool</code> <p>If True and logging an output, reuse a prior artifact row when the content hash matches.</p> <code>False</code> <code>reuse_scope</code> <code>(same_uri, any_uri)</code> <p>Scope for output reuse checks. \"same_uri\" restricts reuse to the same URI, while \"any_uri\" allows reuse across different URIs with the same hash.</p> <code>\"same_uri\"</code> <code>enabled</code> <code>bool</code> <p>If False, returns a noop artifact object without requiring an active run.</p> <code>True</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's flexible <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If <code>key</code> is not provided when <code>path</code> is a path-like (str/Path).</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_artifacts","level":2,"title":"<code>log_artifacts(outputs, *, direction='output', driver=None, metadata_by_key=None, reuse_if_unchanged=False, reuse_scope='same_uri', enabled=True, **shared_meta)</code>","text":"<p>Log multiple artifacts in a single call for efficiency.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.log_artifacts</code>. It logs a batch of related artifacts with optional per-key metadata customization.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Mapping[str, ArtifactRef]</code> <p>Mapping of key -&gt; path/Artifact to log. Keys must be strings and explicitly chosen by the caller (not inferred from filenames).</p> required <code>direction</code> <code>str</code> <p>\"input\" or \"output\" for the current run context.</p> <code>\"output\"</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify driver for all artifacts. If None, inferred from file extension.</p> <code>None</code> <code>metadata_by_key</code> <code>Optional[Mapping[str, Dict[str, Any]]]</code> <p>Per-key metadata overrides applied on top of shared metadata.</p> <code>None</code> <code>enabled</code> <code>bool</code> <p>If False, returns noop artifact objects without requiring an active run.</p> <code>True</code> <code>**shared_meta</code> <code>Any</code> <p>Metadata key-value pairs applied to ALL logged artifacts.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[str, ArtifactLike]</code> <p>Mapping of key -&gt; logged Artifact-like objects.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If metadata_by_key contains keys not in outputs, or if any value is None.</p> <code>TypeError</code> <p>If mapping keys are not strings.</p> <p>Examples:</p> <p>Log multiple outputs with shared metadata: <pre><code>artifacts = consist.log_artifacts(\n    {\n        \"persons\": \"results/persons.parquet\",\n        \"households\": \"results/households.parquet\",\n        \"jobs\": \"results/jobs.parquet\"\n    },\n    year=2030,\n    scenario=\"base\"\n)\n# All three artifacts get year=2030 and scenario=\"base\"\n</code></pre></p> <p>Mix shared and per-key metadata: <pre><code>artifacts = consist.log_artifacts(\n    {\n        \"persons\": \"output/persons.parquet\",\n        \"households\": \"output/households.parquet\"\n    },\n    metadata_by_key={\n        \"households\": {\"role\": \"primary_unit\", \"weight\": 1.0}\n    },\n    dataset_version=\"v2\",\n    simulation_id=\"run_001\"\n)\n# persons gets: dataset_version=\"v2\", simulation_id=\"run_001\"\n# households gets: dataset_version=\"v2\", simulation_id=\"run_001\",\n#                  role=\"primary_unit\", weight=1.0\n</code></pre></p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_input","level":2,"title":"<code>log_input(path, key=None, *, schema=None, driver=None, content_hash=None, force_hash_override=False, validate_content_hash=False, enabled=True, **meta)</code>","text":"<p>Log an input artifact to the active run.</p> <p>An input artifact represents a data source that the current run reads from. Logging it creates a lineage link, allowing Consist to track which versions of data produced which results.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>File path (str/Path) or an existing <code>Artifact</code> object.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact (e.g. \"raw_households\"). Required if <code>path</code> is a file path.</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Optional SQLModel class defining the data structure.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicit format driver (e.g. \"parquet\"). Inferred from extension if None.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed hash to avoid re-hashing large files.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>Overwrite existing hash in the database if different from <code>content_hash</code>.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>Re-hash the file on disk to ensure it matches the provided <code>content_hash</code>.</p> <code>False</code> <code>enabled</code> <code>bool</code> <p>If False, returns a dummy artifact object for disconnected execution.</p> <code>True</code> <code>**meta</code> <code>Any</code> <p>Additional metadata fields to store with the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ArtifactLike</code> <p>The logged artifact reference.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_output","level":2,"title":"<code>log_output(path, key=None, *, schema=None, driver=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', enabled=True, **meta)</code>","text":"<p>Log an output artifact produced by the current run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>File path (str/Path) or an existing <code>Artifact</code> object.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact (e.g. \"processed_results\"). Required if <code>path</code> is a file path.</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Optional SQLModel class defining the data structure.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicit format driver (e.g. \"parquet\"). Inferred from extension if None.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed hash to avoid re-hashing large files.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>Overwrite existing hash in the database if different from <code>content_hash</code>.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>Re-hash the file on disk to ensure it matches the provided <code>content_hash</code>.</p> <code>False</code> <code>reuse_if_unchanged</code> <code>bool</code> <p>If True, and the content hash matches a previous run's output, Consist may reuse that historical artifact record.</p> <code>False</code> <code>reuse_scope</code> <code>(same_uri, any_uri)</code> <p>Whether to restrict reuse to the exact same file path or any path with the same hash.</p> <code>\"same_uri\"</code> <code>enabled</code> <code>bool</code> <p>If False, returns a dummy artifact object.</p> <code>True</code> <code>**meta</code> <code>Any</code> <p>Additional metadata fields to store.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ArtifactLike</code> <p>The logged artifact reference.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_dataframe","level":2,"title":"<code>log_dataframe(df, key, schema=None, direction='output', tracker=None, path=None, driver=None, meta=None, **to_file_kwargs)</code>","text":"<p>Serialize a DataFrame, log it as an artifact, and trigger optional ingestion.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data to persist.</p> required <code>key</code> <code>str</code> <p>Logical artifact key.</p> required <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Schema used for ingestion, if provided.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Artifact direction relative to the run.</p> <code>\"output\"</code> <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to use; defaults to the active tracker. If None and no active run context exists, raises RuntimeError.</p> <code>None</code> <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Output path; defaults to <code>&lt;run_dir&gt;/outputs/&lt;run_subdir&gt;/&lt;key&gt;.&lt;driver&gt;</code> for the active run as determined by the tracker's run subdir configuration.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>File format driver (e.g., \"parquet\" or \"csv\").</p> <code>None</code> <code>meta</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the artifact.</p> <code>None</code> <code>**to_file_kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>pd.DataFrame.to_parquet</code> or <code>to_csv</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The artifact logged for the written dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested driver is unsupported.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_meta","level":2,"title":"<code>log_meta(**kwargs)</code>","text":"<p>Updates the active run's metadata with the provided key-value pairs.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.log_meta</code>. It allows users to log additional information about the current run, such as performance metrics, experimental parameters, or tags, directly to the run's metadata. This information is then persisted to both the JSON log and the DuckDB database.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary key-value pairs to merge into the <code>meta</code> dictionary of the current run. Existing keys will be updated, and new keys will be added.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called when no <code>Tracker</code> is active in the current context.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.ingest","level":2,"title":"<code>ingest(artifact, data=None, schema=None, run=None)</code>","text":"<p>Ingests data associated with an <code>Artifact</code> into the active run's database.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.ingest</code>. It materializes data into the DuckDB, leveraging <code>dlt</code> for efficient loading and injecting provenance information. This is part of the \"Hot Data Strategy\".</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact object representing the data being ingested.</p> required <code>data</code> <code>Optional[Union[Iterable[Dict[str, Any]], Any]]</code> <p>The data to ingest. Can be an iterable of dictionaries (rows), a file path (str or Path), a Pandas DataFrame, or any other data type that <code>dlt</code> can handle. If <code>None</code>, Consist attempts to read the data directly from the artifact's resolved file path.</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the ingested data. If provided, <code>dlt</code> will use this for strict validation and schema inference.</p> <code>None</code> <code>run</code> <code>Optional[Run]</code> <p>The <code>Run</code> context for ingestion. If provided, the data will be tagged with this run's ID (Offline Mode). If <code>None</code>, it defaults to the currently active run (Online Mode).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result information from the underlying <code>dlt</code> ingestion process.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no database is configured for the active <code>Tracker</code> or if <code>ingest</code> is called outside of an active run context.</p> <code>Exception</code> <p>Any exception raised by the underlying <code>dlt</code> ingestion process.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.register_views","level":2,"title":"<code>register_views(*models)</code>","text":"<p>Register hybrid view models for the active tracker.</p> <p>Parameters:</p> Name Type Description Default <code>*models</code> <code>Type[SQLModel]</code> <p>SQLModel classes describing the schema of hybrid views.</p> <code>()</code> <p>Returns:</p> Type Description <code>Dict[str, Type[SQLModel]]</code> <p>Mapping from model class name to the generated view model.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.find_run","level":2,"title":"<code>find_run(tracker=None, **filters)</code>","text":"<p>Convenience proxy for <code>Tracker.find_run</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to query; defaults to the active tracker.</p> <code>None</code> <code>**filters</code> <code>Any</code> <p>Filter values forwarded to <code>Tracker.find_run</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>Matching run or <code>None</code> when no match exists.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.find_runs","level":2,"title":"<code>find_runs(tracker=None, **filters)</code>","text":"<p>Convenience proxy for <code>Tracker.find_runs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to query; defaults to the active tracker.</p> <code>None</code> <code>**filters</code> <code>Any</code> <p>Filter values forwarded to <code>Tracker.find_runs</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[list[Run], Dict[Hashable, Run]]</code> <p>Results returned by <code>Tracker.find_runs</code>.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.db_session","level":2,"title":"<code>db_session(tracker=None)</code>","text":"<p>Provide a SQLModel <code>Session</code> connected to the tracker's database.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance supplying the engine; defaults to active tracker.</p> <code>None</code> <p>Yields:</p> Type Description <code>Session</code> <p>SQLModel session bound to the tracker engine.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.run_query","level":2,"title":"<code>run_query(query, tracker=None)</code>","text":"<p>Execute a SQLModel/SQLAlchemy query via the tracker engine.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Executable</code> <p>Query object (<code>select</code>, etc.).</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance supplying the engine; defaults to the active tracker.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>Results of the executed query.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.pivot_facets","level":2,"title":"<code>pivot_facets(*, namespace, keys, value_column='value_num', value_columns=None, label_prefix='', label_map=None, run_id_label='run_id', table=RunConfigKV)</code>","text":"<p>Build a pivoted facet subquery keyed by run_id.</p> <p>This is a convenience helper for turning flattened config facets (<code>run_config_kv</code>) into a wide table suitable for joins.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Optional[str]</code> <p>Facet namespace to filter by (typically the model name). If <code>None</code>, the namespace filter is skipped.</p> required <code>keys</code> <code>Iterable[str]</code> <p>Facet keys to pivot into columns.</p> required <code>value_column</code> <code>str</code> <p>Default value column to read from. Must be one of: <code>value_num</code>, <code>value_str</code>, <code>value_bool</code>, <code>value_json</code>.</p> <code>\"value_num\"</code> <code>value_columns</code> <code>Optional[Mapping[str, str]]</code> <p>Optional per-key override of <code>value_column</code>.</p> <code>None</code> <code>label_prefix</code> <code>str</code> <p>Optional prefix for pivoted column labels.</p> <code>\"\"</code> <code>label_map</code> <code>Optional[Mapping[str, str]]</code> <p>Optional per-key label overrides.</p> <code>None</code> <code>run_id_label</code> <code>str</code> <p>Label to use for the run id column in the returned subquery.</p> <code>\"run_id\"</code> <code>table</code> <code>Type[RunConfigKV]</code> <p>Table/model providing the facet KV rows.</p> <code>RunConfigKV</code> <p>Returns:</p> Type Description <code>Any</code> <p>A SQLAlchemy subquery with columns: <code>run_id</code> and one column per key.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.capture_outputs","level":2,"title":"<code>capture_outputs(directory, pattern='*', recursive=False)</code>","text":"<p>Context manager to automatically capture and log new or modified files in a directory within the current active run context.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.capture_outputs</code>. It watches a specified <code>directory</code> for any file changes (creations or modifications) that occur within its <code>with</code> block. These changes are then automatically logged as output artifacts of the current Consist run.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>The path to the directory to monitor for new or modified files.</p> required <code>pattern</code> <code>str</code> <p>A glob pattern (e.g., \".csv\", \"data_.parquet\") to filter which files are captured within the specified directory. Defaults to all files.</p> <code>\"*\"</code> <code>recursive</code> <code>bool</code> <p>If True, the capture will recursively scan subdirectories within <code>directory</code> for changes.</p> <code>False</code> <p>Yields:</p> Type Description <code>OutputCapture</code> <p>An <code>OutputCapture</code> object containing a list of <code>Artifact</code> objects that were captured and logged after the <code>with</code> block finishes.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>capture_outputs</code> is used outside of an active <code>start_run</code> context.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.load","level":2,"title":"<code>load(artifact, tracker=None, *, db_fallback='inputs-only', **kwargs)</code>","text":"<pre><code>load(artifact: ZarrArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; xarray.Dataset\n</code></pre><pre><code>load(artifact: NetCdfArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; xarray.Dataset\n</code></pre><pre><code>load(artifact: OpenMatrixArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; xarray.Dataset\n</code></pre><pre><code>load(artifact: SpatialArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; geopandas.GeoDataFrame\n</code></pre><pre><code>load(artifact: HdfStoreArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; pd.HDFStore\n</code></pre><pre><code>load(artifact: DataFrameArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; duckdb.DuckDBPyRelation\n</code></pre><pre><code>load(artifact: JsonArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; duckdb.DuckDBPyRelation\n</code></pre><pre><code>load(artifact: TabularArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; duckdb.DuckDBPyRelation\n</code></pre><pre><code>load(artifact: Artifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; LoadResult\n</code></pre><pre><code>load(artifact: ArtifactLike, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; LoadResult\n</code></pre> <p>Smart loader that retrieves data for an artifact from the best available source.</p> <p>This function attempts to load the data associated with an <code>Artifact</code> object. It prioritizes loading from disk (raw format) if the file exists. If the file is missing but the artifact is marked as ingested, it can optionally recover the data from the Consist DuckDB database (\"Ghost Mode\"). By default, DB recovery is only allowed when the artifact is an input to an active, non-cached run.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The Consist <code>Artifact</code> object whose data is to be loaded.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>The <code>Tracker</code> instance to use for path resolution and database access. If <code>None</code>, the function attempts to use the active global tracker context. Explicitly passing a <code>tracker</code> is recommended for clarity or when no global context is available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the underlying data loader function (e.g., <code>pd.read_parquet</code>, <code>pd.read_csv</code>, <code>xr.open_zarr</code>, <code>pd.read_sql</code>).</p> <code>{}</code> <code>db_fallback</code> <code>str</code> <p>Controls when the loader is allowed to fall back to DuckDB (\"Ghost Mode\") when the file is missing but the artifact is marked as ingested.</p> <ul> <li>\"inputs-only\": allow DB fallback only if the artifact is declared as an input   to the current active run AND the current run is not a cache hit.</li> <li>\"always\": allow DB fallback whenever <code>artifact.meta[\"is_ingested\"]</code> is true and   a tracker with a DB connection is available.</li> <li>\"never\": disable DB fallback entirely.</li> </ul> <code>\"inputs-only\"</code> <p>Returns:</p> Type Description <code>LoadResult</code> <p>The loaded data, typically a DuckDB Relation for tabular data, an xarray Dataset for array formats, or another data object depending on the artifact's <code>driver</code> and the data format.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no <code>Tracker</code> instance can be resolved (neither provided nor active in context) and the artifact's absolute path is not directly resolvable. Also if the artifact is marked as ingested but no tracker with a DB connection is available.</p> <code>FileNotFoundError</code> <p>If the artifact's data cannot be found on disk or recovered from the database.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.load_df","level":2,"title":"<code>load_df(artifact, tracker=None, *, db_fallback='inputs-only', close=True, **kwargs)</code>","text":"<p>Load a tabular artifact and return a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to load.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker to use for resolving paths or DB fallback.</p> <code>None</code> <code>db_fallback</code> <code>str</code> <p>Controls when DB recovery is allowed for ingested artifacts.</p> <code>\"inputs-only\"</code> <code>close</code> <code>bool</code> <p>Whether to close the underlying DuckDB connection after materialization when the load returns a Relation.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional loader options.</p> <code>{}</code>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.load_relation","level":2,"title":"<code>load_relation(artifact, tracker=None, *, db_fallback='inputs-only', **kwargs)</code>","text":"<p>Context manager that yields a DuckDB Relation and ensures the underlying connection is closed on exit.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.to_df","level":2,"title":"<code>to_df(relation, *, close=True)</code>","text":"<p>Convert a DuckDB Relation to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>relation</code> <code>DuckDBPyRelation</code> <p>Relation to materialize into a DataFrame.</p> required <code>close</code> <code>bool</code> <p>Whether to close the underlying DuckDB connection after materialization. Use <code>close=False</code> if you plan to continue using the relation.</p> <code>True</code>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.active_relation_count","level":2,"title":"<code>active_relation_count()</code>","text":"<p>Return the number of active DuckDB relations tracked by Consist.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.set_current_tracker","level":2,"title":"<code>set_current_tracker(tracker)</code>","text":"<p>Set the default (fallback) tracker used by Consist entrypoints.</p> <p>Entrypoints like <code>consist.run()</code>, <code>consist.start_run()</code>, and <code>consist.scenario()</code> use this tracker if they are called outside of an active run context and no explicit <code>tracker=</code> argument is provided.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to set as the default, or <code>None</code> to clear.</p> required <p>Returns:</p> Type Description <code>Optional[Tracker]</code> <p>The previously configured default tracker, if any.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.noop_scenario","level":2,"title":"<code>noop_scenario(name, **kwargs)</code>","text":"<p>Creates a scenario context that executes without provenance tracking.</p> <p>This is useful for debugging or running simulations where you want the ergonomics of the Consist scenario API (like the Coupler and RunResult) but do not want to record any metadata or artifacts to the database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the scenario (for display/logging purposes).</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments forwarded to the noop context.</p> <code>{}</code>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_dataframe_artifact","level":2,"title":"<code>is_dataframe_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to tabular types (parquet, csv, h5_table).</p> <p>Use this to enable type-safe loading and IDE autocomplete:</p> <pre><code>if is_dataframe_artifact(artifact):\n    rel = load(artifact)  # Type checker knows return is Relation\n    df.head()  # IDE autocomplete works!\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is parquet, csv, or h5_table.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_tabular_artifact","level":2,"title":"<code>is_tabular_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to any tabular format (parquet, csv, h5_table, json).</p> <p>Note: This is broader than <code>is_dataframe_artifact()</code>, so <code>load()</code> returns a Relation for tabular artifacts. Use <code>load_df()</code> for a pandas escape hatch.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver produces tabular data.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_json_artifact","level":2,"title":"<code>is_json_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is json.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_zarr_artifact","level":2,"title":"<code>is_zarr_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to Zarr format.</p> <p>Use this when you know an artifact should be Zarr and want type-safe loading:</p> <pre><code>if is_zarr_artifact(artifact):\n    ds = load(artifact)  # Type checker knows return is xarray.Dataset\n    ds.dims  # IDE autocomplete works!\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is zarr.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_hdf_artifact","level":2,"title":"<code>is_hdf_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to HDF5 format (h5 or hdf5).</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is h5 or hdf5.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_netcdf_artifact","level":2,"title":"<code>is_netcdf_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to NetCDF format.</p> <p>Use this when you know an artifact should be NetCDF and want type-safe loading:</p> <pre><code>if is_netcdf_artifact(artifact):\n    ds = load(artifact)  # Type checker knows return is xarray.Dataset\n    ds.dims  # IDE autocomplete works!\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is netcdf.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_openmatrix_artifact","level":2,"title":"<code>is_openmatrix_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to OpenMatrix format.</p> <p>Use this when you know an artifact should be OpenMatrix and want type-safe loading:</p> <pre><code>if is_openmatrix_artifact(artifact):\n    matrix_data = load(artifact)  # Type checker knows return is appropriate type\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is openmatrix.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_spatial_artifact","level":2,"title":"<code>is_spatial_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to spatial formats (GeoDataFrame outputs).</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is geojson, shapefile, or geopackage.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/artifact/","level":1,"title":"Artifact","text":"<p>               Bases: <code>SQLModel</code></p> <p>Represents a physical data object in the Consist database.</p> <p>This table stores canonical metadata for any file/dataset Consist tracks. It is linked to runs via <code>run_artifact_link</code> to record whether an artifact was an input or output. The <code>run_id</code> field records the producing run (if any) and is often <code>None</code> for external inputs.</p> <p>Artifacts are the core building blocks of provenance and caching. Each artifact has a unique identity, a virtualized location, and rich metadata, supporting both \"hot\" (ingested) and \"cold\" (file-based) data strategies.</p> <p>Attributes:     id (uuid.UUID): A unique identifier for the artifact.     key (str): A semantic, human-readable name for the artifact (e.g., \"households\", \"parcels\").     container_uri (str): A portable, virtualized Uniform Resource Identifier (URI) for the                artifact's location (e.g., \"inputs://land_use.csv\").     table_path (Optional[str]): Optional path inside a container (e.g., \"/tables/households\").     array_path (Optional[str]): Optional path inside a container for array artifacts.     driver (str): The name of the format handler used to read or write the artifact                   (e.g., \"parquet\", \"csv\", \"zarr\").     hash (Optional[str]): SHA256 content hash of the artifact's data, enabling content-addressable                           lookups and deduplication.     run_id (Optional[str]): The ID of the run that generated this artifact. Null for inputs.     meta (Dict[str, Any]): A flexible JSON field for storing arbitrary metadata, such as                            schema signatures, or data dimensions.     created_at (datetime): The timestamp when the artifact was first logged.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.abs_path","level":2,"title":"<code>abs_path</code>  <code>property</code> <code>writable</code>","text":"<p>Runtime-only helper to access the absolute path of this artifact.</p> <p>This property provides the resolved absolute file system path for the artifact. It is not persisted to the database but is crucial for local file operations and for chaining Consist runs within the same script or environment.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The absolute file system path of the artifact, or <code>None</code> if it has not yet been resolved or set.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.path","level":2,"title":"<code>path</code>  <code>property</code>","text":"<p>Resolve this artifact to a filesystem Path.</p> <p>Uses the tracker when available to handle mount-aware URIs; otherwise falls back to the cached absolute path or the raw URI.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.is_matrix","level":2,"title":"<code>is_matrix</code>  <code>property</code>","text":"<p>Indicates if the artifact represents a multi-dimensional array or matrix-like data.</p> <p>This property helps in dispatching to appropriate data loaders or processing functions that handle array-based data structures, such as those typically found in scientific computing.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the artifact's driver is associated with matrix-like data formats (e.g., Zarr, HDF5, NetCDF, OpenMatrix), False otherwise.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.is_tabular","level":2,"title":"<code>is_tabular</code>  <code>property</code>","text":"<p>Indicates if the artifact represents tabular data (rows and columns).</p> <p>This property assists in identifying artifacts that can be loaded and processed using tools designed for structured, record-based data, such as Pandas DataFrames.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the artifact's driver is associated with tabular data formats (e.g., Parquet, CSV, SQL), False otherwise.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.created_at_iso","level":2,"title":"<code>created_at_iso</code>  <code>property</code>","text":"<p>Return created_at as an ISO 8601 formatted string.</p> <p>Useful for serialization to external systems that expect string timestamps.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The created_at timestamp as ISO 8601 string, or None if not set.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.get_meta","level":2,"title":"<code>get_meta(key, default=None)</code>","text":"<p>Safely retrieves a value from the 'meta' dictionary.</p> <p>Args:     key (str): The key to look up in the metadata.     default (Any, optional): The default value to return if the key is not found.                              Defaults to None.</p> <p>Returns:     Any: The value associated with the key, or the default value if the key is not present.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/identity/","level":1,"title":"Identity Manager","text":"<p>Manages the cryptographic identity of a Run, which is fundamental to Consist's reproducibility and caching features.</p> <p>To achieve robust caching and \"run forking\", a Run's identity is defined as a composite SHA256 hash, ensuring that any change in code, configuration, or input provenance results in a unique run signature.</p> <p>H_run = SHA256( H_code + H_config + H_inputs )</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.canonical_json_str","level":2,"title":"<code>canonical_json_str(obj)</code>","text":"<p>Return a stable JSON string for hashing/IDs.</p> <p>Uses <code>_clean_structure</code> to normalize types and then dumps with deterministic key ordering and compact separators.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.canonical_json_sha256","level":2,"title":"<code>canonical_json_sha256(obj)</code>","text":"<p>SHA256 hex digest of <code>canonical_json_str(obj)</code>.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.normalize_json","level":2,"title":"<code>normalize_json(obj)</code>","text":"<p>Normalize Python structures into JSON-friendly types.</p> <p>This mirrors the canonical hashing cleanup but preserves the full structure without excluding any keys.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.calculate_run_signature","level":2,"title":"<code>calculate_run_signature(code_hash, config_hash, input_hash)</code>","text":"<p>Computes the final cryptographic signature (cache key) for a run.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.get_code_version","level":2,"title":"<code>get_code_version()</code>","text":"<p>Retrieves the global 'Code Identity' using the Git Commit SHA.</p> <p>This uses GitPython directly to avoid subprocess overhead and parsing fragility.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_callable_hash","level":2,"title":"<code>compute_callable_hash(func, strategy='module', extra_deps=None)</code>","text":"<p>Computes a hash for a specific Python function/callable.</p> <p>This allows for granular caching (ignoring global repo changes) by focusing on the relevant code.</p> Strategies: <p>'source':     Hashes ONLY the function's source code (via <code>inspect.getsource</code>).     Use this for pure functions with no external dependencies. 'module':     Hashes the entire file (.py) where the function is defined.     This is the robust \"in-between\": it captures helper functions and     constants in the same file, but ignores changes in unrelated files.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to hash.</p> required <code>strategy</code> <code>str</code> <p>The hashing strategy (\"source\" or \"module\").</p> <code>\"module\"</code> <code>extra_deps</code> <code>List[str]</code> <p>List of additional file paths (relative to project root) that this function depends on. Their content will be mixed into the hash.</p> <code>None</code>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_config_hash","level":2,"title":"<code>compute_config_hash(config, exclude_keys=None)</code>","text":"<p>Generates a deterministic SHA256 hash of the configuration dictionary.</p> <p>This method implements \"Canonical Config Hashing\" by: 1.  Removing specified <code>exclude_keys</code> (e.g., non-deterministic values like timestamps,     or sensitive information that should not affect reproducibility).</p> <ol> <li> <p>Converting any NumPy types to native Python types, addressing \"The NumPy Problem\"     to prevent serialization errors and ensure consistent hashing across different environments.</p> </li> <li> <p>Recursively sorting dictionary keys to guarantee a canonical JSON representation     regardless of the original insertion order.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to hash.</p> required <code>exclude_keys</code> <code>Optional[List[str]]</code> <p>A list of keys whose values should be excluded from the hashing process. Defaults to an empty list.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A SHA256 hex digest of the canonicalized configuration.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_run_config_hash","level":2,"title":"<code>compute_run_config_hash(*, config, model, year=None, iteration=None)</code>","text":"<p>Compute a config hash for a run, mixing in identity-relevant run fields.</p> <p>Tracker persists <code>config</code> for human inspection, but caching identity needs to include some run context fields that are frequently semantically relevant.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_input_hash","level":2,"title":"<code>compute_input_hash(inputs, path_resolver=None, signature_lookup=None)</code>","text":"<p>Generates a deterministic hash representing the state of all input artifacts.</p> <p>This hash contributes to the Merkle DAG by incorporating the unique identities of all inputs, ensuring that a change in any upstream data source results in a new run signature. It handles two main scenarios for inputs:</p> <ol> <li> <p>Provenance Exists: If an <code>Artifact</code> object is linked to a previous run     (<code>artifact.run_id</code> is present), its <code>run_id</code> is used as its identity,     forming a direct link in the provenance graph. This enables \"run forking\"     and efficient cache invalidation.</p> </li> <li> <p>Raw File Input: If the input is a raw file not previously generated by     Consist (i.e., <code>artifact.run_id</code> is None), its content or metadata is hashed     (based on <code>hashing_strategy</code>) to establish its identity. A <code>path_resolver</code>     is required in this case to access the file on the local filesystem.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Artifact]</code> <p>A list of <code>Artifact</code> objects representing the inputs to the run.</p> required <code>path_resolver</code> <code>Optional[Callable[[str], str]]</code> <p>A function that takes an <code>Artifact</code> URI (a portable string like \"inputs://data.csv\") and returns an absolute file path on the local filesystem. This is required for hashing the content of raw files.</p> <code>None</code> <code>signature_lookup</code> <code>Optional[Callable[[str], str]]</code> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A SHA256 hex digest representing the combined and ordered identity of all inputs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a raw file artifact needs to be hashed but no <code>path_resolver</code> function is provided.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_file_checksum","level":2,"title":"<code>compute_file_checksum(file_path)</code>","text":"<p>Computes a cryptographic identifier for a given file or directory based on the configured hashing strategy.</p> <p>This method is critical for establishing the unique identity of raw file-based inputs to a Consist run. It supports two main strategies: 'full' (content-based) and 'fast' (metadata-based), and handles both single files and directories.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The absolute path to the file or directory for which to compute the checksum.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A SHA256 hex digest representing the checksum or identity of the file/directory.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified <code>file_path</code> does not exist on the filesystem.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If 'full' content hashing is performed on a directory, as this can be computationally expensive for large directories.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.label_for_hash_input","level":2,"title":"<code>label_for_hash_input(path)</code>","text":"<p>Create a stable, human-friendly label for a hash input path.</p> <p>This is used when recording inputs that are represented only by their hash (e.g., \"hash-only\" config inputs). The method prefers a path that is relative to <code>project_root</code> for readability and portability, and falls back to the original string if it cannot be made relative.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>A file or directory path used as a hash input.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string label suitable for logs and provenance records.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.digest_path","level":2,"title":"<code>digest_path(path, *, ignore_dotfiles=True, allowlist=None)</code>","text":"<p>Digest a file or directory with optional filtering.</p> <ul> <li>Files: delegated to <code>compute_file_checksum</code> (honors hashing_strategy).</li> <li>Directories: deterministic digest over relative paths + (content or metadata).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ignore_dotfiles</code> <code>bool</code> <p>If True, ignore any file whose relative path includes a component starting with '.'.</p> <code>True</code> <code>allowlist</code> <code>Optional[List[str]]</code> <p>If provided, only include files whose relative path matches at least one glob pattern.</p> <code>None</code>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_hash_inputs_digests","level":2,"title":"<code>compute_hash_inputs_digests(hash_inputs, *, ignore_dotfiles=True, allowlist=None)</code>","text":"<p>Compute digests for external \"hash-only\" config inputs (files or directories).</p> <p>Items may be: - A path (str/Path): label derived from project-relative path when possible. - A (label, path) tuple: explicit label.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/indexing/","level":1,"title":"Indexing Helpers","text":"","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.RunIndexField","level":2,"title":"<code>RunIndexField = Literal['id', 'model_name', 'status', 'year', 'iteration', 'parent_run_id', 'config_hash', 'input_hash', 'git_hash', 'signature', 'description', 'created_at', 'started_at', 'ended_at']</code>  <code>module-attribute</code>","text":"","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.RunFieldIndex","level":2,"title":"<code>RunFieldIndex</code>  <code>dataclass</code>","text":"<p>Index <code>Tracker.find_runs(..., index_by=...)</code> results by a <code>Run</code> attribute.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.FacetIndex","level":2,"title":"<code>FacetIndex</code>  <code>dataclass</code>","text":"<p>Index <code>Tracker.find_runs(..., index_by=...)</code> results by a persisted facet value.</p> <p>The facet must be indexed to <code>RunConfigKV</code> (default when <code>facet_index=True</code>) and requires a DB-backed tracker.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.index_by_field","level":2,"title":"<code>index_by_field(field)</code>","text":"<p>Typed helper for <code>index_by=...</code> keyed by a Run field.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.index_by_facet","level":2,"title":"<code>index_by_facet(key)</code>","text":"<p>Typed helper for <code>index_by=...</code> keyed by a facet key.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/materialize/","level":1,"title":"Materialization","text":"","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.materialize_artifacts","level":2,"title":"<code>materialize_artifacts(tracker, items, *, on_missing='warn')</code>","text":"<p>Copy cached artifact bytes onto the filesystem at caller-specified destinations.</p> <p>This is intentionally copy-only materialization: - If the artifact's resolved source path exists, it is copied to the destination. - If it does not exist, behavior depends on <code>on_missing</code>. - If the destination already exists, it is left untouched. - Destination paths cannot be symlinks and will not be overwritten if the type differs.</p> <p>This function does not attempt database-backed reconstruction. Higher-level code can decide when to call <code>consist.load(...)</code> or enable DB recovery.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Tracker</code> <p>Tracker used to resolve portable artifact URIs to host filesystem paths.</p> required <code>items</code> <code>Sequence[tuple[Artifact, Path]]</code> <p>A list of <code>(artifact, destination_path)</code> pairs to materialize.</p> required <code>on_missing</code> <code>('warn', 'raise')</code> <p>What to do when a resolved source path does not exist.</p> <code>\"warn\"</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Map of <code>artifact.key -&gt; destination_path</code> for successfully materialized items.</p>","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.materialize_artifacts_from_sources","level":2,"title":"<code>materialize_artifacts_from_sources(items, *, allowed_base, on_missing='warn')</code>","text":"<p>Copy artifact bytes from explicit source paths to caller-specified destinations.</p> <p>This is useful for rehydrating cached inputs from historical run directories when the current run directory is different from the original producer. Existing destination paths are left untouched. Destination paths cannot be symlinks and will not be overwritten if the type differs.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_base</code> <code>Path | None</code> <p>Base directory that destination paths must remain within. When None, no base containment check is performed.</p> required","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.build_materialize_items_for_keys","level":2,"title":"<code>build_materialize_items_for_keys(outputs, *, destinations_by_key)</code>","text":"<p>Convenience helper to construct <code>(artifact, destination)</code> pairs by artifact key.</p> <p>Any keys not present in <code>outputs</code> are ignored (caller decides whether to treat this as an error).</p>","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.materialize_ingested_artifact_from_db","level":2,"title":"<code>materialize_ingested_artifact_from_db(*, artifact, tracker, destination)</code>","text":"<p>Reconstruct a CSV/Parquet artifact from DuckDB and write it to disk.</p> <p>This is intended for <code>cache_hydration=\"inputs-missing\"</code> when the original on-disk source is missing but the artifact is ingested (<code>is_ingested=True</code>).</p> <p>Supported drivers: csv, parquet. All other drivers raise ValueError.</p>","path":["API Reference","Materialization"],"tags":[]},{"location":"api/matrix/","level":1,"title":"Matrix Views","text":"<p>A factory class responsible for creating virtual xarray Datasets from Consist's metadata.</p> <p>This factory allows users to query the Consist metadata catalog for multi-dimensional data (matrices) and lazily load the corresponding Zarr stores from disk. It consolidates data across different runs into a single, unified xarray Dataset, indexed by <code>run_id</code>, <code>year</code>, and <code>iteration</code>, facilitating comparative analysis and exploration.</p> <p>Attributes:</p> Name Type Description <code>tracker</code> <code>Tracker</code> <p>An instance of the Consist <code>Tracker</code>, which provides access to the database engine and artifact resolution necessary for identifying and loading matrix data.</p>","path":["API Reference","Matrix Views"],"tags":[]},{"location":"api/matrix/#consist.core.matrix.MatrixViewFactory.load_matrix_view","level":2,"title":"<code>load_matrix_view(concept_key, variables=None, *, run_ids=None, parent_id=None, model=None, status=None)</code>","text":"<p>Returns a lazy xarray Dataset containing all runs that match the <code>concept_key</code>.</p> <p>This method queries the Consist database for all matrix-type artifacts associated with the given <code>concept_key</code>. It then lazily opens each corresponding Zarr store using <code>xarray</code> and concatenates them into a single <code>xarray.Dataset</code> along a new <code>run_id</code> dimension. This allows for convenient analysis of multi-dimensional data across different experimental runs.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key (e.g., \"model_output_grid\", \"simulation_results\") identifying the collection of matrix artifacts to load.</p> required <code>variables</code> <code>Optional[List[str]]</code> <p>A list of variable names to load from each Zarr store. If <code>None</code>, all variables from each store will be loaded.</p> <code>None</code> <code>run_ids</code> <code>Optional[List[str]]</code> <p>Optional list of run IDs to include in the view.</p> <code>None</code> <code>parent_id</code> <code>Optional[str]</code> <p>Optional scenario/parent run ID to filter by.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional model name to filter by.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Optional run status to filter by (e.g., \"completed\").</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>A lazy-loaded <code>xarray.Dataset</code> containing the combined data from all matching matrix artifacts, with a new <code>run_id</code> dimension and <code>year</code>/<code>iteration</code> coordinates. Returns an empty <code>xr.Dataset</code> if no matching artifacts are found or can be loaded.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>xarray</code> library is not installed.</p> <code>RuntimeError</code> <p>If the <code>Tracker</code> instance does not have a configured database connection.</p>","path":["API Reference","Matrix Views"],"tags":[]},{"location":"api/public_api/","level":1,"title":"Public API (v0.1)","text":"<p>This page defines Consist's public API for the <code>0.1.x</code> series. Items listed as Advanced are still public but are primarily targeted at advanced users and may be more verbose, lower-level, or easier to misuse.</p>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#stable-intended-for-external-users","level":2,"title":"Stable (intended for external users)","text":"<ul> <li><code>consist.Tracker</code></li> <li><code>consist.Run</code></li> <li><code>consist.Artifact</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#scenario-workflow-helpers","level":3,"title":"Scenario / workflow helpers","text":"<ul> <li><code>consist.scenario</code></li> <li><code>consist.single_step_scenario</code></li> <li><code>consist.run</code></li> <li><code>consist.trace</code></li> <li><code>consist.start_run</code></li> <li><code>consist.define_step</code></li> <li><code>consist.use_tracker</code></li> <li><code>ScenarioContext</code> (returned by <code>consist.scenario(...)</code>)</li> <li><code>run_id</code>, <code>config</code>, <code>inputs</code>, <code>add_input</code>, <code>declare_outputs</code>, <code>require_outputs</code>, <code>collect_by_keys</code>, <code>run</code>, <code>trace</code></li> <li><code>RunContext</code> (injected via <code>inject_context=True</code>)</li> <li><code>run_dir</code>, <code>inputs</code>, <code>load</code>, <code>log_artifact</code>, <code>log_artifacts</code>, <code>log_input</code>, <code>log_output</code>, <code>log_meta</code>, <code>capture_outputs</code></li> <li><code>Coupler</code> (available at <code>scenario.coupler</code>)</li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#artifact-logging-and-loading","level":3,"title":"Artifact logging and loading","text":"<ul> <li><code>consist.log_artifact</code></li> <li><code>consist.log_dataframe</code></li> <li><code>consist.load</code></li> <li><code>consist.capture_outputs</code></li> <li><code>consist.get_artifact</code></li> <li><code>consist.cached_output</code></li> <li><code>consist.cached_artifacts</code></li> <li><code>consist.log_meta</code></li> <li><code>consist.current_run</code></li> <li><code>consist.current_consist</code></li> <li><code>Tracker.define_step(...)</code> (explicit tracker form)</li> <li><code>Tracker.materialize(...)</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#querying-and-views","level":3,"title":"Querying and views","text":"<ul> <li><code>consist.view</code></li> <li><code>consist.register_views</code></li> <li><code>consist.find_run</code></li> <li><code>consist.find_runs</code></li> <li><code>Tracker.find_latest_run(...)</code></li> <li><code>Tracker.diff_runs(...)</code></li> <li><code>Tracker.get_run_inputs(...)</code> / <code>Tracker.get_run_outputs(...)</code></li> <li><code>Tracker.get_run_config(...)</code></li> <li><code>Tracker.print_lineage(...)</code></li> <li><code>consist.run_query</code></li> <li><code>consist.db_session</code></li> <li><code>consist.pivot_facets</code></li> <li>Indexing helpers: <code>consist.index_by_field</code>, <code>consist.index_by_facet</code>, plus <code>RunFieldIndex</code> / <code>FacetIndex</code></li> <li>Views registry: <code>tracker.views</code> (<code>ViewRegistry</code>)</li> <li>Matrix utilities: <code>Tracker.load_matrix(...)</code>, <code>MatrixViewFactory</code></li> <li>Schema export: <code>Tracker.export_schema_sqlmodel(...)</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#tracker-methods-complete-public-surface","level":3,"title":"Tracker methods (complete public surface)","text":"<p>This section enumerates all non-underscore <code>Tracker</code> methods. If you're new to Consist, start with the Core and Logging/Loading groups and reach for Advanced only as needed.</p>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#core-lifecycle","level":4,"title":"Core lifecycle","text":"<ul> <li><code>begin_run</code>, <code>start_run</code>, <code>run</code>, <code>trace</code>, <code>scenario</code>, <code>end_run</code></li> <li><code>define_step</code>, <code>last_run</code>, <code>is_cached</code>, <code>cached_artifacts</code>, <code>cached_output</code></li> <li><code>suspend_cache_options</code>, <code>restore_cache_options</code>, <code>capture_outputs</code>, <code>log_meta</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#logging-and-loading","level":4,"title":"Logging and loading","text":"<ul> <li><code>log_artifact</code>, <code>log_artifacts</code>, <code>log_input</code>, <code>log_output</code>, <code>log_dataframe</code></li> <li><code>load</code>, <code>materialize</code>, <code>ingest</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#querying-and-history","level":4,"title":"Querying and history","text":"<ul> <li><code>find_runs</code>, <code>find_run</code>, <code>find_latest_run</code>, <code>get_latest_run_id</code></li> <li><code>find_artifacts</code>, <code>get_artifact</code>, <code>get_artifacts_for_run</code></li> <li><code>get_run</code>, <code>get_run_config</code>, <code>get_run_inputs</code>, <code>get_run_outputs</code></li> <li><code>get_artifact_lineage</code>, <code>print_lineage</code>, <code>history</code></li> <li><code>diff_runs</code>, <code>get_config_facet</code>, <code>get_config_facets</code>, <code>get_run_config_kv</code></li> <li><code>get_config_values</code>, <code>get_config_value</code>, <code>find_runs_by_facet_kv</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#views-and-matrices","level":4,"title":"Views and matrices","text":"<ul> <li><code>view</code>, <code>create_view</code>, <code>load_matrix</code>, <code>export_schema_sqlmodel</code></li> <li><code>netcdf_metadata</code>, <code>openmatrix_metadata</code>, <code>spatial_metadata</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#config-canonicalization","level":4,"title":"Config canonicalization","text":"<ul> <li><code>canonicalize_config</code>, <code>prepare_config</code>, <code>apply_config_plan</code>, <code>identity_from_config_plan</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#format-specific-logging","level":4,"title":"Format-specific logging","text":"<ul> <li><code>log_h5_container</code>, <code>log_h5_table</code>, <code>log_netcdf_file</code>, <code>log_openmatrix_file</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#advanced-power-user-lower-level","level":3,"title":"Advanced (power-user / lower-level)","text":"<p>These methods are still public, but are more low-level or easier to misuse.</p> <ul> <li><code>engine</code>, <code>set_run_subdir_fn</code>, <code>run_artifact_dir</code>, <code>resolve_uri</code></li> <li><code>run_query</code>, <code>get_run_record</code>, <code>resolve_historical_path</code>, <code>load_input_bundle</code></li> <li><code>get_artifact_by_uri</code>, <code>get_run_artifact</code>, <code>load_run_output</code>, <code>find_matching_run</code></li> <li><code>on_run_start</code>, <code>on_run_complete</code>, <code>on_run_failed</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#stable-but-optional-extras","level":2,"title":"Stable, but optional extras","text":"<p>These APIs are part of the public surface, but require extra dependencies.</p> <ul> <li>Ingestion helpers: <code>consist.ingest</code> (install with <code>consist[ingest]</code>)</li> <li><code>consist.integrations.containers</code> (container execution + caching; requires Docker or Singularity)</li> <li><code>consist.integrations.dlt_loader</code> (low-level ingestion integration; requires <code>consist[ingest]</code>)</li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/run/","level":1,"title":"Run","text":"<p>               Bases: <code>SQLModel</code></p> <p>Primary run table in the Consist database.</p> <p>Each run captures execution metadata (status, timing, identity hashes, tags) and links to artifacts through <code>run_artifact_link</code>. Full configuration details live in the JSON run snapshot on disk, while the database stores hashes and a queryable subset of metadata for caching and discovery.</p> <p>Attributes:     id (str): A unique identifier for the run, often combining model name, year, and a UUID.     parent_run_id (Optional[str]): The ID of the parent run, if this is a nested execution.     status (str): The current state of the run (e.g., \"running\", \"completed\", \"failed\").     model_name (str): The name of the model or workflow being executed.     description (Optional[str]): Human-readable description of the run's purpose or outcome.     year (Optional[int]): The simulation or data year, if applicable.     iteration (Optional[int]): The iteration number, if applicable.     tags (List[str]): A list of string labels for categorization and filtering (e.g., [\"production\", \"urbansim\"]).     config_hash (Optional[str]): A hash of the run's configuration, used for caching.     git_hash (Optional[str]): The Git commit hash of the code version used for the run.     meta (Dict[str, Any]): A flexible dictionary for storing arbitrary metadata (e.g., hostname).     started_at (datetime): The timestamp when the run execution began.     ended_at (Optional[datetime]): The timestamp when the run execution completed or failed.     created_at (datetime): The timestamp when the run record was created.     updated_at (datetime): The timestamp when the run record was last updated.</p>","path":["API Reference","Run"],"tags":[]},{"location":"api/run/#consist.models.run.Run.duration_seconds","level":2,"title":"<code>duration_seconds</code>  <code>property</code>","text":"<p>Calculate the duration of the run in seconds.</p> <p>Returns the elapsed time between <code>started_at</code> and <code>ended_at</code> if both are set, otherwise returns None (e.g., if the run is still in progress).</p> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>The duration in seconds, or None if the run hasn't ended.</p>","path":["API Reference","Run"],"tags":[]},{"location":"api/tracker/","level":1,"title":"Tracker","text":"<p>The central orchestrator for Consist, managing the lifecycle of a Run and its associated Artifacts.</p> <p>The Tracker is responsible for:</p> <ol> <li> <p>Initiating and managing the state of individual \"Runs\" (e.g., model executions, data processing steps).</p> </li> <li> <p>Logging \"Artifacts\" (input files, output data, etc.) and their relationships to runs.</p> </li> <li> <p>Implementing a dual-write mechanism, logging provenance to both human-readable JSON files (<code>consist.json</code>)     and an analytical DuckDB database (<code>provenance.duckdb</code>).</p> </li> <li> <p>Providing path virtualization to make runs portable across different environments,     as described in the \"Path Resolution &amp; Mounts\" architectural section.</p> </li> <li> <p>Facilitating smart caching based on a Merkle DAG strategy, enabling \"run forking\" and \"hydration\"     of previously computed results.</p> </li> </ol>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.last_run","level":2,"title":"<code>last_run</code>  <code>property</code>","text":"<p>Return the most recent run record observed by this tracker.</p> <p>Returns:</p> Type Description <code>Optional[ConsistRecord]</code> <p>The last completed/failed run record for this tracker instance, or <code>None</code> if no run has executed yet.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.is_cached","level":2,"title":"<code>is_cached</code>  <code>property</code>","text":"<p>Whether the currently active run is a cache hit.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current <code>start_run</code>/<code>run</code>/<code>trace</code> execution is reusing a cached run. Returns <code>False</code> if no run is active.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.engine","level":2,"title":"<code>engine</code>  <code>property</code>","text":"<p>Return the SQLAlchemy engine used by this tracker.</p> <p>Use this for advanced, low-level database access when the higher-level tracker/query helpers are insufficient.</p> <p>Returns:</p> Type Description <code>Optional[Engine]</code> <p>The SQLAlchemy engine if a database is configured, otherwise <code>None</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.begin_run","level":2,"title":"<code>begin_run(run_id, model, config=None, inputs=None, tags=None, description=None, cache_mode='reuse', *, artifact_dir=None, allow_external_paths=None, facet=None, facet_from=None, hash_inputs=None, facet_schema_version=None, facet_index=True, **kwargs)</code>","text":"<p>Start a run imperatively (without context manager).</p> <p>Use this when run start and end are in separate methods, or when integrating with frameworks that have their own lifecycle management. Returns the Run object. Call end_run() when complete.</p> <p>This provides an alternative to the context manager pattern when you need more control over the run lifecycle, such as in external model integrations where start_model_run() and complete_model_run() are separate method calls.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>A unique identifier for the current run.</p> required <code>model</code> <code>str</code> <p>A descriptive name for the model or process being executed (non-empty, length-limited).</p> required <code>config</code> <code>Union[Dict[str, Any], BaseModel, None]</code> <p>Configuration parameters for this run. Keys must be strings; extremely large string values are rejected.</p> <code>None</code> <code>inputs</code> <code>Optional[list[ArtifactRef]]</code> <p>A list of input paths (str/Path) or Artifact references.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>A list of string labels for categorization and filtering (non-empty, length-limited).</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>A human-readable description of the run's purpose.</p> <code>None</code> <code>cache_mode</code> <code>str</code> <p>Strategy for caching: \"reuse\", \"overwrite\", or \"readonly\".</p> <code>\"reuse\"</code> <code>artifact_dir</code> <code>Optional[Union[str, Path]]</code> <p>Override the per-run artifact directory. Relative paths are resolved under <code>&lt;run_dir&gt;/outputs</code>. Absolute paths must remain within <code>run_dir</code> unless allow_external_paths is enabled.</p> <code>None</code> <code>allow_external_paths</code> <code>Optional[bool]</code> <p>Allow artifact_dir and cached-output materialization outside <code>run_dir</code>. Defaults to the Tracker setting when unset.</p> <code>None</code> <code>facet</code> <code>Optional[FacetLike]</code> <p>Optional small, queryable configuration facet to persist alongside the run. This is distinct from <code>config</code> (which is hashed and stored in the JSON snapshot).</p> <code>None</code> <code>facet_from</code> <code>Optional[List[str]]</code> <p>List of config keys to extract into the facet. Extracted values are merged with any explicit <code>facet</code>, with explicit keys taking precedence.</p> <code>None</code> <code>hash_inputs</code> <code>HashInputs</code> <p>Extra inputs to include in the run identity hash without logging them as run inputs/outputs. Useful for config bundles or auxiliary files. Each entry is either a path (str/Path) or a named tuple <code>(name, path)</code>.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Optional schema version tag for the persisted facet.</p> <code>None</code> <code>facet_index</code> <code>bool</code> <p>Whether to flatten and index facet keys/values for DB querying.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata. Special keywords <code>year</code> and <code>iteration</code> can be used. Metadata keys/values are validated and size-limited; use CONSIST_MAX_METADATA_ITEMS/KEY_LENGTH/VALUE_LENGTH to override.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Run</code> <p>The Run object representing the started run.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is already an active run.</p> Example <pre><code>run = tracker.begin_run(\"run_001\", \"urbansim\", config={...})\ntry:\n    tracker.log_artifact(input_file, direction=\"input\")\n    # ... do work ...\n    tracker.log_artifact(output_file, direction=\"output\")\n    tracker.end_run(\"completed\")\nexcept Exception as e:\n    tracker.end_run(\"failed\", error=e)\n    raise\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.start_run","level":2,"title":"<code>start_run(run_id, model, **kwargs)</code>","text":"<p>Context manager to initiate and manage the lifecycle of a Consist run.</p> <p>This is the primary entry point for defining a reproducible and observable unit of work. It wraps the imperative <code>begin_run()</code>/<code>end_run()</code> methods to provide automatic cleanup and exception handling.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>A unique identifier for the current run.</p> required <code>model</code> <code>str</code> <p>A descriptive name for the model or process being executed.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments forwarded to <code>begin_run()</code>, including commonly used keys:</p> <ul> <li><code>config</code>: Union[Dict[str, Any], BaseModel, None]</li> <li><code>inputs</code>: Optional[list[ArtifactRef]]</li> <li><code>tags</code>: Optional[List[str]]</li> <li><code>description</code>: Optional[str]</li> <li><code>cache_mode</code>: str (\"reuse\", \"overwrite\", \"readonly\")</li> <li><code>facet</code>, <code>facet_from</code>, <code>hash_inputs</code>, <code>facet_schema_version</code>, <code>facet_index</code></li> <li><code>year</code>, <code>iteration</code></li> </ul> <code>{}</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The current <code>Tracker</code> instance for use within the <code>with</code> block.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any exception raised within the <code>with</code> block will be caught, the run marked as \"failed\", and then re-raised after cleanup.</p> See Also <p>begin_run : Imperative alternative for starting runs. end_run : Imperative alternative for ending runs.</p> Example <p>with tracker.start_run(\"run_1\", \"my_model\", config={\"p\": 1}): ...     tracker.log_artifact(\"data.csv\", \"input\") ...     # ... execution ... ...     tracker.log_artifact(\"results.parquet\", \"output\")</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.run","level":2,"title":"<code>run(fn=None, name=None, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', load_inputs=None, executor='python', container=None, runtime_kwargs=None, inject_context=False, output_mismatch='warn', output_missing='warn')</code>","text":"<p>Execute a function-shaped run with caching and output handling.</p> <p>This method executes a callable (or container) with automatic provenance tracking, intelligent caching based on code+config+inputs, and artifact logging.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Optional[Callable]</code> <p>The function to execute. Required for executor='python'. Can be None for executor='container'.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Human-readable name for the run. Defaults to function name if not provided.</p> <code>None</code> <code>run_id</code> <code>Optional[str]</code> <p>Unique identifier for this run. Auto-generated if not provided.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model/component name for categorizing runs. Defaults to the run name.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Human-readable description of the run.</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration parameters. Becomes part of the cache signature. Can be a dict or Pydantic model.</p> <code>None</code> <code>config_plan</code> <code>Optional[ConfigPlan]</code> <p>Precomputed config plan (e.g., from ActivitySim adapter). The plan's identity hash is folded into the run config hash and its artifacts/ingestables are applied on cache miss.</p> <code>None</code> <code>config_plan_ingest</code> <code>bool</code> <p>Whether to ingest tables from the config plan.</p> <code>True</code> <code>config_plan_profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas for the config plan.</p> <code>False</code> <code>inputs</code> <code>Optional[Mapping[str, ArtifactRef] | Iterable[ArtifactRef]]</code> <p>Input files or artifacts. - Dict: Maps names to paths/Artifacts. Auto-loads into function parameters (default load_inputs=True). - List/Iterable: Hashed for cache key but not auto-loaded (use load_inputs=False).</p> <code>None</code> <code>input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>optional_input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>depends_on</code> <code>Optional[List[ArtifactRef]]</code> <p>Additional file paths or artifacts to hash for the cache signature (e.g., config files).</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Labels for filtering and organizing runs (e.g., [\"production\", \"baseline\"]).</p> <code>None</code> <code>facet</code> <code>Optional[FacetLike]</code> <p>Queryable metadata facets (small config values) logged to the run.</p> <code>None</code> <code>facet_from</code> <code>Optional[List[str]]</code> <p>List of config keys to extract and log as facets.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Schema version for facet compatibility tracking.</p> <code>None</code> <code>facet_index</code> <code>Optional[bool]</code> <p>Whether to index facets for faster queries.</p> <code>None</code> <code>hash_inputs</code> <code>Optional[HashInputs]</code> <p>Strategy for hashing inputs: \"fast\" (mtime), \"full\" (content), or None (auto-detect).</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Year metadata (for multi-year simulations). Included in provenance.</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Iteration count (for iterative workflows). Included in provenance.</p> <code>None</code> <code>parent_run_id</code> <code>Optional[str]</code> <p>Parent run ID (for nested runs in scenarios).</p> <code>None</code> <code>outputs</code> <code>Optional[List[str]]</code> <p>Names of output artifacts to log (for executor='python' with auto-loaded DataFrames). Maps artifact key to ingested table name.</p> <code>None</code> <code>output_paths</code> <code>Optional[Mapping[str, ArtifactRef]]</code> <p>Output file paths to log. Dict maps artifact keys to host paths or Artifact refs.</p> <code>None</code> <code>capture_dir</code> <code>Optional[Path]</code> <p>Directory to scan for outputs (legacy tools that write to specific dirs).</p> <code>None</code> <code>capture_pattern</code> <code>str</code> <p>Glob pattern for capturing outputs (used with capture_dir).</p> <code>\"*\"</code> <code>cache_mode</code> <code>str</code> <p>Cache behavior: \"reuse\" (return cache hit), \"overwrite\" (always re-execute), or \"skip_check\".</p> <code>\"reuse\"</code> <code>cache_hydration</code> <code>Optional[str]</code> <p>Materialization strategy for cache hits: - \"outputs-requested\": Copy only output_paths to disk - \"outputs-all\": Copy all cached outputs to run_artifact_dir - \"inputs-missing\": Backfill missing inputs from prior runs before executing</p> <code>None</code> <code>validate_cached_outputs</code> <code>str</code> <p>Validation for cached outputs: \"lazy\" (check if files exist), \"strict\", or \"none\".</p> <code>\"lazy\"</code> <code>load_inputs</code> <code>Optional[bool]</code> <p>Whether to auto-load input artifacts into function parameters. Defaults to True if inputs is a dict, False if a list.</p> <code>None</code> <code>executor</code> <code>str</code> <p>Execution backend: \"python\" (call fn directly) or \"container\" (use Docker/Singularity).</p> <code>\"python\"</code> <code>container</code> <code>Optional[Mapping[str, Any]]</code> <p>Container spec (required if executor='container'). Must contain 'image' and 'command'.</p> <code>None</code> <code>runtime_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional kwargs to pass to fn at runtime (merged with auto-loaded inputs). These values are not part of the cache signature; use them for handles or runtime-only dependencies. Consider <code>consist.require_runtime_kwargs</code> to enforce required keys.</p> <code>None</code> <code>inject_context</code> <code>bool | str</code> <p>If True or a parameter name, inject a RunContext as that parameter (for file I/O, output logging).</p> <code>False</code> <code>output_mismatch</code> <code>str</code> <p>Behavior when output count doesn't match: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <code>output_missing</code> <code>str</code> <p>Behavior when expected outputs are missing: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <p>Returns:</p> Type Description <code>RunResult</code> <p>Contains: - <code>outputs</code>: Dict[str, Artifact] of logged output artifacts - <code>cache_hit</code>: bool indicating if this was a cache hit - <code>run_id</code>: The run's unique identifier</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fn is None (for executor='python'), or if container/output_paths not provided for executor='container'.</p> <code>RuntimeError</code> <p>If the function execution fails or container execution returns non-zero code.</p> <p>Examples:</p> <p>Simple data processing:</p> <pre><code>&gt;&gt;&gt; def clean_data(raw: pd.DataFrame) -&gt; pd.DataFrame:\n...     return raw[raw['value'] &gt; 0.5]\n&gt;&gt;&gt;\n&gt;&gt;&gt; result = tracker.run(\n...     fn=clean_data,\n...     inputs={\"raw\": Path(\"raw.csv\")},\n...     outputs=[\"cleaned\"],\n... )\n</code></pre> <p>With config for cache distinction:</p> <pre><code>&gt;&gt;&gt; result = tracker.run(\n...     fn=clean_data,\n...     inputs={\"raw\": Path(\"raw.csv\")},\n...     config={\"threshold\": 0.5},\n...     outputs=[\"cleaned\"],\n... )\n</code></pre> See Also <p>start_run : Manual run context management (more control) trace : Context manager alternative (always executes, even on cache hit)</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.trace","level":2,"title":"<code>trace(name, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', output_mismatch='warn', output_missing='warn')</code>","text":"<p>Context manager for inline tracing of a run with inline execution.</p> <p>This context manager allows you to define a run directly within a <code>with</code> block, with the Python code inside executing every time (even on cache hits). This differs from <code>tracker.run()</code>, which skips execution on cache hits.</p> <p>Use <code>trace()</code> when you need inline control: for data loading, file I/O, or integrations that require code execution regardless of cache state.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable name for the run. Also defaults the model name if not provided.</p> required <code>run_id</code> <code>Optional[str]</code> <p>Unique identifier for this run. Auto-generated if not provided.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model/component name for categorizing runs. Defaults to the run name.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Human-readable description of the run.</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration parameters. Becomes part of the cache signature. Can be a dict or Pydantic model.</p> <code>None</code> <code>config_plan</code> <code>Optional[ConfigPlan]</code> <p>Precomputed config plan (e.g., from ActivitySim adapter). The plan's identity hash is folded into the run config hash and its artifacts/ingestables are applied on cache miss.</p> <code>None</code> <code>config_plan_ingest</code> <code>bool</code> <p>Whether to ingest tables from the config plan.</p> <code>True</code> <code>config_plan_profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas for the config plan.</p> <code>False</code> <code>inputs</code> <code>Optional[Mapping[str, ArtifactRef] | Iterable[ArtifactRef]]</code> <p>Input files or artifacts. - Dict: Maps names to paths/Artifacts. Logged as inputs but not auto-loaded. - List/Iterable: Hashed for cache key but not auto-loaded.</p> <code>None</code> <code>input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>optional_input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>depends_on</code> <code>Optional[List[ArtifactRef]]</code> <p>Additional file paths or artifacts to hash for the cache signature (e.g., config files).</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Labels for filtering and organizing runs (e.g., [\"production\", \"baseline\"]).</p> <code>None</code> <code>facet</code> <code>Optional[FacetLike]</code> <p>Queryable metadata facets (small config values) logged to the run.</p> <code>None</code> <code>facet_from</code> <code>Optional[List[str]]</code> <p>List of config keys to extract and log as facets.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Schema version for facet compatibility tracking.</p> <code>None</code> <code>facet_index</code> <code>Optional[bool]</code> <p>Whether to index facets for faster queries.</p> <code>None</code> <code>hash_inputs</code> <code>Optional[HashInputs]</code> <p>Strategy for hashing inputs: \"fast\" (mtime), \"full\" (content), or None (auto-detect).</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Year metadata (for multi-year simulations). Included in provenance.</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Iteration count (for iterative workflows). Included in provenance.</p> <code>None</code> <code>parent_run_id</code> <code>Optional[str]</code> <p>Parent run ID (for nested runs in scenarios).</p> <code>None</code> <code>outputs</code> <code>Optional[List[str]]</code> <p>Names of output artifacts to log. Each item is a key name for logged outputs.</p> <code>None</code> <code>output_paths</code> <code>Optional[Mapping[str, ArtifactRef]]</code> <p>Output file paths to log. Dict maps artifact keys to host paths or Artifact refs.</p> <code>None</code> <code>capture_dir</code> <code>Optional[Path]</code> <p>Directory to scan for outputs. New/modified files are auto-logged.</p> <code>None</code> <code>capture_pattern</code> <code>str</code> <p>Glob pattern for capturing outputs (used with capture_dir).</p> <code>\"*\"</code> <code>cache_mode</code> <code>str</code> <p>Cache behavior: \"reuse\" (return cache hit), \"overwrite\" (always re-execute), or \"skip_check\".</p> <code>\"reuse\"</code> <code>cache_hydration</code> <code>Optional[str]</code> <p>Materialization strategy for cache hits: - \"outputs-requested\": Copy only output_paths to disk - \"outputs-all\": Copy all cached outputs to run_artifact_dir - \"inputs-missing\": Backfill missing inputs from prior runs before executing</p> <code>None</code> <code>validate_cached_outputs</code> <code>str</code> <p>Validation for cached outputs: \"lazy\" (check if files exist), \"strict\", or \"none\".</p> <code>\"lazy\"</code> <code>output_mismatch</code> <code>str</code> <p>Behavior when output count doesn't match: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <code>output_missing</code> <code>str</code> <p>Behavior when expected outputs are missing: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The current <code>Tracker</code> instance for use within the <code>with</code> block.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If output_mismatch or output_missing are invalid values.</p> <code>RuntimeError</code> <p>If output validation fails based on validation settings.</p> Notes <p>Unlike <code>tracker.run()</code>, the Python code inside a <code>trace()</code> block ALWAYS executes, even on cache hits. This is useful for side effects, data loading, or code that should run regardless of cache state.</p> <p>If you want to skip execution on cache hits (like <code>tracker.run()</code>), consider using <code>tracker.run()</code> with a callable instead.</p> <p>Examples:</p> <p>Simple inline tracing with file capture:</p> <pre><code>&gt;&gt;&gt; with tracker.trace(\n...     \"my_analysis\",\n...     output_paths={\"results\": \"./results.csv\"}\n... ):\n...     df = pd.read_csv(\"raw.csv\")\n...     df[\"value\"] = df[\"value\"] * 2\n...     df.to_csv(\"./results.csv\", index=False)\n</code></pre> <p>Multi-year simulation:</p> <pre><code>&gt;&gt;&gt; with tracker.scenario(\"baseline\") as sc:\n...     for year in [2020, 2030, 2040]:\n...         with sc.trace(name=\"simulate\", year=year):\n...             results = run_model(year)\n...             tracker.log_artifact(results, key=\"output\")\n</code></pre> See Also <p>run : Function-shaped alternative (skips on cache hit) scenario : Multi-step workflow grouping start_run : Imperative alternative for run lifecycle management</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.scenario","level":2,"title":"<code>scenario(name, config=None, tags=None, model='scenario', step_cache_hydration=None, coupler=None, require_outputs=None, **kwargs)</code>","text":"<p>Create a ScenarioContext to manage a grouped workflow of steps.</p> <p>This method initializes a scenario context manager that acts as a \"header\" run. It allows defining multiple steps (runs) that are automatically linked to this header run via <code>parent_run_id</code>, without manual threading.</p> <p>The scenario run is started, then immediately suspended (allowing steps to run), and finally restored and completed when the context exits.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the scenario. This will become the Run ID.</p> required <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Scenario-level configuration. Stored on the header run but NOT automatically inherited by steps.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Tags for the scenario. \"scenario_header\" is automatically appended.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model name for the header run.</p> <code>\"scenario\"</code> <code>step_cache_hydration</code> <code>Optional[str]</code> <p>Default cache hydration policy for all scenario steps unless overridden in a specific <code>scenario.trace(...)</code> or <code>scenario.run(...)</code>.</p> <code>None</code> <code>coupler</code> <code>Optional[Coupler]</code> <p>Optional Coupler instance to use for the scenario.</p> <code>None</code> <code>require_outputs</code> <code>Optional[Iterable[str]]</code> <p>Declare required outputs at scenario creation time.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata or arguments for the header run (including <code>facet_from</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>ScenarioContext</code> <p>A context manager object that provides <code>.trace()</code> and <code>.add_input()</code> methods.</p> Example <pre><code>with tracker.scenario(\"baseline\", config={\"mode\": \"test\"}) as sc:\n    sc.add_input(\"data.csv\", key=\"data\")\n    with sc.step(\"init\"):\n        ...\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.end_run","level":2,"title":"<code>end_run(status='completed', error=None)</code>","text":"<p>End the current run started with begin_run().</p> <p>This method finalizes the run, persists the final state to JSON and database, and emits lifecycle hooks. It is idempotent - calling it multiple times on an already-ended run will log a warning but not raise an error.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>str</code> <p>The final status of the run. Typically \"completed\" or \"failed\".</p> <code>\"completed\"</code> <code>error</code> <code>Optional[Exception]</code> <p>The exception that caused the failure, if status is \"failed\". The error message will be stored in the run's metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>Run</code> <p>The completed Run object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is no active run to end.</p> Example <pre><code>run = tracker.begin_run(\"run_001\", \"urbansim\")\ntry:\n    # ... do work ...\n    tracker.end_run(\"completed\")\nexcept Exception as e:\n    tracker.end_run(\"failed\", error=e)\n    raise\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.define_step","level":2,"title":"<code>define_step(**kwargs)</code>","text":"<p>Attach metadata to a function without changing execution behavior.</p> <p>This decorator lets you attach defaults such as <code>outputs</code>, <code>tags</code>, or <code>cache_mode</code> to a function. <code>Tracker.run</code> and <code>ScenarioContext.run</code> read this metadata when executing the function.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Step metadata (e.g., <code>outputs</code>, <code>tags</code>, <code>cache_mode</code>, <code>inject_context</code>) to attach to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A decorator that returns the original function with attached metadata.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.cached_artifacts","level":2,"title":"<code>cached_artifacts(direction='output')</code>","text":"<p>Returns hydrated artifacts for the active run when it is a cache hit.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>str</code> <p>\"output\" or \"input\" to filter hydrated artifacts.</p> <code>\"output\"</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping of artifact key to Artifact for the specified direction. Returns an empty dict if no cache hit or no artifacts.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.cached_output","level":2,"title":"<code>cached_output(key=None)</code>","text":"<p>Convenience to fetch a hydrated cached output artifact for the current run.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[str]</code> <p>If provided, returns the artifact with this key; otherwise returns the first available cached output.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>The cached output artifact, or None if not cached / not found.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.suspend_cache_options","level":2,"title":"<code>suspend_cache_options()</code>","text":"<p>Suspend active-run cache options and reset them to defaults.</p> <p>This is useful for helper functions that want default cache behavior without mutating the caller's options.</p> <p>Returns:</p> Type Description <code>ActiveRunCacheOptions</code> <p>The previously active cache options, for later restoration.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.restore_cache_options","level":2,"title":"<code>restore_cache_options(options)</code>","text":"<p>Restore previously suspended active-run cache options.</p> <p>This should typically be paired with a prior <code>suspend_cache_options</code> call to restore the caller's cache behavior.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>ActiveRunCacheOptions</code> <p>Cache options to restore (usually returned by <code>suspend_cache_options</code>).</p> required","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.capture_outputs","level":2,"title":"<code>capture_outputs(directory, pattern='*', recursive=False)</code>","text":"<p>A context manager to automatically capture and log new or modified files in a directory.</p> <p>This context manager is used within a <code>tracker.run</code>/<code>tracker.trace</code> call or <code>start_run</code> block to monitor a specified directory. Any files created or modified within this directory during the execution of the <code>with</code> block will be automatically logged as output artifacts of the current run.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>The path to the directory to monitor for new or modified files.</p> required <code>pattern</code> <code>str</code> <p>A glob pattern (e.g., \".csv\", \"data_.parquet\") to filter which files are captured within the specified directory. Defaults to all files.</p> <code>\"*\"</code> <code>recursive</code> <code>bool</code> <p>If True, the capture will recursively scan subdirectories within <code>directory</code>.</p> <code>False</code> <p>Yields:</p> Type Description <code>OutputCapture</code> <p>An <code>OutputCapture</code> object containing a list of <code>Artifact</code> objects that were captured and logged after the <code>with</code> block finishes.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>capture_outputs</code> is used outside of an active <code>start_run</code> context.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_meta","level":2,"title":"<code>log_meta(**kwargs)</code>","text":"<p>Updates the metadata for the current run.</p> <p>This method allows logging additional key-value pairs to the <code>meta</code> field of the currently active <code>Run</code> object. This is particularly useful for recording runtime metrics (e.g., accuracy, loss, F1-score), tags, or any other arbitrary information generated during the run's execution. The metadata is immediately flushed to both the JSON log and the database.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary key-value pairs to merge into the <code>meta</code> dictionary of the current run. Existing keys will be updated, and new keys will be added.</p> <code>{}</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_artifact","level":2,"title":"<code>log_artifact(path, key=None, direction='output', schema=None, driver=None, table_path=None, array_path=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', profile_file_schema=None, file_schema_sample_rows=None, **meta)</code>","text":"<p>Logs an artifact (file or data reference) within the current run context.</p> <p>This method supports:</p> <ul> <li> <p>Automatic Input Discovery: If an input <code>path</code> matches a previously     logged output artifact, Consist automatically links them, building the     provenance graph. This is a key part of \"Auto-Forking\".</p> </li> <li> <p>Path Virtualization: Converts absolute file system paths to portable URIs     (e.g., <code>inputs://data.csv</code>) using configured mounts, adhering to     \"Path Resolution &amp; Mounts\".</p> </li> <li> <p>Schema Metadata Injection: Embeds schema information (if provided) into the     artifact's metadata, useful for later \"Strict Mode\" validation or introspection.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged. Passing an <code>Artifact</code> is useful for explicitly linking an already-logged artifact as an input or output in the current run.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact (e.g., \"households\"). Required if <code>path</code> is a path-like (str/Path).</p> <code>None</code> <code>direction</code> <code>str</code> <p>Specifies whether the artifact is an \"input\" or \"output\" for the current run. Defaults to \"output\".</p> <code>\"output\"</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the artifact's data. Its name will be stored in artifact metadata.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify the driver (e.g., 'h5_table'). If None, the driver is inferred from the file extension.</p> <code>None</code> <code>table_path</code> <code>Optional[str]</code> <p>Optional table path inside a container (e.g., HDF5).</p> <code>None</code> <code>array_path</code> <code>Optional[str]</code> <p>Optional array path inside a container (e.g., Zarr group).</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>reuse_if_unchanged</code> <code>bool</code> <p>If True and logging an output, reuse a prior artifact row when the content hash matches.</p> <code>False</code> <code>reuse_scope</code> <code>(same_uri, any_uri)</code> <p>Scope for output reuse checks. \"same_uri\" restricts reuse to the same URI, while \"any_uri\" allows reuse across different URIs with the same hash.</p> <code>\"same_uri\"</code> <code>profile_file_schema</code> <code>bool</code> <p>If True, profile a lightweight schema for file-based tabular artifacts. Use \"if_changed\" to skip profiling when a matching content hash already has a schema.</p> <code>False</code> <code>file_schema_sample_rows</code> <code>Optional[int]</code> <p>Maximum rows to sample when profiling file-based schemas.</p> <code>None</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's flexible <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If <code>key</code> is not provided when <code>path</code> is a path-like (str/Path).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_artifacts","level":2,"title":"<code>log_artifacts(outputs, direction='output', driver=None, metadata_by_key=None, reuse_if_unchanged=False, reuse_scope='same_uri', **shared_meta)</code>","text":"<p>Log multiple artifacts in a single call for efficiency.</p> <p>This is a convenience method for bulk artifact logging, particularly useful when a model produces many output files or when registering multiple inputs. This requires an explicit mapping so artifact keys are always deliberate.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>mapping</code> <p>Mapping of key -&gt; path/Artifact to log.</p> required <code>direction</code> <code>str</code> <p>Specifies whether the artifacts are \"input\" or \"output\" for the current run.</p> <code>\"output\"</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify the driver for all artifacts. If None, driver is inferred from each file's extension individually.</p> <code>None</code> <code>metadata_by_key</code> <code>Optional[Mapping[str, Dict[str, Any]]]</code> <p>Per-key metadata overrides applied on top of shared metadata.</p> <code>None</code> <code>**shared_meta</code> <code>Any</code> <p>Metadata key-value pairs to apply to ALL logged artifacts. Useful for tagging a batch of related files.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping of key -&gt; logged Artifact.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If metadata_by_key contains keys not present in outputs.</p> <code>TypeError</code> <p>If mapping keys are not strings.</p> Example <pre><code># Log explicit outputs\noutputs = tracker.log_artifacts(\n    {\"persons\": \"output/persons.parquet\", \"households\": \"output/households.parquet\"},\n    metadata_by_key={\"households\": {\"role\": \"primary\"}},\n    year=2030,\n)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_input","level":2,"title":"<code>log_input(path, key=None, content_hash=None, force_hash_override=False, validate_content_hash=False, **meta)</code>","text":"<p>Log an input artifact. Convenience wrapper for log_artifact(direction='input').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_output","level":2,"title":"<code>log_output(path, key=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', **meta)</code>","text":"<p>Log an output artifact. Convenience wrapper for log_artifact(direction='output').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_dataframe","level":2,"title":"<code>log_dataframe(df, key, schema=None, direction='output', path=None, driver=None, meta=None, profile_file_schema=False, file_schema_sample_rows=1000, **to_file_kwargs)</code>","text":"<p>Serialize a DataFrame, log it as an artifact, and trigger optional ingestion.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data to persist.</p> required <code>key</code> <code>str</code> <p>Logical artifact key.</p> required <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Schema used for ingestion, if provided.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Artifact direction relative to the run.</p> <code>\"output\"</code> <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Output path; defaults to <code>&lt;run_dir&gt;/outputs/&lt;run_subdir&gt;/&lt;key&gt;.&lt;driver&gt;</code> where <code>run_subdir</code> is derived from <code>run_subdir_fn</code> (or the default pattern).</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>File format driver (e.g., \"parquet\" or \"csv\").</p> <code>None</code> <code>meta</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the artifact.</p> <code>None</code> <code>profile_file_schema</code> <code>bool</code> <p>If True, profile a lightweight schema for file-based tabular artifacts.</p> <code>False</code> <code>file_schema_sample_rows</code> <code>Optional[int]</code> <p>Maximum rows to sample when profiling file-based schemas.</p> <code>1000</code> <code>**to_file_kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>pd.DataFrame.to_parquet</code> or <code>to_csv</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The artifact logged for the written dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested driver is unsupported.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load","level":2,"title":"<code>load(artifact, **kwargs)</code>","text":"<p>Load an artifact using the public API while binding this tracker context.</p> <p>This is equivalent to <code>consist.load(artifact, tracker=self, ...)</code> and uses the artifact driver to select the appropriate loader.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact to load.</p> required <code>**kwargs</code> <code>Any</code> <p>Loader-specific options forwarded to <code>consist.load</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The loaded data object (e.g., DuckDB Relation, xarray.Dataset, etc.).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.materialize","level":2,"title":"<code>materialize(artifact, destination_path, *, on_missing='warn')</code>","text":"<p>Materialize a cached artifact onto the filesystem.</p> <p>This copies bytes from the resolved artifact URI to <code>destination_path</code>. It does not perform database-backed reconstruction.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The destination path for the materialized artifact, or <code>None</code> if missing and <code>on_missing=\"warn\"</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.ingest","level":2,"title":"<code>ingest(artifact, data=None, schema=None, run=None, profile_schema=True)</code>","text":"<p>Ingests data associated with an <code>Artifact</code> into the Consist DuckDB database.</p> <p>This method is central to Consist's \"Hot Data Strategy\", where data is materialized into the database for faster query performance and easier sharing. It leverages the <code>dlt</code> (Data Load Tool) integration for efficient and robust data loading, including support for schema inference and evolution.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact object representing the data being ingested. If the artifact was logged with a schema (e.g., <code>log_artifact(path, schema=MySchema)</code>) and that schema was registered with the Tracker at initialization (e.g., <code>Tracker(..., schemas=[MySchema])</code>), it will be automatically looked up and used for ingestion.</p> required <code>data</code> <code>Optional[Union[Iterable[Dict[str, Any]], Any]]</code> <p>An iterable (e.g., list of dicts, generator) where each item represents a row of data to be ingested. If <code>data</code> is omitted, Consist attempts to stream it directly from the artifact's file URI, resolving the path. Can also be other data types that <code>dlt</code> can handle directly (e.g., Pandas DataFrame).</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the ingested data. If provided, <code>dlt</code> will use this for strict validation and this parameter takes precedence over any auto-detected schema. If not provided, Consist will automatically look up the schema by name from schemas registered in Tracker.init (using artifact.meta[\"schema_name\"]).</p> <code>None</code> <code>run</code> <code>Optional[Run]</code> <p>If provided, tags data with this run's ID (Offline Mode). If None, uses the currently active run (Online Mode).</p> <code>None</code> <code>profile_schema</code> <code>bool</code> <p>If True, profile and persist a deduped schema record for the ingested table, writing <code>schema_id</code>/<code>schema_summary</code> (and optionally <code>schema_profile</code>) into <code>Artifact.meta</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result information from the <code>dlt</code> ingestion process.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no database is configured (<code>db_path</code> was not provided during Tracker initialization) or if <code>ingest</code> is called outside of an active run context.</p> <code>Exception</code> <p>Any exception raised by the underlying <code>dlt</code> ingestion process.</p> <p>Examples:</p> <p>Auto-detected schema workflow (register schemas at tracker init):</p> <pre><code>&gt;&gt;&gt; tracker = Tracker(..., schemas=[MyDataSchema])\n&gt;&gt;&gt; art = tracker.log_artifact(file.csv, schema=MyDataSchema)\n&gt;&gt;&gt; tracker.ingest(art, data=df)  # Automatically looks up and uses MyDataSchema\n</code></pre> <p>Cross-session workflow (schemas persist in metadata):</p> <pre><code>&gt;&gt;&gt; # Session 1:\n&gt;&gt;&gt; tracker = Tracker(..., schemas=[MyDataSchema])\n&gt;&gt;&gt; art = tracker.log_artifact(file.csv, schema=MyDataSchema)\n&gt;&gt;&gt; # Session 2:\n&gt;&gt;&gt; tracker2 = Tracker(..., schemas=[MyDataSchema])\n&gt;&gt;&gt; art2 = tracker2.get_artifact(\"mydata\")\n&gt;&gt;&gt; tracker2.ingest(art2, data=df)  # Looks up MyDataSchema by artifact's schema_name\n</code></pre> <p>Override auto-detection (explicit schema always wins):</p> <pre><code>&gt;&gt;&gt; tracker.ingest(art, data=df, schema=DifferentSchema)  # Uses DifferentSchema\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_runs","level":2,"title":"<code>find_runs(tags=None, year=None, iteration=None, model=None, status=None, parent_id=None, metadata=None, limit=100, index_by=None, name=None)</code>","text":"<p>Retrieve runs matching the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[List[str]]</code> <p>Filter runs that contain all provided tags.</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Filter by run year.</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Filter by run iteration.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by run model name.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Filter by run status (e.g., \"completed\", \"failed\").</p> <code>None</code> <code>parent_id</code> <code>Optional[str]</code> <p>Filter by scenario/header parent id.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Filter by exact matches in <code>Run.meta</code> (client-side filter).</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to return.</p> <code>100</code> <code>index_by</code> <code>Optional[Union[str, IndexBySpec]]</code> <p>If provided, returns a dict keyed by a run attribute or facet value. Supported forms: - <code>\"year\"</code> / <code>\"iteration\"</code> / any Run attribute name - <code>\"facet.&lt;key&gt;\"</code> or <code>\"facet:&lt;key&gt;\"</code> to key by a persisted facet value - <code>IndexBySpec</code> helpers like <code>index_by_field(...)</code> / <code>index_by_facet(...)</code></p> <p>Note: if multiple runs share the same key, the last one wins.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Filter by <code>Run.model_name</code>/name alias used by DatabaseManager.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[Run], Dict[Hashable, Run]]</code> <p>List of runs, or a dict keyed by <code>index_by</code> when requested.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>index_by</code> is an unsupported type.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_run","level":2,"title":"<code>find_run(**kwargs)</code>","text":"<p>Find exactly one run matching the criteria.</p> <p>This is a convenience wrapper around <code>find_runs(...)</code> that enforces uniqueness.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Filters forwarded to <code>find_runs(...)</code>. Special cases: - <code>id</code> or <code>run_id</code>: if provided, performs a direct primary-key lookup.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Run</code> <p>The matching run.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runs match, or more than one run matches.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_latest_run","level":2,"title":"<code>find_latest_run(*, parent_id=None, model=None, status=None, year=None, tags=None, metadata=None, limit=10000)</code>","text":"<p>Return the most recent run matching the filters.</p> <p>Selection priority: 1) Highest <code>iteration</code> (when present) 2) Newest <code>created_at</code> (fallback when no iteration is set)</p> <p>Parameters:</p> Name Type Description Default <code>parent_id</code> <code>Optional[str]</code> <p>Filter by scenario/parent run ID.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by model name.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Filter by run status.</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Filter by run year.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Filter runs that contain all provided tags.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Filter by exact matches in <code>Run.meta</code> (client-side filter).</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to consider.</p> <code>10_000</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_latest_run_id","level":2,"title":"<code>get_latest_run_id(**kwargs)</code>","text":"<p>Convenience wrapper to return the latest run ID for the given filters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Filters forwarded to <code>find_latest_run</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The run ID of the latest matching run.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runs match the provided filters.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_artifacts","level":2,"title":"<code>find_artifacts(*, creator=None, consumer=None, key=None, limit=100)</code>","text":"<p>Find artifacts by producing/consuming runs and key.</p> <p>Parameters:</p> Name Type Description Default <code>creator</code> <code>Optional[Union[str, Run]]</code> <p>Run ID (or Run) that logged the artifact as an output.</p> <code>None</code> <code>consumer</code> <code>Optional[Union[str, Run]]</code> <p>Run ID (or Run) that logged the artifact as an input.</p> <code>None</code> <code>key</code> <code>Optional[str]</code> <p>Exact artifact key to match.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of artifacts to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>list</code> <p>Matching artifact records (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifact","level":2,"title":"<code>get_artifact(key_or_id)</code>","text":"<p>Retrieves an Artifact by its semantic key or UUID.</p> <p>This method provides a flexible way to locate artifacts, first checking the in-memory context of the current run, and then querying the database for persistent records.</p> <p>Parameters:</p> Name Type Description Default <code>key_or_id</code> <code>Union[str, UUID]</code> <p>The artifact's 'key' (e.g., \"households\") or its unique UUID. When a string is provided, the most recently created artifact matching that key is returned.</p> required <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>The found <code>Artifact</code> object, or <code>None</code> if no matching artifact is found.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifacts_for_run","level":2,"title":"<code>get_artifacts_for_run(run_id)</code>","text":"<p>Retrieve inputs and outputs for a specific run, organized by key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <p>Returns:</p> Type Description <code>RunArtifacts</code> <p>Container with <code>inputs</code> and <code>outputs</code> dicts. Returns empty collections if the database is not configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run","level":2,"title":"<code>get_run(run_id)</code>","text":"<p>Retrieve a single Run by its ID from the database.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The unique identifier of the run to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The Run object if found, or <code>None</code> if missing or no database is configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_config","level":2,"title":"<code>get_run_config(run_id, *, allow_missing=False)</code>","text":"<p>Load the full config snapshot for a historical run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>allow_missing</code> <code>bool</code> <p>Return <code>None</code> if the snapshot is missing instead of raising.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>The stored config payload, or <code>None</code> if missing and <code>allow_missing</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_inputs","level":2,"title":"<code>get_run_inputs(run_id)</code>","text":"<p>Return input artifacts for a run, keyed by artifact key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Input artifacts keyed by artifact key. Returns an empty dict if the database is not configured or the run is unknown.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_outputs","level":2,"title":"<code>get_run_outputs(run_id)</code>","text":"<p>Return output artifacts for a run, keyed by artifact key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Output artifacts keyed by artifact key. Returns an empty dict if the database is not configured or the run is unknown.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifact_lineage","level":2,"title":"<code>get_artifact_lineage(artifact_key_or_id, *, max_depth=None)</code>","text":"<p>Recursively builds a lineage tree for a given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_key_or_id</code> <code>Union[str, UUID]</code> <p>Artifact key or UUID.</p> required <code>max_depth</code> <code>Optional[int]</code> <p>Maximum depth to traverse (0 returns only the artifact). Useful for large graphs or iterative workflows.</p> <code>None</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.print_lineage","level":2,"title":"<code>print_lineage(artifact_key_or_id, *, max_depth=None, show_run_ids=False)</code>","text":"<p>Print a formatted lineage tree for an artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_key_or_id</code> <code>Union[str, UUID]</code> <p>Artifact key or UUID to print.</p> required <code>max_depth</code> <code>Optional[int]</code> <p>Maximum depth to traverse (0 prints only the artifact).</p> <code>None</code> <code>show_run_ids</code> <code>bool</code> <p>Include run IDs alongside artifact entries.</p> <code>False</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.history","level":2,"title":"<code>history(limit=10, tags=None)</code>","text":"<p>Return recent runs as a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of runs to include.</p> <code>10</code> <code>tags</code> <code>Optional[List[str]]</code> <p>If provided, filter runs to those containing any of the given tags.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of recent runs (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.diff_runs","level":2,"title":"<code>diff_runs(run_id_a, run_id_b, *, namespace=None, prefix=None, keys=None, limit=10000, include_equal=False)</code>","text":"<p>Compare flattened config facets between two runs.</p> <p>Parameters:</p> Name Type Description Default <code>run_id_a</code> <code>str</code> <p>Baseline run identifier.</p> required <code>run_id_b</code> <code>str</code> <p>Comparison run identifier.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Namespace for facets. Defaults to each run's model name.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Filter keys by prefix (e.g. <code>\"inputs.\"</code>).</p> <code>None</code> <code>keys</code> <code>Optional[Iterable[str]]</code> <p>Only include specific keys when provided.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of entries to inspect per run.</p> <code>10_000</code> <code>include_equal</code> <code>bool</code> <p>If True, include keys whose values are unchanged.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with <code>namespace</code> metadata and <code>changes</code> mapping keys to values.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_facet","level":2,"title":"<code>get_config_facet(facet_id)</code>","text":"<p>Retrieve a single persisted config facet by ID.</p> <p>Parameters:</p> Name Type Description Default <code>facet_id</code> <code>str</code> <p>The facet identifier.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The facet record if present, otherwise <code>None</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_facets","level":2,"title":"<code>get_config_facets(*, namespace=None, schema_name=None, limit=100)</code>","text":"<p>List persisted config facets, optionally filtered.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Optional[str]</code> <p>Filter facets by namespace.</p> <code>None</code> <code>schema_name</code> <code>Optional[str]</code> <p>Filter facets by schema name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of facet records to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of facet records (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_config_kv","level":2,"title":"<code>get_run_config_kv(run_id, *, namespace=None, prefix=None, limit=10000)</code>","text":"<p>Retrieve flattened key/value config entries for a run.</p> <p>This is primarily used for querying and debugging indexed config facets.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Filter by namespace.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Filter keys by prefix (e.g. <code>\"inputs.\"</code>).</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of entries to return.</p> <code>10_000</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of key/value rows (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_values","level":2,"title":"<code>get_config_values(run_id, *, namespace=None, prefix=None, keys=None, limit=10000)</code>","text":"<p>Return a flattened config facet as a dict of key/value pairs.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Namespace for the facet. Defaults to the run's model name when available.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Filter keys by prefix (e.g. <code>\"inputs.\"</code>).</p> <code>None</code> <code>keys</code> <code>Optional[Iterable[str]]</code> <p>Only include specific keys when provided.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of entries to return.</p> <code>10_000</code> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping of flattened keys to typed values.</p> Notes <p>Keys are stored as flattened dotted paths. If an original key contains a literal dot, it is escaped as <code>\"\\.\"</code> in the stored key.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_value","level":2,"title":"<code>get_config_value(run_id, key, *, namespace=None, default=None)</code>","text":"<p>Retrieve a single config value from a flattened config facet.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>key</code> <code>str</code> <p>Flattened key to fetch.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Namespace for the facet. Defaults to the run's model name when available.</p> <code>None</code> <code>default</code> <code>Any</code> <p>Value to return when the key is missing.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The typed value for the key, or <code>default</code> if missing.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_runs_by_facet_kv","level":2,"title":"<code>find_runs_by_facet_kv(*, namespace, key, value_type=None, value_str=None, value_num=None, value_bool=None, limit=100)</code>","text":"<p>Find runs by a flattened config facet key/value.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Facet namespace.</p> required <code>key</code> <code>str</code> <p>Flattened facet key.</p> required <code>value_type</code> <code>Optional[str]</code> <p>Optional discriminator for the value column (implementation dependent).</p> <code>None</code> <code>value_str</code> <code>Optional[str]</code> <p>String value to match.</p> <code>None</code> <code>value_num</code> <code>Optional[float]</code> <p>Numeric value to match.</p> <code>None</code> <code>value_bool</code> <code>Optional[bool]</code> <p>Boolean value to match.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>list</code> <p>Matching run records (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.view","level":2,"title":"<code>view(model, key=None)</code>","text":"<p>Create/register a hybrid view for a given SQLModel schema.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[SQLModel]</code> <p>SQLModel schema defining the logical columns for the concept.</p> required <code>key</code> <code>Optional[str]</code> <p>Override the concept key (defaults to <code>model.__tablename__</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[SQLModel]</code> <p>The dynamic SQLModel view class exposed via <code>tracker.views</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the tracker has no database configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.create_view","level":2,"title":"<code>create_view(view_name, concept_key)</code>","text":"<p>Create a named hybrid view over a registered concept.</p> <p>This is a lower-level helper than <code>Tracker.view(...)</code>. It is useful when you want to create multiple named views over the same concept key, or when you want explicit control over the view name.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>The SQL view name to create in the database (e.g., <code>\"v_persons\"</code>).</p> required <code>concept_key</code> <code>str</code> <p>The registered concept key to materialize (typically a table/artifact key).</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Backend-specific result from <code>ViewFactory.create_hybrid_view</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load_matrix","level":2,"title":"<code>load_matrix(concept_key, variables=None, *, run_ids=None, parent_id=None, model=None, status=None)</code>","text":"<p>Convenience wrapper for loading a matrix view from tracked artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>Semantic key for the matrix artifacts.</p> required <code>variables</code> <code>Optional[List[str]]</code> <p>Variables to load from each Zarr store; defaults to all variables.</p> <code>None</code> <code>run_ids</code> <code>Optional[List[str]]</code> <p>Restrict to specific run IDs.</p> <code>None</code> <code>parent_id</code> <code>Optional[str]</code> <p>Filter by scenario/parent run ID.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by model name.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Filter by run status.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>An <code>xarray.Dataset</code> containing the combined matrix data.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.export_schema_sqlmodel","level":2,"title":"<code>export_schema_sqlmodel(*, schema_id=None, artifact_id=None, out_path=None, table_name=None, class_name=None, abstract=True, include_system_cols=False, include_stats_comments=True, prefer_source=None)</code>","text":"<p>Export a captured artifact schema as a SQLModel stub for manual editing.</p> <p>Exactly one of <code>schema_id</code> or <code>artifact_id</code> must be provided. The generated Python source is returned and can optionally be written to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>schema_id</code> <code>Optional[str]</code> <p>Schema identifier to export (from the schema registry). If provided, prefer_source is ignored and this specific schema is used.</p> <code>None</code> <code>artifact_id</code> <code>Optional[Union[str, UUID]]</code> <p>Artifact ID to export the associated schema. When used, the schema selection respects the prefer_source parameter.</p> <code>None</code> <code>out_path</code> <code>Optional[Path]</code> <p>If provided, write the stub to this path and return its contents.</p> <code>None</code> <code>table_name</code> <code>Optional[str]</code> <p>Override the SQL table name in the generated class.</p> <code>None</code> <code>class_name</code> <code>Optional[str]</code> <p>Override the Python class name in the generated class.</p> <code>None</code> <code>abstract</code> <code>bool</code> <p>Whether to mark the generated class as abstract.</p> <code>True</code> <code>include_system_cols</code> <code>bool</code> <p>Whether to include Consist system columns in the stub.</p> <code>False</code> <code>include_stats_comments</code> <code>bool</code> <p>Whether to include column-level stats as comments.</p> <code>True</code> <code>prefer_source</code> <code>(file, duckdb)</code> <p>Preference hint for when user_provided schema does not exist. This is useful when an artifact has both a file profile (pandas dtypes) and a duckdb profile (post-ingestion types). Ignored if schema_id is provided directly.</p> <p>IMPORTANT: User-provided schemas (manually curated with FK constraints, indexes, etc.) are ALWAYS preferred if they exist. This parameter does not override user_provided schemas.</p> <ul> <li>\"file\": Prefer the original file schema (CSV/Parquet with pandas dtypes)</li> <li>\"duckdb\": Prefer the post-ingestion schema from the DuckDB table</li> <li>None (default): Prefer file, as it preserves richer type information   (e.g., pandas category)</li> </ul> <code>\"file\"</code> <p>Returns:</p> Type Description <code>str</code> <p>The rendered SQLModel stub source.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the tracker has no database configured or if the selector is invalid.</p> <code>KeyError</code> <p>If no schema is found for the provided selector.</p> <p>Examples:</p> <p>Export file schema (original raw file dtypes):</p> <pre><code>&gt;&gt;&gt; tracker.export_schema_sqlmodel(artifact_id=art.id)\n</code></pre> <p>Export ingested table schema (after dlt normalization):</p> <pre><code>&gt;&gt;&gt; tracker.export_schema_sqlmodel(artifact_id=art.id, prefer_source=\"duckdb\")\n</code></pre> <p>Export a specific schema directly by ID:</p> <pre><code>&gt;&gt;&gt; tracker.export_schema_sqlmodel(schema_id=\"abc123xyz\")\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.netcdf_metadata","level":2,"title":"<code>netcdf_metadata(concept_key)</code>","text":"<p>Access NetCDF metadata views for a given artifact key.</p> <p>This provides convenient access to query and explore NetCDF file structures stored in Consist's metadata catalog.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key identifying the NetCDF artifact.</p> required <p>Returns:</p> Type Description <code>NetCdfMetadataView</code> <p>A view object with methods to explore variables, dimensions, and attributes.</p> Example <pre><code>view = tracker.netcdf_metadata(\"climate\")\nvariables = view.get_variables(year=2024)\nprint(view.summary(\"climate\"))\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.openmatrix_metadata","level":2,"title":"<code>openmatrix_metadata(concept_key)</code>","text":"<p>Access OpenMatrix metadata views for a given artifact key.</p> <p>This provides convenient access to query and explore OpenMatrix file structures stored in Consist's metadata catalog.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key identifying the OpenMatrix artifact.</p> required <p>Returns:</p> Type Description <code>OpenMatrixMetadataView</code> <p>A view object with methods to explore matrices, zones, and attributes.</p> Example <pre><code>view = tracker.openmatrix_metadata(\"demand\")\nmatrices = view.get_matrices(year=2024)\nzones = view.get_zone_counts()\nprint(view.summary(\"demand\"))\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.spatial_metadata","level":2,"title":"<code>spatial_metadata(concept_key)</code>","text":"<p>Access spatial metadata views for a given artifact key.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key identifying the spatial artifact.</p> required <p>Returns:</p> Type Description <code>SpatialMetadataView</code> <p>A view object with methods to explore spatial metadata.</p> Example <pre><code>view = tracker.spatial_metadata(\"parcels\")\nbounds = view.get_bounds(\"parcels\")\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.canonicalize_config","level":2,"title":"<code>canonicalize_config(adapter, config_dirs, *, run=None, run_id=None, strict=False, ingest=True, profile_schema=False, options=None)</code>","text":"<p>Canonicalize a model-specific config directory and ingest queryable slices.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>ConfigAdapter</code> <p>Adapter implementation for the model (e.g., ActivitySim).</p> required <code>config_dirs</code> <code>Iterable[Union[str, Path]]</code> <p>Ordered config directories to canonicalize.</p> required <code>run</code> <code>Optional[Run]</code> <p>Run context to attach to; defaults to the active run.</p> <code>None</code> <code>run_id</code> <code>Optional[str]</code> <p>Run identifier; must match the active run when provided.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>If True, adapter should error on missing references.</p> <code>False</code> <code>ingest</code> <code>bool</code> <p>Whether to ingest any queryable tables produced by the adapter.</p> <code>True</code> <code>profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas.</p> <code>False</code> <code>options</code> <code>Optional[ConfigAdapterOptions]</code> <p>Shared adapter options that override strict/ingest defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigContribution</code> <p>Structured summary of logged artifacts and ingestables.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.prepare_config","level":2,"title":"<code>prepare_config(adapter, config_dirs, *, strict=False, options=None, validate_only=False, facet_spec=None, facet_schema_name=None, facet_schema_version=None, facet_index=None)</code>","text":"<p>Prepare a config plan without logging artifacts or ingesting data.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>ConfigAdapter</code> <p>Adapter implementation for the model (e.g., ActivitySim).</p> required <code>config_dirs</code> <code>Iterable[Union[str, Path]]</code> <p>Ordered config directories to canonicalize.</p> required <code>strict</code> <code>bool</code> <p>If True, adapter should error on missing references.</p> <code>False</code> <code>options</code> <code>Optional[ConfigAdapterOptions]</code> <p>Shared adapter options that override strict defaults.</p> <code>None</code> <code>validate_only</code> <code>bool</code> <p>If True, validate ingestables without logging or ingesting.</p> <code>False</code> <code>facet_spec</code> <code>Optional[Dict[str, Any]]</code> <p>Adapter-specific facet extraction spec.</p> <code>None</code> <code>facet_schema_name</code> <code>Optional[str]</code> <p>Optional facet schema name for persistence.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Optional facet schema version for persistence.</p> <code>None</code> <code>facet_index</code> <code>Optional[bool]</code> <p>Optional flag controlling KV facet indexing.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigPlan</code> <p>Pre-run config plan containing artifacts and ingestables.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.apply_config_plan","level":2,"title":"<code>apply_config_plan(plan, *, run=None, ingest=True, profile_schema=False, adapter=None, options=None)</code>","text":"<p>Apply a pre-run config plan to the active run.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>ConfigPlan</code> <p>Plan produced by <code>prepare_config</code>.</p> required <code>run</code> <code>Optional[Run]</code> <p>Run context to attach to; defaults to the active run.</p> <code>None</code> <code>ingest</code> <code>bool</code> <p>Whether to ingest any queryable tables produced by the adapter.</p> <code>True</code> <code>profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas.</p> <code>False</code> <code>adapter</code> <code>Optional[ConfigAdapter]</code> <p>Adapter instance used to create run-scoped artifacts, if needed.</p> <code>None</code> <code>options</code> <code>Optional[ConfigAdapterOptions]</code> <p>Shared adapter options that override ingest defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigContribution</code> <p>Structured summary of logged artifacts and ingestables.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.identity_from_config_plan","level":2,"title":"<code>identity_from_config_plan(plan)</code>","text":"<p>Return the identity hash derived from a config plan.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>ConfigPlan</code> <p>Config plan produced by <code>prepare_config</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Stable hash representing the canonical config content.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_h5_container","level":2,"title":"<code>log_h5_container(path, key=None, direction='output', discover_tables=True, table_filter=None, hash_tables='if_unchanged', table_hash_chunk_rows=None, **meta)</code>","text":"<p>Log an HDF5 file and optionally discover its internal tables.</p> <p>This method provides first-class HDF5 container support, automatically discovering and logging internal tables as child artifacts. This is particularly useful for model pipelines that use HDF5 files containing multiple datasets or tables.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the HDF5 file.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the container. If not provided, uses the file stem.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether this is an \"input\" or \"output\" artifact.</p> <code>\"output\"</code> <code>discover_tables</code> <code>bool</code> <p>If True, scan the file and create child artifacts for each table/dataset.</p> <code>True</code> <code>table_filter</code> <code>Optional[Union[Callable[[str], bool], List[str]]]</code> <p>Filter which tables to log. Can be: - A callable that takes a table name and returns True to include - A list of table names to include (exact match) If None, all tables are included.</p> <code>None</code> <code>hash_tables</code> <code>Literal['always', 'if_unchanged', 'never']</code> <p>Whether to compute content hashes for discovered tables. \"if_unchanged\" skips hashing when a table appears unchanged based on lightweight checks.</p> <code>\"if_unchanged\"</code> <code>table_hash_chunk_rows</code> <code>Optional[int]</code> <p>Row chunk size to use when hashing large tables.</p> <code>None</code> <code>**meta</code> <code>Any</code> <p>Additional metadata for the container artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Artifact, List[Artifact]]</code> <p>A tuple of (container_artifact, list_of_table_artifacts).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ImportError</code> <p>If h5py is not installed and discover_tables is True.</p> Example <pre><code># Log HDF5 file with auto-discovery of all tables\ncontainer, tables = tracker.log_h5_container(\"data.h5\", key=\"urbansim_data\")\nprint(f\"Logged {len(tables)} tables from container\")\n\n# Filter tables by callable\ncontainer, tables = tracker.log_h5_container(\n    \"data.h5\",\n    key=\"urbansim_data\",\n    table_filter=lambda name: name.startswith(\"/2025/\")\n)\n\n# Filter tables by list of names\ncontainer, tables = tracker.log_h5_container(\n    \"data.h5\",\n    key=\"urbansim_data\",\n    table_filter=[\"households\", \"persons\", \"buildings\"]\n)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_h5_table","level":2,"title":"<code>log_h5_table(path, *, table_path, key=None, direction='output', parent=None, hash_table=True, table_hash_chunk_rows=None, profile_file_schema=False, file_schema_sample_rows=None, **meta)</code>","text":"<p>Log a single HDF5 table as an artifact without scanning the container.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the HDF5 file on disk.</p> required <code>table_path</code> <code>str</code> <p>Internal table/dataset path inside the HDF5 container.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic key for the table artifact. Defaults to the dataset name.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether the table is an \"input\" or \"output\".</p> <code>\"output\"</code> <code>parent</code> <code>Optional[Artifact]</code> <p>Optional parent container artifact to link this table to.</p> <code>None</code> <code>hash_table</code> <code>bool</code> <p>Whether to compute a content hash for the table.</p> <code>True</code> <code>table_hash_chunk_rows</code> <code>Optional[int]</code> <p>Chunk size for hashing large tables.</p> <code>None</code> <code>profile_file_schema</code> <code>bool | Literal['if_changed']</code> <p>Whether to profile table schema and store it as metadata.</p> <code>False</code> <code>file_schema_sample_rows</code> <code>Optional[int]</code> <p>Number of rows to sample when profiling schema.</p> <code>None</code> <code>**meta</code> <code>Any</code> <p>Additional metadata to store on the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created table artifact.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_netcdf_file","level":2,"title":"<code>log_netcdf_file(path, key=None, direction='output', **meta)</code>","text":"<p>Log a NetCDF file as an artifact with metadata extraction.</p> <p>This method provides convenient logging for NetCDF files, automatically detecting the driver and storing structural metadata about variables, dimensions, and coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the NetCDF file.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact. If not provided, uses the file stem.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether this is an \"input\" or \"output\" artifact.</p> <code>\"output\"</code> <code>**meta</code> <code>Any</code> <p>Additional metadata for the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged artifact with metadata extracted from the NetCDF structure.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ImportError</code> <p>If xarray is not installed.</p> Example <pre><code># Log NetCDF file\nart = tracker.log_netcdf_file(\"climate_data.nc\", key=\"temperature\")\n# Optionally ingest metadata\ntracker.ingest(art)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_openmatrix_file","level":2,"title":"<code>log_openmatrix_file(path, key=None, direction='output', **meta)</code>","text":"<p>Log an OpenMatrix (OMX) file as an artifact with metadata extraction.</p> <p>This method provides convenient logging for OpenMatrix files, automatically detecting the driver and storing structural metadata about matrices, dimensions, and attributes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the OpenMatrix file.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact. If not provided, uses the file stem.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether this is an \"input\" or \"output\" artifact.</p> <code>\"output\"</code> <code>**meta</code> <code>Any</code> <p>Additional metadata for the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged artifact with metadata extracted from the OpenMatrix structure.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ImportError</code> <p>If neither h5py nor openmatrix is installed.</p> Example <pre><code># Log OpenMatrix file (e.g., ActivitySim travel demand)\nart = tracker.log_openmatrix_file(\"demand.omx\", key=\"travel_demand\")\n# Optionally ingest metadata\ntracker.ingest(art)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.set_run_subdir_fn","level":2,"title":"<code>set_run_subdir_fn(fn)</code>","text":"<p>Set a callable that returns the per-run artifact subdirectory name.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Optional[Callable[[Run], str]]</code> <p>Callable that accepts a <code>Run</code> and returns a relative directory name. Set to <code>None</code> to disable the custom resolver.</p> required","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.run_artifact_dir","level":2,"title":"<code>run_artifact_dir(run=None)</code>","text":"<p>Resolve the run-specific artifact directory for the active run.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>Optional[Run]</code> <p>Run to resolve the directory for. Defaults to the current run if active.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Directory under <code>run_dir</code> where run artifacts should be written by default. Absolute artifact_dir values outside <code>run_dir</code> are only allowed when allow_external_paths is enabled.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.resolve_uri","level":2,"title":"<code>resolve_uri(uri)</code>","text":"<p>** Delegates to FileSystemManager. **</p> <p>Converts a portable Consist URI back into an absolute file system path.</p> <p>This is the inverse operation of <code>_virtualize_path</code>, crucial for \"Path Resolution &amp; Mounts\". It uses the configured <code>mounts</code> and the <code>run_dir</code> to reconstruct the local absolute path to an artifact, making runs portable across different environments.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The portable URI (e.g., \"inputs://file.csv\", \"./output/data.parquet\") to resolve.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute file system path corresponding to the given URI. If the URI cannot be fully resolved (e.g., scheme not mounted), it returns the most resolved path or the original URI after attempting to make it absolute. Mounted URIs are validated to prevent path traversal outside the mount root.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.run_query","level":2,"title":"<code>run_query(query)</code>","text":"<p>Execute a SQLModel/SQLAlchemy query via the tracker engine.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Executable</code> <p>Query object (<code>select</code>, <code>text</code>, etc.).</p> required <p>Returns:</p> Type Description <code>list</code> <p>Results of the executed query.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no database is configured for this tracker.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_record","level":2,"title":"<code>get_run_record(run_id, *, allow_missing=False)</code>","text":"<p>Load the full run record snapshot from disk.</p> <p>This reads the JSON snapshot produced at run time (<code>consist_runs/&lt;id&gt;.json</code>) and returns the parsed <code>ConsistRecord</code>.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>allow_missing</code> <code>bool</code> <p>Return <code>None</code> if the snapshot file is missing or unreadable instead of raising.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[ConsistRecord]</code> <p>The parsed run record, or <code>None</code> if missing and <code>allow_missing</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.resolve_historical_path","level":2,"title":"<code>resolve_historical_path(artifact, run)</code>","text":"<p>Resolve the on-disk path for an artifact from a prior run.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact whose historical location should be resolved.</p> required <code>run</code> <code>Run</code> <p>The run that originally produced/consumed the artifact.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The resolved filesystem path for the artifact in its original run workspace.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load_input_bundle","level":2,"title":"<code>load_input_bundle(run_id)</code>","text":"<p>Load a set of input artifacts from a prior \"bundle\" run by run_id.</p> <p>This is a convenience helper for shared DuckDB bundles where a dedicated run logs all required inputs as outputs. The returned dict can be passed directly to <code>inputs=[...]</code> on a new run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The run id that logged the bundle outputs.</p> required <p>Returns:</p> Type Description <code>dict[str, Artifact]</code> <p>Mapping of artifact key -&gt; Artifact from the bundle run.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the run does not exist or has no output artifacts.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifact_by_uri","level":2,"title":"<code>get_artifact_by_uri(uri, *, table_path=None, array_path=None)</code>","text":"<p>Find an artifact by its URI.</p> <p>Useful for checking if a specific file has been logged, or for retrieving artifact metadata by path.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The portable URI to search for (e.g., \"inputs://households.csv\").</p> required <code>table_path</code> <code>Optional[str]</code> <p>Optional table path to match.</p> <code>None</code> <code>array_path</code> <code>Optional[str]</code> <p>Optional array path to match.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>The found <code>Artifact</code> object, or <code>None</code> if no matching artifact is found.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_artifact","level":2,"title":"<code>get_run_artifact(run_id, key=None, key_contains=None, direction='output')</code>","text":"<p>Convenience helper to fetch a single artifact for a specific run.</p> <p>Args:     run_id: Run identifier.     key: Exact key to match (if present in logged artifacts).     key_contains: Optional substring to match when the exact key is unknown.     direction: \"output\" (default) or \"input\".</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load_run_output","level":2,"title":"<code>load_run_output(run_id, key, **kwargs)</code>","text":"<p>Load a specific output artifact from a run by key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>key</code> <code>str</code> <p>Output artifact key to load.</p> required <code>**kwargs</code> <code>Any</code> <p>Forwarded to <code>Tracker.load(...)</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded artifact data.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_matching_run","level":2,"title":"<code>find_matching_run(config_hash, input_hash, git_hash)</code>","text":"<p>Find a previously completed run that matches the identity hashes.</p> <p>Parameters:</p> Name Type Description Default <code>config_hash</code> <code>str</code> <p>Hash of the canonicalized config for the run.</p> required <code>input_hash</code> <code>str</code> <p>Hash of the run inputs.</p> required <code>git_hash</code> <code>str</code> <p>Git commit hash captured with the run.</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The matching run, or <code>None</code> if not found or if no database is configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.on_run_start","level":2,"title":"<code>on_run_start(callback)</code>","text":"<p>Register a callback to be invoked when a run starts.</p> <p>The callback receives the <code>Run</code> object after it has been initialized but before any user code executes. This is useful for external integrations like OpenLineage event emission, logging, or notifications.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Run], None]</code> <p>A function that takes a <code>Run</code> object as its only argument.</p> required <p>Returns:</p> Type Description <code>Callable[[Run], None]</code> <p>The same callback, allowing use as a decorator.</p> Example <pre><code>@tracker.on_run_start\ndef log_start(run):\n    print(f\"Starting run: {run.id}\")\n\n# Or without decorator:\ntracker.on_run_start(my_callback_function)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.on_run_complete","level":2,"title":"<code>on_run_complete(callback)</code>","text":"<p>Register a callback to be invoked when a run completes successfully.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Run, List[Artifact]], None]</code> <p>Called with the completed <code>Run</code> and its output artifacts.</p> required <p>Returns:</p> Type Description <code>Callable[[Run, List[Artifact]], None]</code> <p>The same callback, allowing use as a decorator.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.on_run_failed","level":2,"title":"<code>on_run_failed(callback)</code>","text":"<p>Register a callback to be invoked when a run fails.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Run, Exception], None]</code> <p>Called with the failed <code>Run</code> and the raised exception.</p> required <p>Returns:</p> Type Description <code>Callable[[Run, Exception], None]</code> <p>The same callback, allowing use as a decorator.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/views/","level":1,"title":"Views","text":"","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#view-registry","level":2,"title":"View Registry","text":"<p>Registry for dynamic view classes. Accessing a view (e.g. registry.Person) automatically refreshes the underlying DuckDB SQL definition to include new files.</p> <p>Use <code>register(model, key=...)</code> to add SQLModel schemas. Accessing the attribute returns a dynamic SQLModel view class that can be queried via <code>select(...)</code>.</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#consist.core.views.ViewRegistry.register","level":2,"title":"<code>register(model, key=None)</code>","text":"","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#view-factory","level":2,"title":"View Factory","text":"<p>A factory class responsible for generating \"Hybrid Views\" in DuckDB, acting as Consist's \"The Virtualizer\" component.</p> <p>Hybrid Views combine data from materialized tables (often ingested via dlt) with data directly from file-based artifacts (e.g., Parquet, CSV), providing a unified SQL interface to query both \"hot\" and \"cold\" data transparently. This approach is central to Consist's flexible data access strategy.</p> <p>Attributes:</p> Name Type Description <code>tracker</code> <code>Tracker</code> <p>An instance of the Consist <code>Tracker</code>, which provides access to the database engine, artifact resolution, and other run-time context necessary for view creation.</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#consist.core.views.ViewFactory.create_view_from_model","level":2,"title":"<code>create_view_from_model(model, key=None)</code>","text":"<p>Creates both the SQL View and the Python SQLModel class for a given schema.</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#consist.core.views.ViewFactory.create_hybrid_view","level":2,"title":"<code>create_hybrid_view(view_name, concept_key, driver_filter=None, schema_model=None)</code>","text":"<p>Creates or replaces a DuckDB SQL VIEW that combines \"hot\" and \"cold\" data for a given concept.</p> <p>This method generates a \"Hybrid View\" which allows transparent querying across different data storage types. It implements \"View Optimization\" by leveraging DuckDB's capabilities for vectorized reads from files. The resulting view uses <code>UNION ALL BY NAME</code> to gracefully handle \"Schema Evolution\" (different columns across runs or data sources) by nulling out missing columns.</p> <p>\"Hot\" data refers to records already materialized into a DuckDB table (e.g., via ingestion). \"Cold\" data refers to records still residing in file-based artifacts (e.g., Parquet, CSV). Identifiers are quoted for SQL safety; missing cold-file paths are skipped at view creation.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>The name to assign to the newly created or replaced SQL view. This is the name you will use in your SQL queries to access the combined data.</p> required <code>concept_key</code> <code>str</code> <p>The semantic key identifying the data concept (e.g., \"households\", \"transactions\"). Artifacts and materialized tables matching this key will be included in the view.</p> required <code>driver_filter</code> <code>Optional[List[str]]</code> <p>An optional list of artifact drivers (e.g., \"parquet\", \"csv\") to include when querying \"cold\" data. If <code>None</code>, \"parquet\" and \"csv\" drivers are considered by default.</p> <code>None</code> <code>schema_model</code> <code>Type[SQLModel]</code> <p>SQL table definition for underlying data</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the view creation was attempted (even if the view ends up empty), False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the <code>Tracker</code>'s database engine is not configured (i.e., <code>db_path</code> was not provided during <code>Tracker</code> initialization).</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/workflow/","level":1,"title":"Workflow Contexts","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#scenario-context","level":2,"title":"Scenario Context","text":"<p>A context manager for grouping multiple steps into a single \"scenario\".</p> <p>A scenario creates a parent run (the \"header\") that aggregates the results, metadata, and lineage of all steps executed within its block. It provides a <code>coupler</code> to pass artifacts between steps, making it ideal for multi-stage simulation workflows.</p> <p>Attributes:</p> Name Type Description <code>coupler</code> <code>Coupler</code> <p>Scenario-local artifact registry for passing outputs between steps. Supports runtime-declared output validation.</p> <p>Examples:</p> <pre><code>with tracker.scenario(\"base_case\") as sc:\n    # Step 1: Pre-process\n    sc.run(preprocess_fn, inputs={\"raw\": \"data.csv\"}, outputs=[\"clean\"])\n    # Step 2: Model (reads \"clean\" from the coupler automatically)\n    sc.run(model_fn, input_keys=[\"clean\"], outputs=[\"results\"])\n</code></pre>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.run_id","level":2,"title":"<code>run_id</code>  <code>property</code>","text":"<p>Run ID of the scenario header.</p> <p>Returns:</p> Type Description <code>str</code> <p>The run ID for the scenario header (or the scenario name if the header has not been created yet).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.config","level":2,"title":"<code>config</code>  <code>property</code>","text":"<p>Read-only view of the scenario configuration.</p> <p>Returns:</p> Type Description <code>MappingProxyType</code> <p>Immutable mapping of configuration values for the scenario. Updates are applied by changing inputs to the scenario, not by mutating this mapping.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.inputs","level":2,"title":"<code>inputs</code>  <code>property</code>","text":"<p>Read-only view of registered exogenous inputs.</p> <p>Returns:</p> Type Description <code>MappingProxyType</code> <p>Immutable mapping of input keys to artifacts added via <code>add_input</code>. This reflects only scenario-level inputs, not step inputs.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.add_input","level":2,"title":"<code>add_input(path, key, **kwargs)</code>","text":"<p>Log an external input artifact to the scenario header run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>Path (or prebuilt <code>Artifact</code>) representing the input.</p> required <code>key</code> <code>str</code> <p>Semantic key for the artifact.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional metadata forwarded to <code>Tracker.log_artifact</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Logged artifact associated with the scenario.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a step has already started or the scenario context is inactive.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.declare_outputs","level":2,"title":"<code>declare_outputs(*names, required=False, warn_undocumented=False, description=None)</code>","text":"<p>Declare outputs that should be present in the scenario coupler.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Output keys to declare.</p> <code>()</code> <code>required</code> <code>bool | Mapping[str, bool]</code> <p>Whether declared outputs are required. A mapping allows per-key overrides.</p> <code>False</code> <code>warn_undocumented</code> <code>bool</code> <p>If True, warn when outputs are logged that were not declared.</p> <code>False</code> <code>description</code> <code>Optional[Mapping[str, str]]</code> <p>Human-readable descriptions for declared outputs.</p> <code>None</code>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.require_outputs","level":2,"title":"<code>require_outputs(*names, required=True, warn_undocumented=False, description=None)</code>","text":"<p>Declare required outputs that must be present at scenario exit.</p> <p>This is a convenience wrapper around <code>declare_outputs</code> that defaults <code>required=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Output keys to require.</p> <code>()</code> <code>required</code> <code>bool | Mapping[str, bool]</code> <p>Whether required outputs are enforced. A mapping allows per-key overrides.</p> <code>True</code> <code>warn_undocumented</code> <code>bool</code> <p>If True, warn when outputs are logged that were not declared.</p> <code>False</code> <code>description</code> <code>Optional[Mapping[str, str]]</code> <p>Human-readable descriptions for required outputs.</p> <code>None</code>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.collect_by_keys","level":2,"title":"<code>collect_by_keys(artifacts, *keys, prefix='')</code>","text":"<p>Collect explicit artifacts into the scenario coupler by key.</p> <p>Parameters:</p> Name Type Description Default <code>artifacts</code> <code>Mapping[str, Artifact]</code> <p>Source artifacts mapping (usually outputs from a step).</p> required <code>*keys</code> <code>str</code> <p>Keys to collect from the mapping.</p> <code>()</code> <code>prefix</code> <code>str</code> <p>Optional prefix to apply to collected keys in the coupler.</p> <code>\"\"</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>The collected artifacts keyed by their (possibly prefixed) names.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.run","level":2,"title":"<code>run(fn=None, name=None, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', load_inputs=None, executor='python', container=None, runtime_kwargs=None, inject_context=False, output_mismatch='warn', output_missing='warn')</code>","text":"<p>Execute a cached scenario step and update the Coupler with outputs.</p> <p>This method wraps <code>Tracker.run</code> while ensuring the scenario header is updated with step metadata and artifacts. Use <code>runtime_kwargs</code> for runtime-only inputs and <code>consist.require_runtime_kwargs</code> to validate required keys.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.trace","level":2,"title":"<code>trace(name, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', output_mismatch='warn', output_missing='warn')</code>","text":"<p>Manual tracing context manager for scenario steps.</p> <p>This wraps <code>Tracker.trace</code> to log a step while allowing inline code blocks. Use <code>ScenarioContext.run</code> when you want function execution to be skipped on cache hits.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#run-context","level":2,"title":"Run Context","text":"<p>A lightweight helper object injected into user functions. When you execute a run with <code>inject_context=True</code>, Consist passes a <code>RunContext</code> to your function. This allows you to access run-aware helpers—like the run's dedicated artifact directory and artifact logging methods—without needing to reference a global tracker instance directly.</p> <p>Examples:</p> <pre><code>def my_step(ctx: RunContext):\n    # Access the run's dedicated directory\n    output_path = ctx.run_dir / \"results.csv\"\n    # ... generate file ...\n    ctx.log_artifact(output_path, \"results\")\n</code></pre>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.run_dir","level":2,"title":"<code>run_dir</code>  <code>property</code>","text":"<p>Run-specific output directory for the active step.</p> <p>Returns:</p> Type Description <code>Path</code> <p>The directory where this step should write outputs by default. This value is derived from the active run and respects any per-run artifact directory overrides.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.inputs","level":2,"title":"<code>inputs</code>  <code>property</code>","text":"<p>Mapping of input artifact keys to artifacts for the active step.</p> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Dictionary of input artifacts keyed by their semantic keys. Raises a <code>RuntimeError</code> if accessed outside an active run.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.load","level":2,"title":"<code>load(key_or_artifact)</code>","text":"<p>Load data from an input artifact by key or from an Artifact instance.</p> <p>Parameters:</p> Name Type Description Default <code>key_or_artifact</code> <code>Union[str, Artifact]</code> <p>Input artifact key from <code>inputs</code> or an Artifact object.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data (driver-dependent).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_artifact","level":2,"title":"<code>log_artifact(*args, **kwargs)</code>","text":"<p>Log an artifact within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_artifact</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_artifact</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_artifact</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged artifact.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_artifacts","level":2,"title":"<code>log_artifacts(*args, **kwargs)</code>","text":"<p>Log multiple artifacts within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_artifacts</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_artifacts</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_artifacts</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping of artifact keys to logged artifacts.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_input","level":2,"title":"<code>log_input(*args, **kwargs)</code>","text":"<p>Log an input artifact within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_input</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_input</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_input</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged input artifact.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_output","level":2,"title":"<code>log_output(*args, **kwargs)</code>","text":"<p>Log an output artifact within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_output</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_output</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_output</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged output artifact.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_meta","level":2,"title":"<code>log_meta(**kwargs)</code>","text":"<p>Update metadata for the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_meta</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Metadata key/value pairs to merge into the run record.</p> <code>{}</code>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.capture_outputs","level":2,"title":"<code>capture_outputs(directory, pattern='*')</code>","text":"<p>Capture files written under <code>directory</code> and log them as outputs on exit.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Directory to monitor for new or modified files.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern for files to capture.</p> <code>\"*\"</code> <p>Yields:</p> Type Description <code>OutputCapture</code> <p>Container listing artifacts that were logged during the context.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#coupler","level":2,"title":"Coupler","text":"<p>Scenario-local helper to thread named artifacts between steps.</p> <p>Coupler is intentionally small: - It stores the \"latest Artifact for a semantic key\" in-memory.</p> <p>It does not log artifacts, infer inputs/outputs, or mutate Artifacts as a side effect of reads. Keep provenance operations on the Tracker.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.set","level":2,"title":"<code>set(key, artifact)</code>","text":"<p>Store an artifact under a validated key.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.set_from_artifact","level":2,"title":"<code>set_from_artifact(key, value)</code>","text":"<p>Set an artifact, accepting both Artifact objects and artifact-like values.</p> <p>This method is useful when integrating with optional dependencies (like noop mode) where you may receive either: - A real Artifact (when tracking is enabled) - An artifact-like object with .path and .container_uri properties (noop mode) - A Path or string (fallback)</p> <p>All three forms are stored in the coupler and can be retrieved with get() or require().</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The coupler key (e.g., \"persons\", \"skims\"). Must follow artifact-key rules.</p> required <code>value</code> <code>Artifact or artifact - like or Path or str</code> <p>The value to store. Can be a real Artifact, artifact-like object with .path/.container_uri properties, a Path, or a string path.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The value that was stored.</p> <p>Examples:</p> <p>Using with optional Consist dependency: <pre><code># Works whether log_output returns Artifact or NoopArtifact\nartifact = tracker.log_output(path, key=\"persons\")\ncoupler.set_from_artifact(\"persons\", artifact)\n</code></pre></p> <p>Mixed real and fallback artifacts: <pre><code>artifact = log_output(path) or path  # Fallback to path string\ncoupler.set_from_artifact(\"key\", artifact)  # Handles both\n</code></pre></p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.update","level":2,"title":"<code>update(artifacts=None, /, **kwargs)</code>","text":"<p>Bulk-update the coupler mapping.</p> <p>Examples:</p> <p><code>coupler.update({\"persons\": art})</code> or <code>coupler.update(persons=art)</code></p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.get","level":2,"title":"<code>get(key)</code>","text":"<p>Return the current artifact for <code>key</code>, or None if unset (key is validated).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.require","level":2,"title":"<code>require(key)</code>","text":"<p>Return the artifact for <code>key</code>, raising a clear error if unset.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.keys","level":2,"title":"<code>keys()</code>","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.items","level":2,"title":"<code>items()</code>","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.values","level":2,"title":"<code>values()</code>","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.path","level":2,"title":"<code>path(key, *, required=True)</code>","text":"<p>Resolve an artifact's URI to an absolute host path.</p> <p>This does not mutate the Artifact; it only returns a resolved Path.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.declare_outputs","level":2,"title":"<code>declare_outputs(*names, required=False, warn_undocumented=False, description=None)</code>","text":"<p>Declare expected coupler outputs for runtime validation and documentation.</p> <p>This method enables early detection of missing outputs by validating at scenario exit time that all required outputs have been set. Can be used in conjunction with a schema, or independently for dynamic output declarations.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Output names to declare.</p> <code>()</code> <code>required</code> <code>bool or Mapping[str, bool]</code> <p>If bool: applies to all declared outputs. If Mapping: per-key required status (e.g., {\"persons\": True, \"jobs\": False}).</p> <code>False</code> <code>warn_undocumented</code> <code>bool</code> <p>Warn when setting keys that were not declared.</p> <code>False</code> <code>description</code> <code>Mapping[str, str]</code> <p>Human-readable descriptions of outputs for documentation.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any name is not a string.</p> <code>ValueError</code> <p>If any name is empty after stripping whitespace.</p> <p>Examples:</p> <p>Declare all outputs as required:</p> <pre><code>coupler.declare_outputs(\"persons\", \"households\", required=True)\n</code></pre> <p>Mix required and optional:</p> <pre><code>coupler.declare_outputs(\n    \"persons\", \"households\", \"legacy_format\",\n    required={\"persons\": True, \"households\": True, \"legacy_format\": False}\n)\n</code></pre> <p>With descriptions:</p> <pre><code>coupler.declare_outputs(\n    \"skims\",\n    description={\"skims\": \"Zone-to-zone travel times in Zarr format\"}\n)\n</code></pre> Notes <p>Required status is \"sticky\": once an output is marked required, later declarations cannot downgrade it to optional.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.missing_declared_outputs","level":2,"title":"<code>missing_declared_outputs()</code>","text":"<p>Return required declared outputs that have not been set in the coupler.</p> <p>This checks only outputs declared via declare_outputs(), not schema keys. Use validate_all_schema_keys_set() to check schema-based validation.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of required declared outputs that are missing.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.require_outputs","level":2,"title":"<code>require_outputs(*names, required=True, warn_undocumented=False, description=None)</code>","text":"<p>Declare required outputs with optional undocumented key warnings.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.collect_by_keys","level":2,"title":"<code>collect_by_keys(artifacts, *keys, prefix='')</code>","text":"<p>Collect explicit artifacts into the coupler by key.</p> <p>This method selectively ingests artifacts from a mapping, storing them under optionally-prefixed keys. Useful when a step returns many outputs but you only want specific ones, or when you need to namespace outputs by scenario/year.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.describe_outputs","level":2,"title":"<code>describe_outputs()</code>","text":"<p>Return descriptions of declared outputs (for documentation/introspection).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"concepts/caching-and-hydration/","level":1,"title":"Caching and Artifact Hydration Patterns","text":"<p>Consist's core loop: declare inputs → compute signature → check cache → load or execute.</p> <p>Terminology</p> <p>Hydration recovers metadata and provenance about a prior run's outputs without copying file bytes. Materialization ensures file bytes exist on disk (copying them from their original location). A cache hit always hydrates; it only materializes if you request it via a specific hydration policy.</p> <p>This page covers patterns for making caching reliable, keeping pipelines portable, and choosing when artifacts should be hydrated or materialized.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#terms-used-precisely","level":2,"title":"Terms Used Precisely","text":"<p>This page separates provenance objects (Artifacts, Runs) from physical bytes (files on disk) and database-backed data (DuckDB tables).</p> <p>Cache Hit: Consist finds a prior run with the same signature and returns cached artifact metadata.</p> <p>Hydration: Recovering metadata and location information about a previous run's outputs without copying file bytes.</p> <p>Materialization: Copying artifact bytes to disk (or into DuckDB) so files exist in a new location.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#hydration-vs-materialization","level":2,"title":"Hydration vs Materialization","text":"<p>When Consist finds a cached result, it \"brings back\" information about that result—but what does that mean?</p> <p>Consider a electric grid simulation in <code>/scratch/simulation_2024/</code> that produces a 50GB results file. Six months later, you run analysis with the same inputs. Consist recognizes a cache hit.</p> <p>Artifact Hydration recovers information: the result came from run <code>abc123</code>, it's a Parquet file called <code>results.parquet</code>, it contains monthly precipitation data. Consist creates an <code>Artifact</code> object with metadata, URIs, and provenance.</p> <p>Materialization ensures bytes exist: copying the 50GB file from its original location into your new run directory.</p> <p>Hydration is fast and automatic. Materialization is optional and explicit.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#default-behavior","level":3,"title":"Default Behavior","text":"<p>Consist hydrates metadata only. Metadata recovery is instant (database lookup). Files stay in their original location. You pay for copying only when requested.</p> <p>This matters for parameter sweeps: 100 demand model variations do not require 100 copies of a 50GB input file. Downstream code loads from the original path or database fallback.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#when-to-materialize","level":3,"title":"When to Materialize","text":"<p>Materialize cached outputs (copy bytes to your current run) in three cases:</p> <ol> <li> <p>Extending a scenario in a new workspace: Cached results in <code>/workspace/2024</code>, continuing in <code>/workspace/2025</code>. Use <code>cache_hydration=\"inputs-missing\"</code> to copy input files.</p> </li> <li> <p>Preparing outputs for external tools: Tools expect local files. Use <code>cache_hydration=\"outputs-requested\"</code> with <code>materialize_cached_output_paths</code>. With <code>consist.run(...)</code> or <code>sc.run(...)</code>, use <code>output_paths={...}</code>.</p> </li> <li> <p>Ensuring local reproducibility: All results accessible without remote mounts. Use <code>cache_hydration=\"outputs-all\"</code> to copy all cached outputs.</p> </li> </ol>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#example-electric-grid-modeling-workflow","level":3,"title":"Example: Electric Grid Modeling Workflow","text":"<pre><code>from pathlib import Path\n\nwith tracker.start_run(  # (1)!\n    \"grid_dispatch_baseline\",\n    model=\"grid_sim\",\n    year=2024,\n    cache_mode=\"overwrite\"  # (2)!\n):\n    results = run_dispatch_model()  # (3)!\n    tracker.log_artifact(results, key=\"dispatch\")  # (4)!\n\nwith tracker.start_run(  # (5)!\n    \"reliability_analysis\",\n    model=\"analysis\",\n    inputs=[cached_dispatch_artifact],  # (6)!\n    cache_hydration=\"metadata\",  # (7)!\n):\n    print(f\"Result came from run {cached_dispatch_artifact.run_id}\")  # (8)!\n\n    df = consist.load_df(cached_dispatch_artifact)  # (9)!\n\nwith tracker.start_run(  # (10)!\n    \"export_for_reporting\",\n    model=\"export\",\n    cache_hydration=\"outputs-requested\",  # (11)!\n    materialize_cached_output_paths={\n        \"dispatch\": Path(\"./local_outputs/dispatch.parquet\")  # (12)!\n    }\n):\n    subprocess.run([\"report_generator\", \"annual_summary.py\"])  # (13)!\n</code></pre> <ol> <li>First run: simulate regional grid dispatch for 2024, takes 18 hours.</li> <li>Overwrite mode: Forces re-execution and overwrites any cached results. Use for baseline runs or when upstream data has changed outside Consist's tracking.</li> <li>Dispatch models produce large outputs—hourly resolution across 8,760 hours for all generators and transmission lines.</li> <li>Artifact logging: Consist records the file's hash, location, and lineage. This artifact can now be referenced as an input to downstream runs.</li> <li>Later: analysis that only needs metadata about the result.</li> <li>Input declaration: Tells Consist this run depends on the dispatch artifact. If the upstream run is invalidated, this run's cache is also invalidated.</li> <li>Metadata-only hydration: Consist loads the Artifact object (provenance, signatures, URIs) without copying the 40GB file. Default behavior—saves disk space.</li> <li>Even without file bytes, you can query provenance: which run produced this, when, with what config.</li> <li>Explicit loading: When you actually need the data, <code>load_df()</code> reads from the original location (or database if ingested). This is the only point where bytes move.</li> <li>Later: need to ensure files exist locally for regulatory reporting tool.</li> <li>Outputs-requested mode: Tells Consist to copy cached output files to disk. Use when external tools need real files, not just metadata.</li> <li>Materialization path: Specifies where Consist should copy the cached artifact. The external reporting tool will find the file here.</li> <li>Now the regulatory reporting tool can access real files at known paths—Consist handled the copying.</li> </ol>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#default-behavior-metadata-only-hydration","level":3,"title":"Default Behavior: Metadata-Only Hydration","text":"<p>The default <code>cache_hydration=\"metadata\"</code> performs no filesystem operations and no disk duplication. Use <code>consist.load_df()</code> or <code>consist.load()</code> when you need bytes.</p> <p>This default suits scientific workflows: large simulations benefit from avoiding disk copies, and provenance verification does not need bytes on disk.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#practical-decision-table","level":3,"title":"Practical Decision Table","text":"Your Need Use This Tradeoff Run parameter sweeps on cached simulation <code>metadata</code> (default) No disk copy; must load data via <code>consist.load_df()</code>/<code>consist.load()</code> Extend analysis in a new workspace <code>inputs-missing</code> Copies only missing input files; fast on cache hits Run external tool that needs local files <code>outputs-requested</code> Copy only what you ask for; must list outputs explicitly Ensure all cached data is accessible locally <code>outputs-all</code> Copies everything; uses disk space but guarantees file paths exist","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#signatures-and-cache-behavior","level":3,"title":"Signatures and Cache Behavior","text":"<p>Every run's signature is derived from code hash, config hash, and input hash. If Consist finds a completed run with the same signature and valid outputs, the new run is a cache hit and reuses prior outputs.</p> <code>cache_mode</code> Behavior <code>reuse</code> (default) Reuse matching completed runs <code>overwrite</code> Always execute (no cache lookup) <code>readonly</code> Read cache but avoid persisting changes","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#artifact-identity-vs-bytes","level":3,"title":"Artifact Identity vs. Bytes","text":"<p>Consist distinguishes provenance identity (where an artifact came from, via <code>artifact.run_id</code>) from physical bytes (on-disk file contents).</p> <p>For Consist-produced artifacts, inputs are hashed by linking to the producing run's signature (Merkle-style). For raw files (no <code>run_id</code>), Consist hashes file contents or metadata depending on configuration.</p> <p>To skip hashing when you know the content hash (e.g., after copying a file), pass <code>content_hash=</code> to <code>log_artifact</code>. Consist ignores mismatched overrides unless <code>force_hash_override=True</code>. Use <code>validate_content_hash=True</code> to verify the override against on-disk data.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#portability-and-path-resolution","level":3,"title":"Portability and Path Resolution","text":"<p>An artifact has a portable <code>artifact.container_uri</code> and a runtime-resolved <code>artifact.path</code>. Call <code>tracker.resolve_uri(uri)</code> to translate a portable URI into a host filesystem path using mounts.</p> <p>On cache hits, Consist attaches cached output <code>Artifact</code> objects to the active run context. Code consumes <code>Artifact</code> objects and resolves paths through the tracker, keeping pipelines portable across machines.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#pattern-1-step-caching-with-explicit-artifact-handoff","level":2,"title":"Pattern 1: Step Caching with Explicit Artifact Handoff","text":"<p>For multi-step workflows, each step logs outputs as artifacts and the next step declares those artifacts as inputs.</p> <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    with consist.scenario(\"my_scenario\") as scenario:\n        with scenario.trace(\"ingest\"):\n            consist.log_artifact(\"raw.csv\", key=\"raw\", direction=\"output\")\n\n        with scenario.trace(\"transform\", inputs=[\"raw\"]):  # (1)!\n            raw_art = scenario.coupler.require(\"raw\")  # (2)!\n            df = consist.load_df(raw_art, tracker=tracker)\n</code></pre> <ol> <li><code>inputs=[...]</code> declares step inputs by Coupler key without repeating.</li> <li><code>scenario.coupler.require(...)</code> in the <code>inputs=[...]</code> list.</li> </ol> <p>Use <code>scenario.coupler.require(\"raw\")</code> over <code>get(\"raw\")</code>—it fails loudly if a predecessor did not set the key. Pass upstream artifacts through <code>inputs=[...]</code> for correct caching and provenance.</p> <p>Step bodies execute on cache hits</p> <p>Consist does not skip Python blocks. On cache hits, <code>tracker.log_artifact(..., direction=\"output\")</code> returns the hydrated cached output when the <code>key</code> matches. If code produces a different output, Consist demotes the cache hit to an executing run.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#function-shaped-steps-skip-on-cache-hit","level":3,"title":"Function-Shaped Steps (Skip on Cache Hit)","text":"<p>Use <code>ScenarioContext.run(...)</code> to skip execution on cache hits. The <code>output_paths={...}</code> values are interpreted as relative paths (to <code>t.run_dir</code>), URI-like strings (resolved via mounts), or absolute paths.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#pattern-2-cached-runs-with-consistrun","level":2,"title":"Pattern 2: Cached Runs with <code>consist.run</code>","text":"<p>Use <code>consist.run(...)</code> with declared outputs for cache-aware function execution.</p> <pre><code>def clean(raw_file: Path):\n    ...\n    return {\"cleaned\": cleaned_df}\n\nimport consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean,\n        inputs={\"raw_file\": Path(\"raw.csv\")},\n        outputs=[\"cleaned\"],\n        load_inputs=True,\n    )\n</code></pre>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#pattern-3-resuming-after-failures","level":2,"title":"Pattern 3: Resuming After Failures","text":"<p>When early steps succeed (and cache) but a late step fails, re-running reuses upstream cached results and continues from the failure point.</p> <p>Requirements: upstream outputs are logged as artifacts, downstream steps declare those artifacts as inputs, and Consist only reuses completed runs. Re-running cache-hits earlier steps; the failed step re-executes because no valid completed run exists.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#pattern-4-hot-vs-cold-data","level":2,"title":"Pattern 4: Hot vs. Cold Data","text":"<p>See Data Materialization for ingestion patterns, schema validation, and database fallback behavior.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#what-happens-on-a-cache-hit","level":2,"title":"What Happens on a Cache Hit?","text":"","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#core-tracker-behavior","level":3,"title":"Core Tracker Behavior","text":"<p>On a cache hit, Consist:</p> <ul> <li>Finds a matching completed prior run with the same signature</li> <li>Skips filesystem checks for cached outputs (pass <code>validate_cached_outputs=\"eager\"</code> to require files exist or are ingested)</li> <li>Hydrates artifact objects into the current run context; paths resolve lazily via the active tracker</li> </ul> <p>On a cache hit, Consist does not:</p> <ul> <li>Copy files into a new run directory</li> <li>Recreate missing files from DuckDB automatically</li> <li>Guarantee that <code>artifact.path</code> exists on disk</li> </ul> <p>To access bytes: call <code>consist.load_df(...)</code> for DataFrames or <code>consist.load(...)</code> for Relations. Use <code>consist.load_relation(...)</code> as a context manager to ensure the DuckDB connection closes. Consist warns when active relation count exceeds <code>CONSIST_RELATION_WARN_THRESHOLD</code> (default 100).</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#cache-misses-with-cached-inputs","level":3,"title":"Cache Misses with Cached Inputs","text":"<p>If a run is not a cache hit but its inputs include artifacts from cached runs, those inputs are referenced by portable URIs (e.g., <code>./outputs/foo.parquet</code>). By default, URIs resolve to the current run directory, which may not contain bytes from the original run.</p> <p>To extend a scenario in a new <code>run_dir</code> without re-running cached steps, use the cache hydration policy:</p> <pre><code>with tracker.start_run(\n    \"next_step\",\n    model=\"step\",\n    inputs=[cached_artifact],\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n</code></pre> <p>With <code>cache_hydration=\"inputs-missing\"</code>, Consist detects missing input paths, locates the original run directory from provenance, and copies input bytes into the current <code>run_dir</code> before execution.</p> <p>If the original file is missing but the artifact was ingested, Consist reconstructs inputs from DuckDB for CSV and Parquet artifacts only. Other drivers raise a <code>ValueError</code> to avoid silent or lossy conversions.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#optional-cache-hit-materialization","level":3,"title":"Optional Cache-Hit Materialization","text":"<p>Consist supports opt-in materialization: copying bytes from a cached artifact's resolved path to a destination you provide. This is copy-only—it does not reconstruct files from DuckDB.</p> <p>Enable at run start:</p> <pre><code>with tracker.start_run(\n    \"r2\",\n    model=\"step\",\n    cache_hydration=\"outputs-all\",\n    materialize_cached_outputs_dir=Path(\"some/output/dir\"),\n):\n    ...\n</code></pre> <p>Materialize specific cached outputs by artifact key:</p> <pre><code>with tracker.start_run(\n    \"r2\",\n    model=\"step\",\n    cache_hydration=\"outputs-requested\",\n    materialize_cached_output_paths={\"features\": Path(\"outputs/features.csv\")},\n):\n    ...\n</code></pre> <p>Equivalent with <code>consist.run(...)</code>:</p> <pre><code>result = consist.run(\n    fn=step,\n    cache_hydration=\"outputs-requested\",\n    output_paths={\"features\": Path(\"outputs/features.csv\")},\n)\n</code></pre> <p>Default is <code>cache_hydration=\"metadata\"</code> (artifact hydration only). For <code>consist.run(...)</code> or <code>sc.run(...)</code>, use <code>output_paths={...}</code> with <code>cache_hydration=\"outputs-requested\"</code>.</p> Policy Requires Rejects <code>outputs-requested</code> <code>materialize_cached_output_paths</code> or <code>output_paths</code> <code>materialize_cached_outputs_dir</code> <code>outputs-all</code> <code>materialize_cached_outputs_dir</code> <code>materialize_cached_output_paths</code> <code>metadata</code> / <code>inputs-missing</code> — Both materialization args <p><code>outputs-requested</code> logs a warning for missing keys. <code>outputs-all</code> raises <code>FileNotFoundError</code> if any cached output source file is missing.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#containers-integration","level":3,"title":"Containers Integration","text":"<p>The containers API defaults to copying cached outputs to requested host output paths so downstream tooling sees real files. This uses the same copy-based materialization helper as core runs, but containers default to \"requested outputs\" because the caller explicitly provides host output paths.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#hydration-policy-summary","level":2,"title":"Hydration Policy Summary","text":"Workflow <code>cache_hydration</code> When Bytes Copy Notes Fast cache hits <code>metadata</code> Never Default; paths may not exist in new run dirs Extend scenario in new <code>run_dir</code> <code>inputs-missing</code> Cache misses, missing inputs only Ensures executing steps read cached inputs Materialize specific outputs <code>outputs-requested</code> Cache hits, requested outputs Requires <code>materialize_cached_output_paths</code> or <code>output_paths</code> All outputs local <code>outputs-all</code> Cache hits, all outputs Requires <code>materialize_cached_outputs_dir</code> <p>Use <code>metadata</code> (the default) unless downstream tools require files on disk.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#see-also","level":2,"title":"See Also","text":"<ul> <li>Data Materialization — When to ingest data and use hybrid views</li> <li>Config Management — How configuration affects cache invalidation</li> </ul>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#practical-guidance","level":2,"title":"Practical Guidance","text":"","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#make-caching-predictable","level":3,"title":"Make Caching Predictable","text":"<p>Declare all dependencies as inputs (<code>inputs=[...]</code> or task parameters). Keep configs deterministic—avoid embedding timestamps or randomness unless intended. Prefer stable code identity in tests (the Consist test suite patches the code hash for determinism).</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#decide-what-to-ingest","level":3,"title":"Decide What to Ingest","text":"<p>Ingest for queryable DuckDB tables, schema profiling, and optional recovery if files are deleted. Keep as cold files when you do not want to duplicate storage or artifacts are large and already in an external system.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#safe-deletion","level":3,"title":"Safe Deletion","text":"<p>Delete artifacts when you are confident they will not be loaded again, or when they were ingested and you allow DB recovery (via <code>db_fallback=\"always\"</code> or by declaring them as inputs). For aggressive GC while keeping provenance usable, use ingestion + intentional DB fallback.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#cache-hit-performance-and-diagnostics","level":2,"title":"Cache-Hit Performance and Diagnostics","text":"<p>Consist's cache-hit path minimizes filesystem work by default: <code>validate_cached_outputs=\"lazy\"</code> skips per-output existence checks, and <code>cache_hydration=\"metadata\"</code> avoids copying bytes.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#configuration-summary","level":3,"title":"Configuration Summary","text":"Parameter Options <code>cache_mode</code> <code>reuse</code> (default), <code>overwrite</code>, <code>readonly</code> <code>validate_cached_outputs</code> <code>lazy</code> (default), <code>eager</code> <code>cache_hydration</code> <code>metadata</code> (default), <code>inputs-missing</code>, <code>outputs-requested</code>, <code>outputs-all</code> <code>step_cache_hydration</code> Scenario-level default for all steps <code>materialize_cached_output_paths</code> Required for <code>outputs-requested</code> <code>materialize_cached_outputs_dir</code> Required for <code>outputs-all</code> <p>For large iterative workflows, cache-hit time is spent fetching cached outputs and hashing raw-file inputs. Enable diagnostics:</p> <ul> <li><code>CONSIST_CACHE_TIMING=1</code>: logs timing for signature prefetch, input hashing, cache lookup, validation, hydration</li> <li><code>CONSIST_CACHE_DEBUG=1</code>: logs cache hit/miss details and signatures</li> </ul> <pre><code>[cache_timing] signature_prefetch=2.1ms input_hashing=145.8ms cache_lookup=3.4ms validate=0.6ms hydration=0.2ms\n</code></pre> <p>If input hashing dominates, ingest large tabular inputs or pass Consist-produced artifacts instead of raw files.</p> <pre><code>[cache_debug] hit=True signature=7e9c1c inputs=3 outputs=2 hydration=metadata\n</code></pre> <p>If downstream tools need files, select an outputs hydration mode. Use <code>validate_cached_outputs=\"eager\"</code> for strict output existence checks.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#pattern-5-shared-input-bundles","level":2,"title":"Pattern 5: Shared Input Bundles","text":"<p>Package all required inputs in a standalone DuckDB file. Create a \"bundle\" run that logs inputs as outputs and ingests them. Share the DB file to reproduce inputs without shipping original raw files.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#bundle-creation-producer","level":3,"title":"Bundle Creation (Producer)","text":"<pre><code>tracker = Tracker(run_dir=\"bundle_build\", db_path=\"inputs.duckdb\")\n\nwith tracker.start_run(\"inputs_bundle_v1\", model=\"input_bundle\", cache_mode=\"overwrite\"):\n    households = tracker.log_artifact(\"households.csv\", key=\"households\", direction=\"output\")\n    persons = tracker.log_artifact(\"persons.csv\", key=\"persons\", direction=\"output\")\n    tracker.ingest(households)\n    tracker.ingest(persons)\n</code></pre> <p>Run the bundle creation code once. Share <code>inputs.duckdb</code> with collaborators—they do not need the original CSV files.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#bundle-consumption-consumer","level":3,"title":"Bundle Consumption (Consumer)","text":"<pre><code>tracker = Tracker(run_dir=\"runs\", db_path=\"inputs.duckdb\")\n\nbundle_outputs = tracker.load_input_bundle(\"inputs_bundle_v1\")\ninputs = list(bundle_outputs.values())\n\nwith tracker.start_run(\n    \"simulate\",\n    model=\"simulate\",\n    inputs=inputs,\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n</code></pre> <p>The bundle is identified by <code>run_id</code> only—no hard-coded hashes. With <code>cache_hydration=\"inputs-missing\"</code>, Consist reconstructs CSV/Parquet inputs from the shared DuckDB file if original bytes are missing.</p>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/caching-and-hydration/#multi-model-workflows","level":3,"title":"Multi-Model Workflows","text":"<p>Bundles contain inputs for multiple downstream steps. Each run selects the inputs it needs:</p> <pre><code>bundle_outputs = tracker.load_input_bundle(\"inputs_bundle_v1\")\n\nwith tracker.start_run(\n    \"model_a\",\n    model=\"model_a\",\n    inputs=[bundle_outputs[\"input_a\"]],\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n\nwith tracker.start_run(\n    \"model_b\",\n    model=\"model_b\",\n    inputs=[bundle_outputs[\"input_b\"]],\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n</code></pre>","path":["Concepts","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"concepts/config-management/","level":1,"title":"Config Management: Caching vs. Filtering","text":"<p>When configuring a run, decide whether each parameter should trigger re-execution on change or be searchable for later filtering—or both.</p>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#decision-tree","level":2,"title":"Decision Tree","text":"<ul> <li>Should changing this parameter re-run my model? → <code>config</code></li> <li>Do I need to search/filter by this value later? → <code>facet</code></li> <li>Both? → Use both. A value can appear in config (cache invalidation) and facet (queryable).</li> </ul>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#reference-table","level":2,"title":"Reference Table","text":"Parameter Affects Cache? Queryable? Use For <code>config=</code> Yes No (by default) Parameters that change behavior <code>facet=</code> No Yes (indexed) Metadata for filtering/grouping <code>facet_from=</code> — Yes Extract keys from config for queries","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#the-distinction","level":2,"title":"The Distinction","text":"<p>Config affects reproducibility: Consist hashes config values into the run signature. If config changes, the signature changes, and cached results do not apply—the run re-executes.</p> <p>Facet enables filtering: Facets are indexed in DuckDB without affecting caching. Query \"all runs where year=2030 and scenario='baseline'\" without storing entire configs.</p>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#practical-example","level":2,"title":"Practical Example","text":"<p>Store a 100KB ActivitySim config file as <code>config=...</code> (hashes into signature; changes trigger re-runs). Extract small, queryable pieces as facets:</p> <pre><code>consist.run(\n    fn=my_model,\n    config={\"full_config_path\": \"activitysim_config.yaml\"},  # (1)!\n    facet={\"year\": 2030, \"mode_choice_coefficient\": 0.5},   # (2)!\n    inputs={...},\n    outputs=[...]\n)\n</code></pre> <ol> <li>Hashed; changes invalidate cache.</li> <li>Queryable; doesn't affect cache.</li> </ol> <p>This approach enables:</p> <ol> <li>Queries like <code>mode_choice_coefficient &gt; 0.4</code> without bloating the database with raw config files</li> <li>Cache misses when the full config file changes (reproducibility)</li> <li>Post-hoc filtering by year or coefficient</li> </ol> <p>Example: Running demand models for 10 years (2020–2050) × 3 scenarios (baseline, transit-friendly, congestion pricing). Set <code>facet={\"year\": 2030, \"scenario\": \"transit-friendly\"}</code> for each run. Query <code>facet_year=2040</code> to retrieve all 2040 runs instantly—no manual search through 500 directories.</p>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#config-hashing","level":2,"title":"Config Hashing","text":"<p>Consist uses canonical hashing: converting dictionaries (and YAML/JSON) into a deterministic fingerprint regardless of field order or number formatting. Both <code>{\"a\": 1, \"b\": 2}</code> and <code>{\"b\": 2, \"a\": 1}</code> produce the same hash, so cache remains stable when config dicts are built in different orders.</p>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#config-adapters","level":2,"title":"Config Adapters","text":"<p>Building an integration for ActivitySim, MATSim, or similar tools requires a config adapter—a function that transforms the tool's native config into a Consist-compatible format.</p>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#when-to-use-each-adapter","level":3,"title":"When to Use Each Adapter","text":"Adapter Purpose Required? <code>canonicalize_config()</code> Transform native config into a deterministic dict for hashing. Exclude irrelevant fields (logging, caches). Yes <code>prepare_config()</code> Merge scenario parameters into native config for execution. Apply templating or defaults. No <p>If you need only one, implement <code>canonicalize_config()</code>—it is the minimum requirement for caching.</p>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#canonicalize_confignative_config-dict","level":3,"title":"<code>canonicalize_config(native_config) -&gt; dict</code>","text":"<p>Returns a stable representation for cache signatures:</p> <pre><code>def canonicalize_config(activitysim_config_dict):\n    return {\n        \"year\": activitysim_config_dict[\"year\"],\n        \"scenario\": activitysim_config_dict[\"scenario\"],\n        \"mode_choice_coefficients\": activitysim_config_dict[\"coefficients\"],  # (1)!\n    }\n</code></pre> <ol> <li>Return only the fields that affect model behavior.</li> </ol>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#prepare_confignative_config-scenario_id-year-dict","level":3,"title":"<code>prepare_config(native_config, scenario_id, year) -&gt; dict</code>","text":"<p>Returns the full config the tool needs at runtime:</p> <pre><code>def prepare_config(base_config, scenario_id, year):\n    config = base_config.copy()\n    config[\"scenario_id\"] = scenario_id\n    config[\"year\"] = year\n    return config\n</code></pre>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/config-management/#see-also","level":2,"title":"See Also","text":"<ul> <li>Caching &amp; Hydration — How config changes affect cache behavior</li> <li>Config Adapters — Detailed integration guide for adapters</li> </ul>","path":["Concepts","Config Management: Caching vs. Filtering"],"tags":[]},{"location":"concepts/data-materialization/","level":1,"title":"Data Virtualization &amp; Materialization Strategy","text":"<p>This guide explains when to keep data on disk (virtualize) versus ingesting it into DuckDB (materialize).</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#why-the-distinction-matters","level":2,"title":"Why the Distinction Matters","text":"<p>Simulation workflows produce many artifacts across runs. Comparing results requires querying across runs—but ingesting every output doubles your database size. The cold/hot distinction lets you choose: keep storage efficient for artifacts you rarely query, enable SQL analysis for artifacts you compare frequently.</p> <p>The tradeoff: ingested data is queryable and recoverable; cold data requires keeping original files but adds no database overhead.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#two-storage-modes","level":2,"title":"Two Storage Modes","text":"<p>Cold Data: Files remain on disk. Provenance metadata is tracked but data is not queryable via SQL until ingested. Load on demand with <code>consist.load_df(artifact)</code>. Minimal database size.</p> <p>Hot Data: Ingested into DuckDB. Queryable via SQL, indexed alongside provenance, and recoverable if original files are deleted. Trades disk space for query performance.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#when-to-ingest","level":2,"title":"When to Ingest","text":"<p>Ingest when you need:</p> <ul> <li>SQL-native analytics over many runs</li> <li>Schema tracking and exportable SQLModel stubs</li> <li>Recovery of tabular data if files are missing</li> <li>Cross-run comparisons via unified SQL views</li> </ul> <p>Ingest key tabular outputs you compare across runs:</p> <ul> <li>Transportation: Trip tables, skim matrices, mode choice summaries</li> <li>Electric grid: Aggregated metrics (monthly averages, distribution-level summaries)</li> <li>Urban planning: Parcel or zoning summaries, development capacity estimates</li> </ul> <p>Ingestion is appropriate for outputs where (1) you need SQL queries across runs, and (2) the storage cost is acceptable—ingestion approximately doubles storage for that artifact. For a 500MB output compared across 100 runs, ingestion adds ~50GB to the database.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#when-to-stay-cold","level":2,"title":"When to Stay Cold","text":"<p>Keep data as cold files when:</p> <ul> <li>Storage duplication is unacceptable (multi-GB rasters, video files, binary blobs)</li> <li>Data already lives in an external system</li> <li>No cross-run SQL analysis is needed</li> </ul> <p>Cold data lives only on disk; provenance metadata is tracked but data is not queryable via SQL or included in hybrid views until ingested.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#ingesting-artifacts","level":2,"title":"Ingesting Artifacts","text":"<p>Install the optional DLT dependency:</p> <pre><code>pip install \"consist[ingest]\"\n</code></pre> <p>Basic usage:</p> <pre><code>from consist import Tracker\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\nwith tracker.start_run(\"ingest_example\", model=\"demo\"):\n    artifact = tracker.log_artifact(\"data/people.csv\", key=\"people\", driver=\"csv\")\n    tracker.ingest(artifact)\n</code></pre> <p>Ingest explicit data payloads (dicts, DataFrames, generators):</p> <pre><code>with tracker.start_run(\"ingest_inline\", model=\"demo\"):\n    artifact = tracker.log_artifact(\"inputs/people.json\", key=\"people\")\n    tracker.ingest(artifact, data=[{\"id\": 1, \"name\": \"A\"}])\n</code></pre>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#strict-schema-ingestion","level":2,"title":"Strict Schema Ingestion","text":"<p>Provide a SQLModel schema to validate data:</p> <pre><code>from sqlmodel import SQLModel, Field\n\nclass Person(SQLModel, table=True):\n    __tablename__ = \"people\"\n    id: int = Field(primary_key=True)\n    name: str\n\nwith tracker.start_run(\"strict_ingest\", model=\"demo\"):\n    artifact = tracker.log_artifact(\"data/people.csv\", key=\"people\", schema=Person)\n    tracker.ingest(artifact, schema=Person)\n</code></pre> <p>Strict mode rejects incompatible data rather than silently coercing it.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#provenance-columns","level":2,"title":"Provenance Columns","text":"<p>Ingested tables include system columns:</p> Column Purpose <code>consist_run_id</code> Run that created this row <code>consist_artifact_id</code> Source artifact <code>consist_year</code>, <code>consist_iteration</code> Scenario dimensions (optional) <p>These enable joins across runs and filtering by scenario or year in hybrid views.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#hybrid-views","level":2,"title":"Hybrid Views","text":"<p>Hybrid views are SQL views that union hot data (ingested into DuckDB) with cold data (read from files on disk). Query both as one logical table without requiring all data in the database.</p> <p>Register a SQLModel schema to activate a view:</p> <pre><code>from sqlmodel import SQLModel, Field\n\nclass Person(SQLModel, table=True):\n    __tablename__ = \"people\"\n    person_id: int = Field(primary_key=True)\n    age: int\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person],\n)\n\nVPerson = tracker.views.Person\n</code></pre> <p>Once registered, you can query across all runs (both ingested and cold data):</p> <pre><code>from sqlmodel import select, func\nimport consist\n\nquery = (\n    select(VPerson.consist_year, func.avg(VPerson.age).label(\"avg_age\"))\n    .group_by(VPerson.consist_year)\n)\nrows = consist.run_query(query, tracker=tracker)\n</code></pre>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#loading-behavior-database-fallback","level":2,"title":"Loading Behavior &amp; Database Fallback","text":"<p><code>consist.load(...)</code> reads from the filesystem. If files are missing and the artifact was ingested, database fallback recovers data:</p> Policy Behavior <code>inputs-only</code> (default) Recover inside active run for declared inputs only <code>always</code> Recover whenever artifact is ingested <code>never</code> Disable recovery <p>Example:</p> <pre><code>df = consist.load_df(artifact, tracker=tracker, db_fallback=\"always\")\n</code></pre>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#loader-kwargs","level":2,"title":"Loader Kwargs","text":"<p><code>consist.load(...)</code> validates loader kwargs and raises <code>ValueError</code> for unsupported options.</p> Driver Supported kwargs <code>parquet</code> <code>columns</code> <code>csv</code> <code>columns</code>, <code>delimiter</code>, <code>header</code> (alias: <code>sep</code>) <code>json</code> <code>orient</code>, <code>dtype</code>, <code>convert_axes</code>, <code>convert_dates</code>, <code>precise_float</code>, <code>date_unit</code>, <code>encoding</code>, <code>lines</code>, <code>compression</code>, <code>typ</code> <code>h5_table</code> <code>columns</code>, <code>where</code>, <code>start</code>, <code>stop</code> <p>Use <code>consist.load_relation(...)</code> to manage DuckDB Relation lifecycle explicitly. Use <code>consist.load_df(...)</code> for DataFrame with automatic cleanup.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#schema-export-workflow","level":2,"title":"Schema Export Workflow","text":"<p>For curated schemas with explicit PK/FK constraints: ingest tabular data, run schema export to generate a SQLModel stub, then edit and register the stub for views. See schema-export.md for details.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#hdf5-native-support-roadmap","level":2,"title":"HDF5 Native Support Roadmap","text":"<p>Currently <code>h5_table</code> uses a staging bridge (<code>pandas.read_hdf(...)</code> + <code>conn.from_df(...)</code>). Native HDF5 support will replace this with DuckDB's HDF5 extension for direct queries.</p> <p>Remaining work: switch <code>h5_table</code> to <code>hdf5_read(...)</code> once stable, validate extension version pinning, handle PyTables/object dtype edge cases, update schema capture for native relation path, add performance testing.</p> <p>Benefits: zero-copy streaming reads, lower memory usage, consistent SQL workflows across Parquet/CSV/HDF5.</p>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#decision-summary","level":2,"title":"Decision Summary","text":"Need Approach When Query results across runs in SQL Ingest Cross-run comparisons, storage cost acceptable Minimize database size Stay cold Large files, no cross-run SQL analysis Query ingested and raw files together Hybrid views Mixed—some runs ingested, some not Recover data if files deleted Ingest Need resilience to file deletion","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/data-materialization/#see-also","level":2,"title":"See Also","text":"<ul> <li>Caching &amp; Hydration — How materialization interacts with caching</li> <li>DLT Loader Guide — Detailed ingestion patterns using DLT</li> <li>Schema Export — Generate and manage schemas for ingested data</li> </ul>","path":["Concepts","Data Virtualization &amp; Materialization Strategy"],"tags":[]},{"location":"concepts/overview/","level":1,"title":"Core Concepts Overview","text":"<p>This section establishes the mental model for Consist before covering API details.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#core-abstractions","level":2,"title":"Core Abstractions","text":"<p>Artifact: A file with provenance metadata—its path, format, content hash (SHA256), producing run, and ingestion status.</p> <p>Run: A single execution with tracked inputs, configuration, outputs, status, and timing. Each run has a signature computed from code, config, and inputs that enables cache reuse.</p> <p>Scenario: A parent run grouping related child runs for multi-variant studies or iterative workflows.</p> <p>Coupler: A helper that passes artifacts between steps in a scenario, linking lineage automatically.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#how-caching-works","level":2,"title":"How Caching Works","text":"<p>Consist computes a signature from code version, config, and input artifact hashes:</p> <pre><code>graph LR\n    Code[Code Version] --&gt; Hash[SHA256 Signature]\n    Config[Configuration] --&gt; Hash\n    Inputs[Input Artifacts] --&gt; Hash\n    Hash --&gt; Lookup{Cache Lookup}\n    Lookup --&gt;|Hit| Return[Return Cached Outputs]\n    Lookup --&gt;|Miss| Execute[Execute &amp; Record New Run]</code></pre> <p>Same signature → return cached outputs. Different signature → execute and record new lineage.</p> <p>On cache hits, Consist returns output artifact metadata without copying files. Load or hydrate outputs when you need bytes.</p> <p>Example: In a parameter sweep testing 20 demand elasticity values, the first run executes preprocessing and the demand model. Runs 2–20 cache-hit on preprocessing (same inputs, same code) but cache-miss on the demand model (different elasticity). Consist skips 19 preprocessing executions.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#provenance-lineage","level":2,"title":"Provenance &amp; Lineage","text":"<p>Provenance: The complete history of a result—code version, configuration, input data, and compute environment. Consist records provenance automatically for every run.</p> <p>Lineage: The dependency chain showing which run created an artifact, which inputs that run consumed, and which runs produced those inputs.</p> <pre><code>graph TD\n    Raw[Raw Data] --&gt; Step1[Run: Clean]\n    Config1[Threshold=0.5] --&gt; Step1\n    Step1 --&gt; Art1[Artifact: Cleaned]\n    Art1 --&gt; Step2[Run: Analyze]\n    Config2[GroupByKey=category] --&gt; Step2\n    Step2 --&gt; Art2[Artifact: Summary]</code></pre> <p>Provenance answers three questions: Can I re-run this exactly? (reproducibility), Which config produced this figure? (accountability), and Why did this change? (debugging).</p> <p>Example: You published a land-use forecast. A reviewer asks which scenario produced Figure 3. Run <code>consist show &lt;run_id&gt;</code> to see the code version (commit SHA), config parameters, input parcel data, and execution timestamp.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#key-terms-dependency-ordered","level":2,"title":"Key Terms (Dependency-Ordered)","text":"","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#identity-caching-foundation","level":3,"title":"Identity &amp; Caching Foundation","text":"<p>Signature: A fingerprint of a run computed by hashing code version, configuration, and input data.</p> <p>Canonical Hashing: Converting configuration data into a deterministic fingerprint regardless of field order or number formatting—<code>{\"a\": 1, \"b\": 2}</code> and <code>{\"b\": 2, \"a\": 1}</code> produce the same hash.</p> <p>Merkle DAG: A graph structure where each step's inputs link to previous steps' outputs, creating an immutable lineage record that enables cache validation and data recovery.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#configuration-querying","level":3,"title":"Configuration &amp; Querying","text":"<p>Config: Parameters that affect computation and are included in the cache signature; changing config triggers re-execution.</p> <p>Identity Config: The subset of configuration parameters that affect a run's cache signature.</p> <p>Facet: A queryable subset of configuration (e.g., <code>{\"year\": 2030, \"scenario\": \"baseline\"}</code>) indexed in DuckDB for filtering runs without affecting caching.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#cache-behavior","level":3,"title":"Cache Behavior","text":"<p>Cache Hit: Consist finds a previous run with identical signature and returns cached artifact metadata.</p> <p>Cache Miss: No matching cached result exists; the function executes and outputs are recorded as new artifacts.</p> <p>Hydration: Recovering metadata and location information about a previous run's output without copying file bytes. Hydration ≠ copying files.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#data-storage-recovery","level":3,"title":"Data Storage &amp; Recovery","text":"<p>Ingestion: Loading artifact data into DuckDB for SQL-native analysis.</p> <p>Materialization: Copying artifact bytes into DuckDB so data is recoverable if original files are deleted.</p> <p>Ghost Mode: Recovering artifacts from DuckDB when physical files no longer exist.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#data-virtualization","level":3,"title":"Data Virtualization","text":"<p>Virtualization: Querying multiple artifacts as a single table without loading all data into memory.</p> <p>Hybrid View: A SQL view combining hot data (ingested into DuckDB) with cold data (read from files on-the-fly).</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#data-types","level":3,"title":"Data Types","text":"<p>Cold Data: File-based data on disk; provenance is tracked but data is not queryable via SQL until ingested.</p> <p>Hot Data: Data ingested into DuckDB for SQL queries, schema tracking, and optional recovery.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#integrations-extensions","level":3,"title":"Integrations &amp; Extensions","text":"<p>DLT (Data Load Tool): A Python library for loading data into warehouses; Consist uses DLT to ingest artifacts with provenance columns.</p> <p>Coupler: A pattern for multi-step workflows that passes artifacts between steps and links lineage through scenario trees.</p> <p>Trace: The execution path through a multi-step workflow showing which runs executed, which cache-hit, and which artifacts were passed.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#how-inputs-and-outputs-are-treated","level":2,"title":"How Inputs and Outputs Are Treated","text":"<p>Inputs are files or values that influence computation. File inputs are hashed by content or metadata depending on the hashing strategy (<code>full</code> vs <code>fast</code>).</p> <p>Outputs are named artifacts declared via <code>consist.run(...)</code> or <code>tracker.run(...)</code>. Consist stores their paths and provenance metadata for lookup and querying.</p> <p>Write outputs under <code>tracker.run_dir</code> or a mounted <code>outputs://</code> root. This keeps artifact paths relative and portable across machines.</p>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#input-mappings-and-auto-loading","level":3,"title":"Input mappings and auto-loading","text":"<p>Inputs can be passed as a list (hash-only) or a mapping (hash + parameter injection). When using a mapping, Consist matches input keys to function parameters and auto-loads artifacts by default. To pass raw paths instead, set <code>load_inputs=False</code> and provide paths via <code>runtime_kwargs</code>.</p> <p>Concrete example:</p> <pre><code>import pandas as pd\n\ndef summarize_trips(trips_df):\n    return trips_df[\"distance_miles\"].mean()\n\nresult = consist.run(  # (1)!\n    fn=summarize_trips,\n    inputs={\"trips_df\": trips_artifact},\n)\n\ndef summarize_trips_from_path(trips_path: str):\n    df = pd.read_parquet(trips_path)\n    return df[\"distance_miles\"].mean()\n\nresult = consist.run(  # (2)!\n    fn=summarize_trips_from_path,\n    inputs={\"trips_path\": trips_artifact},\n    load_inputs=False,\n    runtime_kwargs={\"trips_path\": trips_artifact.path},\n)\n</code></pre> <ol> <li>Auto-loading: mapping keys match function params, so Consist loads the DataFrame.</li> <li>No auto-loading: you get the raw path and load it yourself.</li> </ol>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"concepts/overview/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Config Management — Understand the config vs. facet distinction and when to use each</li> <li>Caching &amp; Hydration — Caching patterns and data recovery strategies</li> <li>Data Materialization — When to ingest data and use hybrid views</li> </ul>","path":["Concepts","Core Concepts Overview"],"tags":[]},{"location":"getting-started/first-workflow/","level":1,"title":"First Workflow","text":"<p>This tutorial builds a two-step data pipeline demonstrating artifact passing, cache invalidation, and lineage tracking. Each step is a separate cached run; changing config or code in one step invalidates only that step and its dependents.</p>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/first-workflow/#create-input-data","level":2,"title":"Create Input Data","text":"<p>Save this as <code>setup_data.py</code> and run it once:</p> <pre><code>from pathlib import Path\nimport pandas as pd\n\nPath(\"./data\").mkdir(exist_ok=True)\ndf = pd.DataFrame({\"id\": [1, 2, 3, 4, 5], \"value\": [10, 20, 30, 40, 50]})\ndf.to_csv(\"./data/raw.csv\", index=False)\n</code></pre> <pre><code>python setup_data.py\n</code></pre>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/first-workflow/#define-the-pipeline","level":2,"title":"Define the Pipeline","text":"<p>Save the following as <code>workflow.py</code>:</p> <pre><code>from pathlib import Path\nimport pandas as pd\nimport consist\nfrom consist import Tracker\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\n\n@tracker.define_step(outputs=[\"cleaned\"])  # (1)!\ndef clean_data(raw_path: Path, threshold: float) -&gt; Path:\n    \"\"\"Remove rows below threshold and write cleaned output.\"\"\"\n    df = pd.read_csv(raw_path)\n    df_clean = df[df[\"value\"] &gt;= threshold]\n\n    out_path = tracker.run_dir / \"cleaned.parquet\"  # (2)!\n    df_clean.to_parquet(out_path, index=False)\n    return out_path  # (3)!\n\n\n@tracker.define_step(outputs=[\"summary\"])\ndef summarize(cleaned_artifact) -&gt; Path:  # (4)!\n    \"\"\"Compute summary statistics from cleaned data.\"\"\"\n    df = pd.read_parquet(cleaned_artifact.path)\n    summary = {\"mean\": df[\"value\"].mean(), \"count\": len(df)}\n\n    out_path = tracker.run_dir / \"summary.json\"\n    pd.Series(summary).to_json(out_path)\n    return out_path\n\n\n# --- Execute the pipeline ---\n\nwith consist.use_tracker(tracker):\n    # Step 1: Clean\n    clean_result = tracker.run(\n        fn=clean_data,\n        name=\"clean\",\n        config={\"threshold\": 15},  # (5)!\n        inputs=[Path(\"./data/raw.csv\")],  # (6)!\n        runtime_kwargs={\"raw_path\": Path(\"./data/raw.csv\"), \"threshold\": 15},\n    )\n    print(f\"Clean: {clean_result.run.status}\")\n\n    # Step 2: Summarize (consumes output from Step 1)\n    cleaned_artifact = clean_result.outputs[\"cleaned\"]  # (7)!\n    summary_result = tracker.run(\n        fn=summarize,\n        name=\"summarize\",\n        inputs={\"cleaned_artifact\": cleaned_artifact},  # (8)!\n    )\n    print(f\"Summarize: {summary_result.run.status}\")\n\n    # Load final result\n    final = pd.read_json(summary_result.outputs[\"summary\"].path, typ=\"series\")\n    print(f\"Result: {final.to_dict()}\")\n</code></pre> <ol> <li><code>@tracker.define_step</code> declares which output keys this function produces. Consist creates artifacts for each key.</li> <li>Write outputs under <code>tracker.run_dir</code>—Consist manages this directory per run.</li> <li>Return the output path. Consist registers it as the <code>cleaned</code> artifact.</li> <li>Artifact parameters are auto-loaded when input keys match parameter names.</li> <li><code>config</code> is hashed into the run's signature. Changing <code>threshold</code> causes a cache miss.</li> <li><code>inputs</code> lists files or artifacts whose content hashes are included in the signature.</li> <li>Access outputs by key from the result object. Each output is a full <code>Artifact</code> with path, hash, and metadata.</li> <li>Passing an artifact as input creates lineage: the summarize run depends on the clean run.</li> </ol>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/first-workflow/#run-and-observe-caching","level":2,"title":"Run and Observe Caching","text":"<p>Execute the pipeline:</p> <pre><code>python workflow.py\n</code></pre> <p>Output:</p> <pre><code>Clean: completed\nSummarize: completed\nResult: {'mean': 35.0, 'count': 4}\n</code></pre> <p>Run again without changes:</p> <pre><code>python workflow.py\n</code></pre> <p>Both steps return cached results instantly. The function bodies do not execute.</p>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/first-workflow/#invalidate-with-config-change","level":2,"title":"Invalidate with Config Change","text":"<p>Edit <code>workflow.py</code> to change the threshold:</p> <pre><code>config={\"threshold\": 25},  # Changed from 15\nruntime_kwargs={\"raw_path\": Path(\"./data/raw.csv\"), \"threshold\": 25},\n</code></pre> <p>Run again:</p> <pre><code>python workflow.py\n</code></pre> <p>Consist detects the config change and re-executes <code>clean_data</code>. Because its output artifact changes, <code>summarize</code> also re-executes—its input hash differs from the cached version.</p> <pre><code>Clean: completed\nSummarize: completed\nResult: {'mean': 40.0, 'count': 3}\n</code></pre> <p>This is the Merkle DAG at work: changes propagate downstream automatically.</p>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/first-workflow/#view-lineage","level":2,"title":"View Lineage","text":"<p>Query the provenance database:</p> <pre><code>python -m consist.cli runs --db-path ./runs/provenance.duckdb --limit 10\n</code></pre> <p>Each run shows its inputs, outputs, config hash, and status. Artifacts link to their producing runs, forming a complete lineage graph.</p>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/first-workflow/#summary","level":2,"title":"Summary","text":"<p>This workflow demonstrated:</p> <ul> <li>Step definition with <code>@tracker.define_step</code> declaring outputs</li> <li>Artifact passing where one step's output becomes another's input</li> <li>Cache invalidation triggered by config changes</li> <li>Lineage tracking linking runs through their artifacts</li> </ul>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/first-workflow/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Core Concepts — Mental model for runs, artifacts, and signatures</li> <li>Config Management — Control which parameters affect caching</li> <li>Caching &amp; Hydration — Advanced cache strategies and data recovery</li> <li>Data Materialization — Ingest artifacts for SQL analysis</li> </ul>","path":["Getting Started","First Workflow"],"tags":[]},{"location":"getting-started/installation/","level":1,"title":"Installation","text":"","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>Python 3.11 or newer</li> <li>Git (for source installation and code version tracking)</li> </ul>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#install-from-pypi","level":2,"title":"Install from PyPI","text":"<pre><code>pip install consist\n</code></pre>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#optional-extras","level":3,"title":"Optional Extras","text":"<p>Install with DLT for data ingestion into DuckDB:</p> <pre><code>pip install \"consist[ingest]\"\n</code></pre> <p>Install with example notebook dependencies:</p> <pre><code>pip install \"consist[examples]\"\n</code></pre>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#install-from-source","level":2,"title":"Install from Source","text":"<p>Clone the repository and install in editable mode:</p> <pre><code>git clone https://github.com/LBNL-UCB-STI/consist.git\ncd consist\npip install -e .\n</code></pre> <p>For development with ingestion support:</p> <pre><code>pip install -e \".[ingest]\"\n</code></pre>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#verify-installation","level":2,"title":"Verify Installation","text":"<p>Confirm Consist is installed correctly:</p> <pre><code>python -c \"import consist; print(consist.__file__)\"\n</code></pre> <p>This prints the path to the installed package.</p>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#next-steps","level":2,"title":"Next Steps","text":"<p>Proceed to the Quickstart to run your first cached workflow.</p>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/quickstart/","level":1,"title":"Quickstart","text":"<p>This example demonstrates Consist's core capability: caching a function so that repeated calls with the same inputs skip re-execution and return stored results.</p>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#create-a-cached-run","level":2,"title":"Create a Cached Run","text":"<p>Save the following as <code>quickstart.py</code>. This example uses <code>consist.run()</code>, which is the simplest way to track one-off functions. For building reusable pipeline libraries, you may prefer the <code>@tracker.define_step</code> decorator shown in the next tutorial.</p> <pre><code>from pathlib import Path\nimport consist\nfrom consist import Tracker\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")  # (1)!\n\ndef process_data(value: int) -&gt; dict:\n    print(f\"Executing with value={value}\")  # (2)!\n    return {\"squared\": value ** 2}\n\nwith consist.use_tracker(tracker):  # (3)!\n    result = consist.run(\n        fn=process_data,\n        name=\"square\",\n        config={\"value\": 5},\n        runtime_kwargs={\"value\": 5},  # (4)!\n    )\n\nprint(result.run.id, result.outputs)\n</code></pre> <ol> <li>The Tracker manages the provenance database and run directory. All runs and artifacts are stored here.</li> <li>This print statement appears only when the function executes—not on cache hits.</li> <li><code>use_tracker</code> sets the active tracker for the context. Functions inside this block use this tracker.</li> <li><code>runtime_kwargs</code> passes arguments to the function. <code>config</code> is hashed to determine cache validity.</li> </ol>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#run-twice-observe-caching","level":2,"title":"Run Twice, Observe Caching","text":"<p>Execute the script twice:</p> <pre><code>python quickstart.py\npython quickstart.py\n</code></pre> <p>First run: The function executes, printing <code>Executing with value=5</code>. Consist records the run's signature—a hash of the code version, config, and inputs—and stores the output.</p> <p>Second run: Consist finds a run with an identical signature (a cache hit) and returns the stored result. The function body does not execute; no print statement appears.</p> <p>This is Consist's core value proposition: execute once, reuse forever—until code or config changes.</p>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#inspect-provenance","level":2,"title":"Inspect Provenance","text":"<p>Query the provenance database to see recorded runs:</p> <pre><code>python -m consist.cli runs --db-path ./provenance.duckdb --limit 5\n</code></pre> <p>Output includes run ID, model name, status, and execution time.</p>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>First Workflow — Build a multi-step pipeline with data passing between tasks</li> <li>Core Concepts — Understand artifacts, signatures, and lineage</li> </ul>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"integrations/","level":1,"title":"Integrations","text":"<p>These integrations extend Consist for container execution, data ingestion, and configuration management.</p>","path":["Integrations"],"tags":[]},{"location":"integrations/#when-to-use-each-integration","level":2,"title":"When to Use Each Integration","text":"Integration Use when Container Wrapping existing tools (ActivitySim, SUMO, BEAM) without source modification DLT Loader Cross-run SQL queries on 100K+ row datasets with schema validation Config Adapters Tracking file-based configs (YAML/HOCON hierarchies) with queryable parameters","path":["Integrations"],"tags":[]},{"location":"integrations/#container-integration","level":2,"title":"Container Integration","text":"<p>Execute Docker and Singularity containers with provenance tracking.</p> <ul> <li>Image digest-based caching for reproducibility</li> <li>Volume mounting and output capture</li> <li>Full guide | API reference</li> </ul>","path":["Integrations"],"tags":[]},{"location":"integrations/#dlt-loader","level":2,"title":"DLT Loader","text":"<p>Schema-validated data ingestion with DuckDB.</p> <ul> <li>Automatic provenance column injection (<code>consist_run_id</code>, etc.)</li> <li>Cross-run SQL queries with registered schemas</li> <li>Full guide | API reference</li> </ul>","path":["Integrations"],"tags":[]},{"location":"integrations/#config-adapters","level":2,"title":"Config Adapters","text":"<p>Model-specific configuration discovery and tracking.</p> <ul> <li>Automatic discovery of YAML/HOCON hierarchies</li> <li>Queryable configuration tables for sensitivity analysis</li> <li>Full guide | ActivitySim | BEAM</li> </ul>","path":["Integrations"],"tags":[]},{"location":"integrations/config_adapters/","level":1,"title":"Config Adapters","text":"<p>Config adapters provide model-specific interfaces to discover, canonicalize, and ingest complex file-based configurations. They enable tracking of calibration-sensitive parameters as queryable database tables while preserving full config provenance via artifacts.</p>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#decision-guide","level":2,"title":"Decision Guide","text":"Criterion Use Config Adapters Use In-Memory Config Config location Multiple files (YAML, CSV, HOCON) Python dict or Pydantic model Config structure Layered inheritance, file hierarchies Flat or simple nesting Queryable parameters Calibration-sensitive values Simple facets suffice File tracking Need content hashing per file No file artifacts needed <p>For in-memory configuration, see Configuration, Identity, and Facets.</p>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#architecture","level":3,"title":"Architecture","text":"<p>Config adapters implement three phases:</p> <pre><code>Config Directory\n    ↓\ndiscover()           ← Locate files, compute content hash\n    ↓\nCanonicalConfig\n    ↓\ncanonicalize()       ← Generate artifact specs + ingest specs\n    ↓\nCanonicalizationResult (artifacts + ingestables)\n    ↓\ntracker.canonicalize_config()  ← Log artifacts, ingest to DB\n    ↓\nQueryable Tables + Full Provenance\n</code></pre>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#adapter-guides","level":2,"title":"Adapter Guides","text":"<p>Detailed, model-specific implementations live here:</p> <ul> <li>ActivitySim Config Adapter</li> <li>BEAM Config Adapter</li> </ul> <p>These guides include discovery/canonicalization behaviors, ingestion schemas, and query examples. The API patterns are shared: you construct an adapter, call <code>tracker.canonicalize_config(...)</code> or <code>tracker.prepare_config(...)</code>, and then query the resulting cache tables.</p>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#extensibility","level":3,"title":"Extensibility","text":"<p>Config adapters follow a Protocol interface, making it straightforward to add support for other models. Future adapters might include:</p> <ul> <li>Additional HOCON/YAML models: Layered config discovery + calibration-sensitive key extraction</li> <li>Custom Models: Any model with file-based config that needs parameterization tracking</li> </ul> <p>If you'd like to add an adapter for your own model, consult the ActivitySim adapter source as a reference implementation.</p>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#see-also","level":2,"title":"See Also","text":"<ul> <li>Configuration, Identity, and Facets — In-memory config and facets</li> <li>Ingestion and Hybrid Views — DLT-based data ingestion</li> <li>Caching and Hydration — Run signature and cache identity</li> </ul>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters_activitysim/","level":1,"title":"ActivitySim Config Adapter","text":"<p>The ActivitySim config adapter discovers and canonicalizes ActivitySim configuration directories (with support for YAML inheritance, CSV references, and config bundling).</p>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#overview","level":2,"title":"Overview","text":"<p>Features:</p> <ul> <li>YAML Discovery: Resolves <code>settings.yaml</code> and active model YAMLs via <code>inherit_settings</code> and <code>include_settings</code></li> <li>CSV Reference Detection: Finds coefficients, probabilities, and specification files via registry + heuristics</li> <li>Constants Extraction: Flattens YAML <code>CONSTANTS</code> sections into queryable rows</li> <li>Config Bundling: Creates tarball archives for full config provenance</li> <li>Config Materialization: Apply parameter overrides to base bundles for scenario-based runs</li> <li>Strict/Lenient Modes: Error on file integrity issues or gracefully skip</li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#use-cases","level":2,"title":"Use Cases","text":"<p>1. Track calibration parameters across scenarios</p> <p>Compare how constants and coefficients change between runs:</p> <pre><code>from consist.integrations.activitysim import ActivitySimConfigAdapter\n\nadapter = ActivitySimConfigAdapter()\n\n# Run baseline scenario\nrun_a = tracker.begin_run(\"baseline\", \"activitysim\", cache_mode=\"overwrite\")\ntracker.canonicalize_config(adapter, [config_dir], ingest=True)\ntracker.end_run()\n\n# Run adjusted scenario\nrun_b = tracker.begin_run(\"adjusted\", \"activitysim\", cache_mode=\"overwrite\")\ntracker.canonicalize_config(adapter, [config_dir_adjusted], ingest=True)\ntracker.end_run()\n\n# Query which runs used a specific sample_rate\nrows_by_run = adapter.constants_by_run(\n    key=\"sample_rate\",\n    collapse=\"first\",\n    tracker=tracker,\n)\n\n# Use in joins/facet analyses\nsample_rate_sq = adapter.constants_query(key=\"sample_rate\").subquery()\n</code></pre> <p>2. Precompute config plans for caching + orchestration</p> <p>Prepare config artifacts and ingestion specs before a run, then apply them inside <code>consist.run</code>/<code>consist.trace</code> (or <code>Tracker.run</code>/<code>Tracker.trace</code>):</p> <pre><code>import consist\nfrom consist import use_tracker\n\nadapter = ActivitySimConfigAdapter()\nplan = tracker.prepare_config(adapter, [overlay_dir, base_dir])\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=run_activitysim,\n        name=\"activitysim\",\n        config={\"scenario\": \"baseline\"},\n        config_plan=plan,\n        cache_mode=\"auto\",\n    )\n</code></pre> <p>You can also pass shared adapter options and run validation without ingesting:</p> <pre><code>from consist.core.config_canonicalization import ConfigAdapterOptions\n\noptions = ConfigAdapterOptions(strict=True, bundle=False, ingest=False)\nplan = tracker.prepare_config(\n    adapter,\n    [overlay_dir, base_dir],\n    options=options,\n    validate_only=True,\n)\nif plan.diagnostics and not plan.diagnostics.ok:\n    raise ValueError(\"Config validation failed.\")\n</code></pre> <p>3. Apply parameter adjustments for sensitivity testing</p> <p>Use the <code>materialize()</code> method to apply overrides to a baseline config:</p> <pre><code>from consist.integrations.activitysim import ConfigOverrides\n\n# Load baseline bundle\nbaseline_bundle = Path(\"outputs/base_run/config_bundle_xxxxx.tar.gz\")\n\n# Create overrides\noverrides = ConfigOverrides(\n    constants={\n        (\"settings.yaml\", \"sample_rate\"): 0.1,\n        (\"accessibility.yaml\", \"CONSTANTS.AUTO_TIME\"): 60.0,\n    },\n    coefficients={\n        (\"tour_mode_coeffs.csv\", \"car_ASC\", \"\"): -0.5,  # \"\" for direct coefficients\n    }\n)\n\n# Materialize new config with overrides\nmaterialized = adapter.materialize(\n    baseline_bundle,\n    overrides,\n    output_dir=Path(\"temp/adjusted_config\"),\n    identity=tracker.identity,\n)\n\n# Canonicalize the adjusted config\nrun = tracker.begin_run(\"sensitivity_test\", \"activitysim\")\ntracker.canonicalize_config(adapter, materialized.root_dirs, ingest=True)\ntracker.end_run()\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#discovery-and-canonicalization-workflow","level":2,"title":"Discovery and Canonicalization Workflow","text":"","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#discovery-phase-adapterdiscover","level":3,"title":"Discovery Phase (<code>adapter.discover()</code>)","text":"<ol> <li>Locates <code>settings.yaml</code> and loads active <code>models</code> list</li> <li>Resolves model YAMLs via suffix stripping and alias mapping</li> <li>Supports YAML <code>include_settings</code> for file references</li> <li>Computes content hash of all config directories</li> <li>Returns <code>CanonicalConfig</code> with file inventory</li> </ol> <p>Configuration options:</p> <pre><code>adapter = ActivitySimConfigAdapter(\n    model_name=\"activitysim\",        # Metadata label\n    adapter_version=\"0.1\",           # For run.meta tracking\n    allow_heuristic_refs=True,       # Detect *_FILE, *_PATH keys\n    bundle_configs=True,             # Create tarball archive\n    bundle_cache_dir=None,           # Default: &lt;run_dir&gt;/config_bundles\n)\n\ncanonical = adapter.discover(\n    root_dirs=[config_dir],\n    identity=tracker.identity,\n    strict=False,  # Warn on missing settings.yaml\n)\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#canonicalization-phase-adaptercanonicalize","level":3,"title":"Canonicalization Phase (<code>adapter.canonicalize()</code>)","text":"<ol> <li>Loads effective settings (after merging inheritance chain)</li> <li>Logs active YAMLs and referenced CSVs as input artifacts</li> <li>Extracts <code>CONSTANTS</code> + allowlisted settings → ingestion spec</li> <li>Classifies CSVs as coefficients or probabilities → ingestion specs</li> <li>Creates config bundle tarball (cached by content hash)</li> <li>Returns specs for artifact logging and ingest</li> </ol> <p>Key behaviors:</p> <ul> <li>Constant Attribution: Constants attributed to effective YAML after inheritance</li> <li>CSV Classification: Files ending in <code>_coefficients.csv</code>, <code>_coeffs.csv</code>, <code>_coefficients_template.csv</code>, or <code>_probs.csv</code></li> <li>CSV Format Support: Both direct (<code>value</code> column) and template (segment columns) coefficient formats</li> <li>Gzip Support: Transparently handles <code>.csv.gz</code> files</li> <li>Lenient Mode (default): Missing referenced CSVs logged as warnings; ingestion proceeds</li> <li>Strict Mode: Raises on missing files or malformed CSVs</li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#queryable-schemas","level":2,"title":"Queryable Schemas","text":"<p>ActivitySim config tables are deduplicated by content hash. To map rows back to individual runs, join against <code>activitysim_config_ingest_run_link</code> on <code>content_hash</code> and <code>table_name</code>.</p>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#activitysim_constants_cache","level":3,"title":"<code>activitysim_constants_cache</code>","text":"<p>Flattened YAML constants and allowlisted settings, deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the source file <code>file_name</code> str Primary key: source YAML file name <code>key</code> str Primary key: dot-notation constant key (e.g., <code>CONSTANTS.AUTO_TIME</code>, <code>sample_rate</code>) <code>value_type</code> str Type tag: <code>null</code>, <code>bool</code>, <code>num</code>, <code>str</code>, <code>json</code> <code>value_str</code> str | NULL String value when <code>value_type == \"str\"</code> <code>value_num</code> float | NULL Numeric value when <code>value_type == \"num\"</code> <code>value_bool</code> bool | NULL Boolean value when <code>value_type == \"bool\"</code> <code>value_json</code> JSON | NULL Complex value when <code>value_type == \"json\"</code> <p>Query example: Find runs with specific constant values</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimConstantsCache\n\nwith Session(tracker.engine) as session:\n    # Find runs where AUTO_TIME &gt; 50\n    rows = session.exec(\n        select(\n            ActivitySimConfigIngestRunLink.run_id,\n            ActivitySimConstantsCache.value_num,\n        )\n        .join(\n            ActivitySimConstantsCache,\n            ActivitySimConstantsCache.content_hash\n            == ActivitySimConfigIngestRunLink.content_hash,\n        )\n        .where(ActivitySimConfigIngestRunLink.table_name == \"activitysim_constants_cache\")\n        .where(ActivitySimConstantsCache.key == \"CONSTANTS.AUTO_TIME\")\n        .where(ActivitySimConstantsCache.value_num &gt; 50)\n    ).all()\n\n    for run_id, value_num in rows:\n        print(f\"{run_id}: AUTO_TIME = {value_num}\")\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#activitysim_coefficients_cache","level":3,"title":"<code>activitysim_coefficients_cache</code>","text":"<p>Parsed coefficient rows from CSV files, deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the source file <code>file_name</code> str Primary key: source CSV file name <code>coefficient_name</code> str Primary key: coefficient identifier <code>segment</code> str Primary key: segment name (template) or \"\" (direct) <code>source_type</code> str \"direct\" (value column) or \"template\" (segment columns) <code>value_raw</code> str Raw CSV cell value <code>value_num</code> float | NULL Parsed numeric value <code>constrain</code> str | NULL Constraint flag from CSV (if present) <code>is_constrained</code> bool | NULL Parsed constraint boolean <p>Query example: Compare coefficients across runs</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimCoefficientsCache\n\nwith Session(tracker.engine) as session:\n    # Find how a specific coefficient changed\n    rows = session.exec(\n        select(\n            ActivitySimConfigIngestRunLink.run_id,\n            ActivitySimCoefficientsCache.coefficient_name,\n            ActivitySimCoefficientsCache.segment,\n            ActivitySimCoefficientsCache.value_raw,\n        )\n        .join(\n            ActivitySimCoefficientsCache,\n            ActivitySimCoefficientsCache.content_hash\n            == ActivitySimConfigIngestRunLink.content_hash,\n        )\n        .where(\n            ActivitySimConfigIngestRunLink.table_name\n            == \"activitysim_coefficients_cache\"\n        )\n        .where(ActivitySimCoefficientsCache.coefficient_name == \"car_ASC\")\n    ).all()\n\n    for run_id, coef_name, segment, value_raw in rows:\n        print(f\"{run_id}: {coef_name}[{segment}] = {value_raw}\")\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#activitysim_probabilities_cache","level":3,"title":"<code>activitysim_probabilities_cache</code>","text":"<p>Parsed probability table rows (dims and numeric probabilities separated), deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the source file <code>file_name</code> str Primary key: source CSV file name <code>row_index</code> int Primary key: row index in source file <code>dims</code> JSON Non-numeric dimension values (e.g., <code>{\"mode\": \"drive\", \"income\": \"high\"}</code>) <code>probs</code> JSON Numeric probability/weight values (e.g., <code>{\"prob_0\": 0.3, \"prob_1\": 0.7}</code>) <p>Query example: Inspect probability tables</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimProbabilitiesCache\n\nwith Session(tracker.engine) as session:\n    # Find probability rows for a specific file\n    rows = session.exec(\n        select(\n            ActivitySimConfigIngestRunLink.run_id,\n            ActivitySimProbabilitiesCache.row_index,\n            ActivitySimProbabilitiesCache.dims,\n            ActivitySimProbabilitiesCache.probs,\n        )\n        .join(\n            ActivitySimProbabilitiesCache,\n            ActivitySimProbabilitiesCache.content_hash\n            == ActivitySimConfigIngestRunLink.content_hash,\n        )\n        .where(\n            ActivitySimConfigIngestRunLink.table_name\n            == \"activitysim_probabilities_cache\"\n        )\n        .where(ActivitySimProbabilitiesCache.file_name == \"atwork_probs.csv\")\n    ).all()\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#additional-tables","level":4,"title":"Additional Tables","text":"<ul> <li><code>activitysim_probabilities_entries_cache</code></li> <li><code>activitysim_probabilities_meta_entries_cache</code></li> <li><code>activitysim_coefficients_template_refs_cache</code></li> <li><code>activitysim_config_ingest_run_link</code></li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#configuration-best-practices","level":2,"title":"Configuration Best Practices","text":"<p>1. Organize config directories with inheritance</p> <pre><code>configs/\n  base/\n    settings.yaml      # inherit_settings: true\n    accessibility.yaml\n    coefficients.csv\n  overlay/\n    settings.yaml      # inherits from base\n    local_overrides.yaml\n</code></pre> <p>Call with overlay first (takes precedence):</p> <pre><code>tracker.canonicalize_config(adapter, [overlay_dir, base_dir])\n</code></pre> <p>2. Use strict mode for validation</p> <p>In development/testing, catch config problems early:</p> <pre><code>tracker.canonicalize_config(adapter, config_dirs, strict=True)\n</code></pre> <p>3. Leverage config bundling for reconstruction</p> <p>The bundle tarball preserves full config state, enabling: - Auditing exact config used for a run - Reproducing runs via <code>materialize()</code> - Sharing configs across machines</p> <p>Bundles are cached by content hash, so repeated canonicalizations are efficient.</p> <p>4. Query constants before running</p> <p>Use <code>activitysim_constants_cache</code> + <code>activitysim_config_ingest_run_link</code> queries to validate assumptions:</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimConstantsCache, ActivitySimConfigIngestRunLink\n\n# Verify sample_rate is set to expected value\nwith Session(tracker.engine) as session:\n    result = session.exec(\n        select(ActivitySimConstantsCache.value_num)\n        .join(\n            ActivitySimConfigIngestRunLink,\n            ActivitySimConfigIngestRunLink.content_hash\n            == ActivitySimConstantsCache.content_hash,\n        )\n        .where(ActivitySimConfigIngestRunLink.run_id == run.id)\n        .where(ActivitySimConfigIngestRunLink.table_name == \"activitysim_constants_cache\")\n        .where(ActivitySimConstantsCache.key == \"sample_rate\")\n    ).first()\n    value_num = result[0] if result else None\n    assert value_num == 0.25, f\"Expected sample_rate=0.25, got {value_num}\"\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#api-reference","level":2,"title":"API Reference","text":"<p>For detailed method signatures, parameters, and return types, see:</p> <ul> <li><code>ActivitySimConfigAdapter</code></li> <li><code>ConfigOverrides</code></li> <li><code>Tracker.canonicalize_config()</code></li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/","level":1,"title":"BEAM Config Adapter","text":"<p>The BEAM config adapter canonicalizes HOCON <code>.conf</code> configurations, resolves includes, and ingests every resolved key/value for fast queries. Paths that exist on disk are logged as input artifacts; missing paths produce warnings.</p> <p>Dependencies: - Requires <code>pyhocon</code> for parsing <code>.conf</code> files. - Requires <code>pandas</code> only if you use tabular ingestion via <code>BeamIngestSpec</code>.</p>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#usage","level":2,"title":"Usage","text":"<pre><code>from pathlib import Path\n\nfrom consist.integrations.beam import BeamConfigAdapter\n\nconfig_root = Path(\"/path/to/beam/production/sfbay\")\nadapter = BeamConfigAdapter(\n    primary_config=config_root / \"sfbay-pilates-base.conf\",\n    env_overrides={\"PWD\": str(config_root.parent.parent)},\n)\n\nrun = tracker.begin_run(\"beam_baseline\", \"beam\")\ntracker.canonicalize_config(adapter, [config_root], ingest=True)\ntracker.end_run()\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#facets","level":2,"title":"Facets","text":"<pre><code>plan = tracker.prepare_config(\n    adapter,\n    [config_root],\n    facet_spec={\n        \"keys\": [\n            \"beam.agentsim.simulationName\",\n            {\"key\": \"beam.physsim.name\", \"alias\": \"physsim\"},\n        ],\n    },\n    facet_schema_name=\"beam_config\",\n)\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#tabular-ingestion-by-config-key","level":2,"title":"Tabular Ingestion by Config Key","text":"<pre><code>from sqlmodel import Field, SQLModel\n\nfrom consist.integrations.beam import BeamIngestSpec\n\n\nclass BeamVehicleTypesCache(SQLModel, table=True):\n    __tablename__ = \"beam_vehicletypes_cache\"\n    __table_args__ = {\"schema\": \"global_tables\"}\n\n    id: int = Field(primary_key=True)\n    value: str\n    content_hash: str = Field(index=True)\n\n\nadapter = BeamConfigAdapter(\n    primary_config=config_root / \"sfbay-pilates-base.conf\",\n    ingest_specs=[\n        BeamIngestSpec(\n            key=\"beam.agentsim.agents.vehicles.vehicleTypesFilePath\",\n            table_name=\"beam_vehicletypes_cache\",\n            schema=BeamVehicleTypesCache,\n        ),\n    ],\n)\n</code></pre> <p>Notes: - Schemas used with <code>BeamIngestSpec</code> should include a <code>content_hash</code> column for dedupe. - If your configs use optional env substitutions (e.g., <code>${?BEAM_OUTPUT}</code>), set them via <code>env_overrides</code> to avoid unresolved keys during canonicalization/materialization.</p>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#tables","level":2,"title":"Tables","text":"","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#beam_config_cache","level":3,"title":"<code>beam_config_cache</code>","text":"<p>Canonicalized config key/value rows, deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the config <code>key</code> str Primary key: dotted config path <code>value_type</code> str One of <code>str</code>, <code>num</code>, <code>bool</code>, <code>null</code>, <code>json</code> <code>value_str</code> str | NULL String values <code>value_num</code> float | NULL Numeric values <code>value_bool</code> bool | NULL Boolean values <code>value_json_str</code> str | NULL JSON-encoded values","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#beam_config_ingest_run_link","level":3,"title":"<code>beam_config_ingest_run_link</code>","text":"<p>Links runs to ingested config hashes for query joins.</p> Column Type Notes <code>run_id</code> str Primary key: Consist run id <code>table_name</code> str Primary key: cache table name <code>content_hash</code> str Primary key: config content hash <code>config_name</code> str Primary key: config file name <p>Query example: Compare a key across runs</p> <pre><code>from sqlmodel import Session, select\n\nfrom consist.models.beam import BeamConfigCache, BeamConfigIngestRunLink\n\nwith Session(tracker.engine) as session:\n    rows = session.exec(\n        select(\n            BeamConfigIngestRunLink.run_id,\n            BeamConfigCache.key,\n            BeamConfigCache.value_num,\n            BeamConfigCache.value_str,\n        )\n        .join(\n            BeamConfigCache,\n            BeamConfigCache.content_hash == BeamConfigIngestRunLink.content_hash,\n        )\n        .where(BeamConfigIngestRunLink.table_name == \"beam_config_cache\")\n        .where(BeamConfigCache.key == \"beam.agentsim.agentSampleSizeAsFractionOfPopulation\")\n    ).all()\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#behavior-notes","level":2,"title":"Behavior Notes","text":"<ul> <li><code>resolve_substitutions=True</code> resolves HOCON substitutions; set to False to keep raw expressions.</li> <li><code>env_overrides</code> supplies environment variables for optional substitutions (e.g., <code>${?BEAM_OUTPUT}</code>).</li> <li><code>strict=True</code> raises on missing referenced files; otherwise missing paths are logged as warnings.</li> </ul>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#materialize-overrides","level":2,"title":"Materialize Overrides","text":"<pre><code>from consist.integrations.beam import BeamConfigOverrides\n\noverrides = BeamConfigOverrides(\n    values={\n        \"beam.agentsim.agentSampleSizeAsFractionOfPopulation\": 0.75,\n        \"beam.agentsim.lastIteration\": 5,\n    }\n)\n\nmaterialized = adapter.materialize(\n    [config_root],\n    overrides,\n    output_dir=Path(\"tmp/beam_materialized\"),\n    identity=tracker.identity,\n)\n</code></pre> <p>If you already built a config plan (e.g., for caching), you can reuse its <code>config_dirs</code> metadata:</p> <pre><code>plan = tracker.prepare_config(adapter, [config_root])\nmaterialized = adapter.materialize_from_plan(\n    plan,\n    overrides,\n    output_dir=Path(\"tmp/beam_materialized\"),\n    identity=tracker.identity,\n)\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#api-reference","level":2,"title":"API Reference","text":"<p>For detailed method signatures, parameters, and return types, see:</p> <ul> <li><code>BeamConfigAdapter</code></li> <li><code>BeamConfigOverrides</code></li> <li><code>Tracker.canonicalize_config()</code></li> </ul>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/containers/","level":1,"title":"Containers","text":"<p>Consist Container API Module</p> <p>This module provides a high-level API for executing containerized steps (e.g., Docker, Singularity/Apptainer) with automatic provenance tracking and caching through Consist. It abstracts away the complexities of interacting directly with container runtimes and integrates seamlessly with Consist's <code>Tracker</code> to log container execution details, input dependencies, and output artifacts.</p> <p>Key functionalities include: -   Container Execution with Provenance: Wraps container execution     within a Consist <code>start_run</code> context, ensuring that container image     identity, commands, environment hash, and file I/O are tracked. -   Backend Agnosticism: Supports different container runtimes     (Docker, Singularity/Apptainer) via a unified interface. -   Automated Input/Output Logging: Automatically logs host-side     files as inputs and scans specified paths for outputs, linking them     to the container run.</p> <p>               Bases: <code>BaseModel</code></p> <p>Represents the 'Configuration' of a container run for hashing purposes.</p> <p>This model captures all relevant parameters that define a containerized execution, allowing Consist to compute a canonical hash for the container's configuration. This hash is critical for determining cache hits and ensuring reproducibility.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>str</code> <p>The name or reference of the container image (e.g., \"ubuntu:latest\").</p> <code>image_digest</code> <code>Optional[str]</code> <p>A content-addressable SHA digest of the container image, used for precise reproducibility. If None, the image tag is used.</p> <code>command</code> <code>List[str]</code> <p>The command and its arguments to execute inside the container, represented as a list of strings (exec form).</p> <code>environment</code> <code>Dict[str, str]</code> <p>A dictionary of environment variables passed to the container. Values are not persisted in run metadata; only a deterministic hash is stored for caching.</p> <code>backend</code> <code>str</code> <p>The container backend used to execute this container (e.g., \"docker\", \"singularity\").</p> <code>extra_args</code> <code>Dict[str, Any]</code> <p>Additional arguments or configuration specific to the container backend that might influence the execution but are not part of the core identity (e.g., resource limits, specific volume options).</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.api.ContainerResult","level":2,"title":"<code>ContainerResult</code>  <code>dataclass</code>","text":"<p>Return value for run_container with cached output artifacts.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.api.ContainerResult.output","level":3,"title":"<code>output</code>  <code>property</code>","text":"<p>Convenience: return the first (or only) output artifact if present.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.api.run_container","level":2,"title":"<code>run_container(tracker, run_id, image, command, volumes, inputs, outputs, environment=None, working_dir=None, backend_type='docker', pull_latest=False, lineage_mode='full', strict_mounts=True)</code>","text":"<p>Executes a containerized step with optional provenance tracking and caching via Consist.</p> <p>This function acts as a high-level wrapper that integrates container execution with Consist's <code>Tracker</code>. In lineage mode \"full\" it initiates a <code>Consist</code> run (or attaches to an active run), uses the container's image and command as part of the run's identity (code/config), and tracks host-side files as inputs and outputs. In lineage mode \"none\" it only executes the container and returns a stable manifest/hash for callers to incorporate into an enclosing step's identity.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Tracker</code> <p>The active Consist <code>Tracker</code> instance to use for provenance logging.</p> required <code>run_id</code> <code>str</code> <p>A unique identifier for this container execution run within Consist.</p> required <code>image</code> <code>str</code> <p>The container image to use (e.g., \"ubuntu:latest\", \"my_repo/my_image:tag\").</p> required <code>command</code> <code>Union[str, List[str]]</code> <p>The command to execute inside the container. Can be a string or a list of strings (for exec form). Commands are validated for non-empty tokens and a maximum length.</p> required <code>volumes</code> <code>Dict[str, str]</code> <p>A dictionary mapping host paths to container paths for volume mounts. Example: <code>{\"/host/path\": \"/container/path\"}</code>. Host paths are resolved and validated against tracker mounts. When strict_mounts is False, any absolute host path is permitted. Relative paths are resolved against the first mount root.</p> required <code>inputs</code> <code>List[ArtifactRef]</code> <p>A list of paths (str/Path) or <code>Artifact</code> objects on the host machine that serve as inputs to the containerized process. These are logged as Consist inputs.</p> required <code>outputs</code> <code>List[str]</code> <p>A list of paths on the host machine that are expected to be generated or modified by the containerized process. These paths will be scanned and logged as Consist output artifacts. Host paths are validated against tracker mounts and must remain within run_dir unless allow_external_paths is enabled.</p> required <code>outputs</code> <code>Dict[str, str]</code> <p>Alternatively, pass a mapping of logical output keys to host paths. The artifact will be logged with the provided key instead of the filename. Host paths are validated against tracker mounts and must remain within run_dir unless allow_external_paths is enabled.</p> required <code>environment</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary of environment variables to set inside the container. Defaults to empty.</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>The working directory inside the container where the command will be executed. If None, the default working directory of the container image will be used.</p> <code>None</code> <code>backend_type</code> <code>str</code> <p>The container runtime backend to use. Currently supports \"docker\" and \"singularity\".</p> <code>\"docker\"</code> <code>pull_latest</code> <code>bool</code> <p>If True, the Docker backend will attempt to pull the latest image before execution. (Applicable only for 'docker' backend).</p> <code>False</code> <code>lineage_mode</code> <code>Literal['full', 'none']</code> <p>\"full\" performs Consist provenance tracking, caching, and output scanning. \"none\" skips Consist logging/caching and does not scan outputs.</p> <code>\"full\"</code> <code>strict_mounts</code> <code>bool</code> <p>If True, require tracker mounts to be configured and constrain container host paths to those roots. Set to False to allow any absolute host path.</p> <code>True</code> <p>Returns:</p> Type Description <code>ContainerResult</code> <p>Structured result containing logged output artifacts and cache metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown <code>backend_type</code> is specified.</p> <code>RuntimeError</code> <p>If the container execution itself fails (e.g., non-zero exit code). If the underlying backend fails to resolve image digest or run the container.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.backend","level":2,"title":"<code>backend</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.command","level":2,"title":"<code>command</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.declared_outputs","level":2,"title":"<code>declared_outputs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.environment","level":2,"title":"<code>environment</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.extra_args","level":2,"title":"<code>extra_args = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.image","level":2,"title":"<code>image</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.image_digest","level":2,"title":"<code>image_digest = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.volumes","level":2,"title":"<code>volumes = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.working_dir","level":2,"title":"<code>working_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.to_hashable_config","level":2,"title":"<code>to_hashable_config()</code>","text":"<p>Returns a clean dictionary representation of the container configuration suitable for hashing.</p> <p>This method generates a dictionary that excludes <code>None</code> values, ensuring a canonical representation of the configuration for consistent hash computation. This is crucial for Consist's caching mechanism.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the essential configuration parameters of the container, stripped of any <code>None</code> values, ready for hashing.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/dlt_loader/","level":1,"title":"DLT Loader","text":"<p>Requires the optional <code>ingest</code> extra: <code>pip install \"consist[ingest]\"</code>.</p> <p>Consist dlt (Data Load Tool) Integration Module</p> <p>This module provides the integration layer between Consist and the <code>dlt</code> library, facilitating the robust and efficient ingestion of artifact data into the DuckDB database. It is responsible for materializing various data formats (e.g., Pandas DataFrames, Parquet, CSV, Zarr metadata) and ensuring that Consist's system-level provenance columns (such as <code>consist_run_id</code>, <code>consist_artifact_id</code>) are correctly injected into the data.</p> <p>Key functionalities include: -   Dynamic Schema Extension: User-defined <code>SQLModel</code> schemas are dynamically extended     with Consist's provenance-tracking system columns. -   Flexible Ingestion Strategies: Supports different data ingestion mechanisms,     including vectorized loading (for Pandas DataFrames, PyArrow tables) and streaming     for large datasets. -   Format-Specific Handlers: Contains specialized functions for processing and     preparing data from common file formats like Parquet, CSV, and extracting     structural metadata from Zarr archives. -   dlt Pipeline Integration: Leverages the <code>dlt</code> pipeline for robust data loading,     automatic schema inference, and optional strict validation, ensuring data quality     and consistency.</p>","path":["Integrations","DLT Loader"],"tags":[]},{"location":"integrations/dlt_loader/#consist.integrations.dlt_loader.ingest_artifact","level":2,"title":"<code>ingest_artifact(artifact, run_context, db_path, data_iterable=None, schema_model=None)</code>","text":"<p>Ingests artifact data into a DuckDB database using the <code>dlt</code> (Data Load Tool) library.</p> <p>This function supports various data sources (file paths, Pandas DataFrames, iterables of dicts) and automatically injects Consist's provenance system columns (<code>consist_run_id</code>, <code>consist_artifact_id</code>, <code>consist_year</code>, <code>consist_iteration</code>) into the data. It leverages <code>dlt</code> for robust schema handling, including inference and optional strict validation based on a provided <code>SQLModel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The Consist <code>Artifact</code> object representing the data to be ingested. Its driver information is used to determine the appropriate data handler.</p> required <code>run_context</code> <code>Run</code> <p>The <code>Run</code> object providing the context (ID, year, iteration) for provenance tracking.</p> required <code>db_path</code> <code>str</code> <p>The file system path to the DuckDB database where the data will be loaded.</p> required <code>data_iterable</code> <code>Optional[Union[Iterable[Any], str, DataFrame]]</code> <p>The data to ingest. Can be: - A file path (str) to a Parquet, CSV, HDF5, JSON, or Zarr file. - A Pandas DataFrame (will be treated as a single batch). - An iterable (e.g., list of dicts, generator) where each item represents a row. If <code>None</code>, it implies the data should be read directly from the <code>artifact</code>'s URI.</p> <code>None</code> <code>schema_model</code> <code>Optional[Type[SQLModel]]</code> <p>An optional <code>SQLModel</code> class that defines the expected schema for the data. If provided, <code>dlt</code> will use this for strict validation and schema management. If <code>None</code>, <code>dlt</code> will infer the schema.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LoadInfo, str]</code> <p>A tuple containing: - <code>dlt.LoadInfo</code>: An object providing detailed information about the data loading process. - <code>str</code>: The actual normalized table name where the data was loaded in the database.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no data is provided for ingestion, if the artifact driver is unsupported, or if a <code>schema_model</code> is provided but a schema contract violation occurs (e.g., new columns found in strict mode).</p> <code>ImportError</code> <p>If a required library for a specific driver (e.g., <code>pyarrow</code> for Parquet, <code>tables</code> for HDF5, <code>xarray</code>/<code>zarr</code> for Zarr) is not installed.</p>","path":["Integrations","DLT Loader"],"tags":[]}]}