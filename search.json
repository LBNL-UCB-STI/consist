{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"Consist","text":"<p>Consist automatically records what code, configuration, and input data produced each result in your simulation pipeline. It uses this to avoid redundant computation, lets you query results across runs, and traces lineage from any output back to its sources.</p>","path":["Consist"],"tags":[]},{"location":"#getting-started","level":2,"title":"Getting Started","text":"<p>New to Consist? Follow this path:</p>","path":["Consist"],"tags":[]},{"location":"#1-install-quickstart","level":3,"title":"1. Install &amp; Quickstart","text":"<ul> <li>Install Consist from source and run a small first example</li> <li>Creates your first run and provenance database</li> </ul>","path":["Consist"],"tags":[]},{"location":"#2-concepts","level":3,"title":"2. Concepts","text":"<ul> <li>Learn the mental model: Runs, Artifacts, Scenarios, and how caching works</li> <li>Start here to understand core ideas</li> </ul>","path":["Consist"],"tags":[]},{"location":"#3-usage-guide","level":3,"title":"3. Usage Guide","text":"<ul> <li>Build your first multi-step workflow</li> <li>Learn patterns for runs, scenarios, and cross-run querying</li> </ul>","path":["Consist"],"tags":[]},{"location":"#4-example-notebooks","level":3,"title":"4. Example Notebooks","text":"<ul> <li>Run the quickstart notebook to see caching in action</li> <li>Follow other examples for your domain (parameter sweeps, iterative workflows, multi-scenario analysis)</li> </ul>","path":["Consist"],"tags":[]},{"location":"#by-user-type","level":2,"title":"By User Type","text":"<p>I develop or maintain simulation tools (ActivitySim, MATSim, etc.): - Start with Concepts for mental models - Read Usage Guide for API patterns and integration examples - See the Container Integration Guide for wrapping legacy code</p> <p>I'm an MPO official or practitioner: - If you prefer not to write Python, use the CLI reference to query and compare results from the command line - Example: <code>consist lineage traffic_volumes</code> shows what produced your result</p> <p>I'm building research workflows: - Start with Concepts - Read Ingestion &amp; Hybrid Views for SQL-native analysis - See Mounts &amp; Portability for reproducible, shareable studies</p>","path":["Consist"],"tags":[]},{"location":"#by-topic","level":2,"title":"By Topic","text":"","path":["Consist"],"tags":[]},{"location":"#beginner","level":3,"title":"Beginner","text":"<ul> <li>Concepts – Mental models and core ideas</li> <li>Caching Fundamentals – How signature-based caching works</li> <li>Usage Guide: Basic Runs &amp; Scenarios – Simple workflows</li> <li>Example: Quickstart – 5-minute first run</li> </ul>","path":["Consist"],"tags":[]},{"location":"#intermediate","level":3,"title":"Intermediate","text":"<ul> <li>Configuration &amp; Facets – Config hashing and queryable metadata</li> <li>Caching &amp; Hydration – Caching patterns and data management policies</li> <li>Ingestion &amp; Hybrid Views – SQL queries across runs</li> <li>DLT Loader Integration – Schema-validated data ingestion with DuckDB</li> <li>Mounts &amp; Portability – Reproducible workflows with external data</li> </ul>","path":["Consist"],"tags":[]},{"location":"#advanced-reference","level":3,"title":"Advanced / Reference","text":"<ul> <li>Architecture – Implementation details and design decisions</li> <li>Container Integration – Docker/Singularity with ActivitySim, SUMO, and other tools</li> <li>CLI Reference – All command-line tools</li> <li>Troubleshooting – Common errors, debugging, and solutions</li> <li>API Overview – Function and class reference</li> </ul>","path":["Consist"],"tags":[]},{"location":"#key-terms","level":2,"title":"Key Terms","text":"<ul> <li>Run: A single execution with tracked inputs, config, and outputs. Runs are the unit of cache reuse and comparison, so keeping them consistent lets you skip expensive recomputation.</li> <li>Artifact: A file with provenance metadata attached. Artifacts are the concrete outputs you can trace, load, or hydrate later, which is how Consist keeps results reproducible.</li> <li>Signature: Fingerprint of code + config + inputs. Identical signatures lead to a cache hit.</li> <li>Scenario: A parent run grouping related child runs</li> <li>Provenance: Complete history of where a result came from</li> </ul> <p>See Glossary for full definitions of all terms, or Concepts for detailed explanations with examples.</p>","path":["Consist"],"tags":[]},{"location":"#common-tasks","level":2,"title":"Common Tasks","text":"<p>I need to speed up my pipeline: → Caching &amp; Hydration</p> <p>I want to debug a cache miss or other issue: → Troubleshooting</p> <p>I want to know which config produced a result: → CLI: <code>consist lineage</code></p> <p>I want to compare results across 50 scenarios: → Usage Guide: Cross-Run Queries</p> <p>I want to ingest data for SQL analysis across runs: → DLT Loader Integration</p> <p>I want to share my study for reproducibility: → Mounts &amp; Portability</p> <p>I want to integrate with an existing tool (ActivitySim, SUMO, BEAM): → Container Integration Guide</p>","path":["Consist"],"tags":[]},{"location":"architecture/","level":1,"title":"Architecture","text":"","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#how-caching-works","level":2,"title":"How Caching Works","text":"<p>Consist identifies runs using a three-part signature:</p> <pre><code>SHA256(code_hash | config_hash | input_hash)\n</code></pre> <p>Code hash: Derived from your Git commit SHA by default. If your repo has tracked Python changes, Consist appends a stable <code>-dirty-&lt;hash&gt;</code> suffix based on the diff (untracked files are ignored so artifacts don't invalidate caches). If Git isn't available, Consist falls back to <code>unknown_code_version</code>.</p> <p>Config hash: Canonical representation of configuration, normalized for dictionary ordering and numeric type variations. Pydantic models are serialized deterministically.</p> <p>Input hash: For Consist-produced artifacts, this references the signature of the producing run (Merkle linking) and the artifact hash when available. For raw files, Consist hashes file contents or metadata depending on the hashing strategy (<code>full</code> vs <code>fast</code>).</p> <p>This Merkle DAG structure means: - Changing a parameter invalidates only downstream runs that depend on it - Identical inputs produce cache hits across machines (given the same code version) - Provenance validity depends on the lineage graph, not file existence</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#cache-modes","level":3,"title":"Cache Modes","text":"Mode Behavior <code>reuse</code> (default) Return cached result if signature matches <code>overwrite</code> Always execute, update cache with new result <code>readonly</code> Use cache but don't persist new results (sandbox mode)","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#ghost-mode","level":3,"title":"Ghost Mode","text":"<p>Consist enables \"Ghost Mode\" — the ability to delete intermediate files while preserving provenance and recoverability. As long as the provenance database records lineage and content hashes, Consist can:</p> <ul> <li>Verify that a cached result is valid (signature matches)</li> <li>Identify which upstream run produced a missing artifact</li> <li>Re-execute only the necessary steps to regenerate data (when strict cache validation is enabled)</li> </ul> <p>When You Need This</p> <p>Long-running research workflows accumulate massive intermediate datasets that you want to keep for lineage but eventually need to reclaim space. Consider a 30-year climate simulation:</p> <pre><code>Year 2020: Compute temperature fields → Store (500GB) → Use as input for 2021\nYear 2021: Compute temperature fields → Store (500GB) → Use as input for 2022\n... (31 years × 500GB = 15.5TB)\nYear 2050: Final results published\n</code></pre> <p>Once you've published results (2030–2050), you can safely delete 2020–2029 intermediate files. The provenance database still tracks what those files contained (via content hashes). If you later need to re-run 2031, Consist can: 1. Detect that 2030's output is missing from disk (via cache validation) 2. Re-run only the missing upstream steps 3. Materialize just the files needed for the downstream run</p> <p>How It Works</p> <p>When you log an artifact in Consist, three pieces of information are persisted:</p> <ul> <li>Content hash (SHA256 of file bytes) — Stored in the database</li> <li>URI and metadata — Stored as provenance</li> <li>Actual file — On disk (optional in Ghost Mode)</li> </ul> <p>On cache hits, if a file is missing from disk, you have two recovery paths: - Re-execute (recommended for non-ingested outputs) by enabling strict cache validation - DB recovery (for ingested tabular outputs) via <code>consist.load_df(..., db_fallback=\"always\")</code></p> <p>Example: recover a missing, ingested tabular artifact from DuckDB: <pre><code>with tracker.start_run(\"2030\", model=\"climate\"):\n    artifact = tracker.log_artifact(\n        \"year_2029_temps.parquet\", key=\"temps\", direction=\"input\"\n    )\n    df = consist.load_df(artifact, db_fallback=\"always\")\n</code></pre></p> <p>Re-execution respects the same cache key (code + config + inputs), so if inputs haven't changed, Consist reuses the prior computation and materializes its output on-demand.</p> <p>Best Practices</p> <ul> <li>Use Ghost Mode for intermediate outputs in long-running studies, not critical published results.</li> <li>Keep the provenance database (<code>provenance.duckdb</code>) on reliable storage; it's the source of truth for recovery.</li> <li>Archive deleted files alongside the database for offline recovery if needed.</li> <li>Test recovery workflows (intentional deletion + re-run or DB recovery) before relying on Ghost Mode in production.</li> </ul>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#data-model","level":2,"title":"Data Model","text":"<p>Consist uses two core entities with a many-to-many relationship:</p> Entity Purpose <code>Run</code> Execution context: model name, config, timestamps, status, parent linkage <code>Artifact</code> File metadata: path (as URI), content hash, driver, schema reference <code>RunArtifactLink</code> Connects runs to their input and output artifacts with direction metadata <p>Key fields for workflow tracking: - <code>Run.parent_run_id</code> — Links scenario steps to their parent scenario; used as the scenario identifier in views (see <code>consist_scenario_id</code>) - <code>Run.year</code> — Simulation year for time-series workflows - <code>Run.tags</code> — String labels for filtering (stored as JSON array) - <code>Artifact.hash</code> — SHA256 content hash for deduplication and verification</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#configuration-management-tracking-large-complex-configs","level":3,"title":"Configuration Management: Tracking Large, Complex Configs","text":"<p>Scientific simulations often involve complex, large configuration files: ActivitySim YAML and csv spanning 5MB, BEAM routing configs, climate model parameter trees. Consist provides strategies to make these configurations part of your provenance without storing them as bulky database fields.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#why-this-matters-configuration-as-provenance","level":4,"title":"Why This Matters: Configuration as Provenance","text":"<p>Configuration is critical for reproducibility: the same code + different config = different results. Consist must: - Track config as part of the cache key (so changing parameters invalidates cache) - Make config searchable (so you can find \"all runs with alpha=0.5\") - Avoid bloating the provenance database with massive YAML files</p> <p>The challenge: A 50MB ActivitySim config file is too large to store as a queryable blob, but you can't just ignore it.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#three-configuration-strategies","level":4,"title":"Three Configuration Strategies","text":"<p>Consist separates configuration into three patterns. Choose based on your workflow:</p> <p>Strategy 1: Identity Config (For Cache Keys, Not Queries)</p> <p>Use this when configuration is important for caching but doesn't need to be queryable in the database.</p> <pre><code>from pydantic import BaseModel\n\nclass SimulationConfig(BaseModel):\n    alpha: float = 0.5\n    beta: float = 1.0\n\ntracker.start_run(\n    \"run_001\",\n    config=SimulationConfig(alpha=0.5, beta=1.0)\n)\n</code></pre> <p>Consist hashes the config deterministically and includes it in the run signature. Changes to alpha or beta invalidate the cache. Use when: Config is part of the cache key but you don't need to filter by it.</p> <p>Strategy 2: Facet (Queryable Configuration)</p> <p>Use this when you want to search and filter runs by configuration values.</p> <pre><code>tracker.start_run(\n    \"run_001\",\n    config=SimulationConfig(alpha=0.5, beta=1.0),\n    facet={\"alpha\": 0.5, \"beta\": 1.0, \"scenario\": \"baseline\"}\n)\n</code></pre> <p>Consist stores the facet as a small JSON snapshot in DuckDB and deduplicates identical facets. You can query: \"Find all runs where alpha &gt; 0.4 and scenario='baseline'\". Use when: You're running parameter sweeps and want to find results by parameter value.</p> <p>Strategy 3: Hash-Only Attachments (For Large Files, No Database Storage)</p> <p>Use this when configuration files are too large to query and don't need to be stored.</p> <pre><code>tracker.start_run(\n    \"run_001\",\n    hash_inputs=[\n        Path(\"./activitysim_config\"),  # 50MB directory\n        Path(\"./beam_config.hocon\"),   # 30MB config file\n    ]\n)\n</code></pre> <p>Consist computes a fingerprint (SHA256 hash) of the config files. This fingerprint becomes part of the run signature (changing config invalidates cache). The actual config content is not stored. Use when: Configuration files are 10MB+ or you don't need to query by config values.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#combining-strategies-the-typical-pattern","level":4,"title":"Combining Strategies: The Typical Pattern","text":"<p>Most real workflows use all three:</p> <pre><code>import consist\nfrom pydantic import BaseModel\n\nclass SimConfig(BaseModel):\n    year: int\n    scenario: str\n    demand_elasticity: float\n\n# Start run with all three\nresult = tracker.start_run(\n    \"baseline_2030\",\n    config=SimConfig(\n        year=2030,\n        scenario=\"baseline\",\n        demand_elasticity=0.8\n    ),\n    facet={\n        # Queryable fields: year and scenario\n        \"year\": 2030,\n        \"scenario\": \"baseline\",\n    },\n    hash_inputs=[\n        # Large external configs hashed for cache key but not stored\n        Path(\"./activitysim\"),\n        Path(\"./network_config.yaml\")\n    ]\n)\n</code></pre> <p>Result: - Cache key includes: config hash + ActivitySim hash + network hash - Database stores only the small facet (year, scenario) - You can query by year/scenario; ActivitySim changes tracked via fingerprints</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#database-tables","level":4,"title":"Database Tables","text":"<p>Consist uses these tables for configuration tracking:</p> <ul> <li><code>config_facet</code>: Deduplicated facet JSON keyed by a canonical hash, namespaced by run model</li> <li><code>run_config_kv</code>: Flattened, typed key/value index for facet filtering</li> </ul> <p>Note: Use <code>Run.year</code> (a first-class indexed column) for time-series filtering rather than duplicating year in the facet.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#dual-write-persistence","level":2,"title":"Dual-Write Persistence","text":"<p>Consist maintains two synchronized records for resilience:</p> <pre><code>┌──────────────────┐\n│     Tracker      │\n└────────┬─────────┘\n         │\n    ┌────┴────┬────────────┐\n    ▼         ▼            ▼\n┌────────┐ ┌──────────┐ ┌──────────┐\n│  JSON  │ │  DuckDB  │ │  Events  │\n│Snapshot│ │ Database │ │ Manager  │\n└────────┘ └──────────┘ └──────────┘\n</code></pre> <p>Write order (safety guarantee): 1. Update in-memory model 2. Flush to <code>consist.json</code> (atomic write) ← Source of truth 3. Attempt DB sync (catch errors, log warning, never crash)</p> <p>JSON snapshots (<code>consist.json</code> per run): Portable, human-readable, version-controllable. Each run directory contains a complete record that survives database corruption.</p> <p>DuckDB database: Enables fast queries across runs, artifacts, and lineage. Can be rebuilt from JSON snapshots if needed. Handles concurrent access with retry logic.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#testing-coverage-focus","level":2,"title":"Testing &amp; Coverage Focus","text":"<p>The test suite prioritizes correctness for production workflows and portability:</p> <ul> <li>Cache hydration across run directories (metadata-only, requested/all outputs, missing-input reconstruction, permission/mount issues).</li> <li>Path virtualization and mount resolution (workspace URIs, symlink handling, stale/moved run directories).</li> <li>Persistence resilience (lock retries, constraint conflict handling, JSON snapshot safety).</li> <li>Ingestion and data virtualization (DLT ingestion paths, strict schema validation, hybrid view behavior).</li> <li>CLI/query helpers for inspection workflows (filters, preview error modes).</li> </ul>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#path-virtualization","level":2,"title":"Path Virtualization","text":"<p>Absolute paths break portability. Consist stores relative URIs and resolves them at runtime. For a focused guide, see Mounts &amp; Portability.</p> <pre><code>User logs: /mnt/data/land_use.csv\n           ↓\nTracker detects mount: mounts={\"inputs\": \"/mnt/data\"}\n           ↓\nStored URI: inputs://land_use.csv\n</code></pre> <p>Workspace URIs: For run-specific output directories, Consist mounts the current run's directory as <code>workspace://</code>. Historical paths are resolved via metadata stored in <code>Run.meta[\"_physical_run_dir\"]</code>.</p> <p>This means: - Provenance stays valid when data moves between machines - Teams can share databases without path conflicts - Cloud and local storage can coexist</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#data-virtualization","level":2,"title":"Data Virtualization","text":"","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#sql-views","level":3,"title":"SQL Views","text":"<p>Register SQLModel schemas to query artifacts across runs as unified tables:</p> <pre><code>class Person(SQLModel, table=True):\n    person_id: int = Field(primary_key=True)\n    age: int\n\ntracker = Tracker(schemas=[Person])\n\n# Access the view\nVPerson = tracker.views.Person\n\n# Query across all runs\nquery = select(VPerson).where(VPerson.consist_year == 2030)\n</code></pre> <p>Views automatically include system columns: - <code>consist_run_id</code> — Which run produced this row - <code>consist_scenario_id</code> — Parent scenario identifier - <code>consist_year</code> — Simulation year - <code>consist_artifact_id</code> — Source artifact</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#hybrid-views","level":3,"title":"Hybrid Views","text":"<p>Consist creates \"hybrid\" views that union: - Hot data: Rows ingested into DuckDB tables (fast queries) - Cold data: Parquet/CSV files on disk (no duplication)</p> <p>This lets you query terabytes of simulation output without loading everything into memory.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#matrix-views-n-dimensional","level":3,"title":"Matrix Views (N-Dimensional)","text":"<p>For Zarr/NetCDF arrays, <code>MatrixViewFactory</code> creates lazy xarray Datasets:</p> <ol> <li>Query the artifact catalog for matching Zarr stores</li> <li>Open each store lazily (no data loaded)</li> <li>Concatenate along <code>run_id</code> dimension with <code>year</code>/<code>iteration</code> as coordinates</li> </ol> <pre><code>ds = tracker.matrix.load(\"skim_matrices\", variables=[\"travel_time\"])\n# ds is an xarray.Dataset with dims: (run_id, origin, destination)\n</code></pre>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#container-integration","level":2,"title":"Container Integration","text":"<p>Containers are treated as pure functions where the image digest becomes part of the cache signature:</p> <pre><code>Signature = SHA256(image_digest | command | env | mount_hashes | input_signatures)\n</code></pre> <p>On cache hit: 1. Verify outputs exist (or are in ghost mode) 2. Relink artifacts to current run 3. Hydrate files to requested host paths (copy from cache)</p> <p>On cache miss: 1. Execute container 2. Capture declared outputs 3. Record image digest and mount metadata</p> <p>Backend abstraction supports Docker and Singularity/Apptainer for HPC environments.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#event-hooks","level":2,"title":"Event Hooks","text":"<p>Register callbacks for run lifecycle events:</p> <pre><code>tracker.events.on_run_complete(lambda run: notify_slack(run.id))\ntracker.events.on_run_failed(lambda run, error: log_to_sentry(error))\n</code></pre> <p>Events are emitted but failures in hooks don't crash the run—they're logged and the workflow continues.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"architecture/#context-stack","level":2,"title":"Context Stack","text":"<p>Consist maintains a context-local stack of active trackers, allowing nested contexts and implicit tracker resolution:</p> <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        # consist.log_artifact() finds the active tracker automatically\n        with sc.step(name=\"simulate\"):\n            consist.log_dataframe(df, key=\"results\")  # No tracker= needed\n</code></pre> <p>This enables clean APIs where most functions don't require explicit tracker parameters.</p>","path":["Support & Reference","Architecture"],"tags":[]},{"location":"caching-and-hydration/","level":1,"title":"Caching and Artifact Hydration Patterns","text":"<p>Consist’s core loop is: 1. Declare inputs (files or upstream artifacts) 2. Compute a run signature (code + config + inputs) 3. Identify existing prior results if they exist (caching) 4. Load data from the best available source if they are needed, create them if not (Look on disk first; database when appropriate; run the model if all else fails)</p> <p>This page focuses on practical patterns for making caching reliable, keeping pipelines portable, and choosing when/how artifacts should be “hydrated” (materialized or recovered) for downstream work.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#key-concepts-and-terminology","level":2,"title":"Key Concepts and Terminology","text":"<p>This page uses a few terms very precisely. The goal is to separate provenance objects (Artifacts and Runs) from physical bytes on disk (files/directories) and from database-backed data (DuckDB tables).</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#hydration-vs-materialization-recovering-information-vs-ensuring-files-exist","level":2,"title":"Hydration vs Materialization: Recovering Information vs. Ensuring Files Exist","text":"<p>When Consist finds a cached result, it needs to \"bring that result back\" into your new run. But what does 'bringing back' mean? This section clarifies the distinction between recovering information about a previous result versus ensuring the actual files exist on your current system.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#the-two-types-of-recovery","level":3,"title":"The Two Types of Recovery","text":"<p>Imagine you run a climate simulation in <code>/scratch/simulation_2024/</code> and it produces a 50GB results file. Six months later, you run a new analysis with the same inputs. Consist recognizes a cache hit. Now:</p> <p>Artifact Hydration = Recovering the information about that result: \"This result came from run <code>abc123</code>, it's a Parquet file called <code>results.parquet</code>, and it contains monthly precipitation data.\" Consist creates an <code>Artifact</code> object with the metadata, URIs, and provenance—all the information needed to reference and load the result.</p> <p>Materialization = Ensuring the actual bytes exist on your new system: copying the 50GB <code>results.parquet</code> file from its original location into your new run directory so your analysis code can read it from disk.</p> <p>The key insight: Hydration is fast and automatic; materialization is optional and explicit.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#why-consist-does-this","level":3,"title":"Why Consist Does This","text":"<p>By default, Consist only hydrates metadata. This saves disk space and time: - Metadata recovery is instant (lookup in provenance database) - Files stay in their original location or database - You pay for file copying only if you ask for it</p> <p>This is useful when: - You're running 100 variations of an analysis on the same simulated data—you don't need 100 copies of that 50GB results file - Your downstream code can load from the original file path (via mounted storage or database fallback) - You want minimal disk overhead for large-scale parameter sweeps</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#when-to-materialize-making-cached-files-local","level":3,"title":"When to Materialize: Making Cached Files Local","text":"<p>You'll want to materialize cached outputs (copy bytes to your current run) in a few cases:</p> <ol> <li> <p>Extending a scenario in a new workspace: You cached results in <code>/workspace/2024</code>, now you want to continue analysis in <code>/workspace/2025</code>. Use <code>cache_hydration=\"inputs-missing\"</code> to automatically copy input files your executing steps need.</p> </li> <li> <p>Preparing outputs for external tools: Some tools expect files to exist locally. Use <code>cache_hydration=\"outputs-requested\"</code> with <code>materialize_cached_output_paths</code> to copy specific cached outputs you need. If you're using <code>consist.run(...)</code> or <code>sc.run(...)</code>, use <code>output_paths={...}</code> instead.</p> </li> <li> <p>Ensuring full reproducibility locally: You want all results locally accessible without remote mounts. Use <code>cache_hydration=\"outputs-all\"</code> to copy all cached outputs into a local directory.</p> </li> </ol>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#example-climate-data-workflow","level":3,"title":"Example: Climate Data Workflow","text":"<pre><code>from pathlib import Path\n\n# First run: simulate climate for years 2020-2030, takes 24 hours\nwith tracker.start_run(\n    \"climate_baseline\",\n    model=\"climate_sim\",\n    year=2030,\n    cache_mode=\"overwrite\"\n):\n    results = run_climate_model()  # 50GB output\n    tracker.log_artifact(results, key=\"precip\")\n\n# Later: analysis that only needs metadata about the result\nwith tracker.start_run(\n    \"analyze_trends\",\n    model=\"analysis\",\n    inputs=[cached_precip_artifact],\n    cache_hydration=\"metadata\",  # Default: no copying\n):\n    # Consist hydrated the Artifact object with metadata\n    # You can query properties, compute signatures, check provenance\n    print(f\"Result came from run {cached_precip_artifact.run_id}\")\n\n    # If you need the actual bytes, you explicitly load them:\n    df = consist.load_df(cached_precip_artifact)  # Loads from original location\n\n# Later: need to ensure files exist locally for external tool\nwith tracker.start_run(\n    \"export_for_visualization\",\n    model=\"export\",\n    cache_hydration=\"outputs-requested\",\n    materialize_cached_output_paths={\n        \"precip\": Path(\"./local_outputs/precip.parquet\")\n    }\n):\n    # Consist copies the cached precip.parquet into ./local_outputs/\n    # Now your external tool can see real files\n    subprocess.run([\"qgis\", \"export_for_visualization.py\"])\n</code></pre>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#default-behavior-metadata-only-hydration","level":3,"title":"Default Behavior: Metadata-Only Hydration","text":"<p>By default, <code>cache_hydration=\"metadata\"</code>: - Fast: no filesystem operations - Efficient: no disk duplication - Requires explicit <code>consist.load_df()</code> or <code>consist.load()</code> when you need bytes</p> <p>This is the right default for scientific workflows because: - You often analyze cached results programmatically (via <code>consist.load_df()</code> or <code>consist.load()</code>) - Large simulations benefit from avoiding disk copies - Provenance and signature verification don't need bytes on disk</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#practical-decision-table","level":3,"title":"Practical Decision Table","text":"Your Need Use This Tradeoff Run parameter sweeps on cached simulation <code>metadata</code> (default) No disk copy; must load data via <code>consist.load_df()</code>/<code>consist.load()</code> Extend analysis in a new workspace <code>inputs-missing</code> Copies only missing input files; fast on cache hits Run external tool that needs local files <code>outputs-requested</code> Copy only what you ask for; must list outputs explicitly Ensure all cached data is accessible locally <code>outputs-all</code> Copies everything; uses disk space but guarantees file paths exist","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#runs-signatures-and-cache-hits","level":3,"title":"Runs, signatures, and cache hits","text":"<p>Every run has a signature derived from: - code hash (tracked via <code>IdentityManager</code>) - config hash - input hash (derived from declared input artifacts)</p> <p>If Consist finds a previously completed run with the same signature and its outputs are still valid, the new run becomes a cache hit and reuses the prior outputs.</p> <p>Cache behavior is controlled via <code>cache_mode</code>: - <code>reuse</code> (default): reuse matching completed runs - <code>overwrite</code>: always execute (no cache lookup) - <code>readonly</code>: avoid persisting changes (useful for inspection)</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#artifact-identity-vs-artifact-bytes","level":3,"title":"Artifact identity vs. artifact bytes","text":"<p>Consist distinguishes: - provenance identity: where an artifact came from (e.g., <code>artifact.run_id</code> links to the producing run) - physical bytes: the actual on-disk file contents</p> <p>For Consist-produced artifacts, inputs are typically hashed by linking to the producing run’s signature (Merkle-style). For raw files (no <code>run_id</code>), Consist hashes file contents/metadata depending on configuration. If you already know the content hash (for example, after copying or moving a file), you can pass <code>content_hash=</code> to <code>log_artifact</code> to reuse it and skip hashing the path on disk. To avoid accidentally mutating existing provenance, Consist ignores mismatched overrides unless <code>force_hash_override=True</code>. Use <code>validate_content_hash=True</code> to verify the override against on-disk data.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#portability-and-path-resolution","level":3,"title":"Portability and path resolution","text":"<p>An artifact has a portable <code>artifact.container_uri</code> and a runtime-resolved <code>artifact.path</code>. - <code>tracker.resolve_uri(uri)</code> translates a portable URI into a host filesystem path using mounts. - On cache hits, Consist performs artifact hydration: it attaches cached output <code>Artifact</code>   objects to the active run context so downstream code can reference them.</p> <p>This is what keeps pipelines portable across different machines/scratch directories: code consumes <code>Artifact</code> objects and resolves paths through the tracker instead of embedding absolute paths.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#pattern-1-step-caching-with-explicit-artifact-handoff","level":2,"title":"Pattern 1: Step caching with explicit artifact handoff","text":"<p>For multi-step workflows, the most robust pattern is: - each step logs outputs as artifacts - the next step declares those artifacts as inputs</p> <p>Using <code>consist.scenario(...)</code>:</p> <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    with consist.scenario(\"my_scenario\") as scenario:\n        with scenario.trace(\"ingest\"):\n            consist.log_artifact(\"raw.csv\", key=\"raw\", direction=\"output\")\n\n        # `inputs=[...]` declares step inputs by Coupler key without repeating\n        # `scenario.coupler.require(...)` in the `inputs=[...]` list.\n        with scenario.trace(\"transform\", inputs=[\"raw\"]):\n            raw_art = scenario.coupler.require(\"raw\")\n            df = consist.load_df(raw_art, tracker=tracker)\n</code></pre> <p>Notes: - Prefer <code>scenario.coupler.require(\"raw\")</code> over <code>get(\"raw\")</code> in tests and templates; it fails loudly if a predecessor didn’t set the key. - Always pass upstream artifacts through <code>inputs=[...]</code> so caching and provenance are correct. - Step bodies still execute on cache hits; Consist does not “skip the Python block”.   - To make cache-agnostic step code easier to write, <code>tracker.log_artifact(..., direction=\"output\")</code> will return the hydrated cached output when the current run is a cache hit and the output <code>key</code> matches a cached output.   - If code attempts to produce a new or different output on a cache hit, Consist demotes the cache hit to an executing run (so provenance remains truthful).</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#function-shaped-steps-skip-on-cache-hit","level":3,"title":"Function-shaped steps (skip on cache hit)","text":"<p>If you have an expensive callable (including imported functions or bound methods) and you want Consist to skip executing it on cache hits, use <code>ScenarioContext.run(...)</code>.</p> <p><code>output_paths={...}</code> values are interpreted as: - relative paths → relative to the step run directory (<code>t.run_dir</code>) - URI-like strings (<code>\"scheme://...\"</code>) → resolved via mounts (<code>tracker.resolve_uri</code>) - absolute paths → used as-is</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#pattern-2-cached-runs-with-consistrun","level":2,"title":"Pattern 2: Cached runs with <code>consist.run</code>","text":"<p>Use <code>consist.run(...)</code> with declared outputs to get cache-aware execution for function-shaped steps.</p> <pre><code>def clean(raw_file: Path):\n    ...\n    return {\"cleaned\": cleaned_df}\n\nimport consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean,\n        inputs={\"raw_file\": Path(\"raw.csv\")},\n        outputs=[\"cleaned\"],\n        load_inputs=True,\n    )\n</code></pre>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#pattern-3-resuming-after-failures-partial-pipeline-reuse","level":2,"title":"Pattern 3: Resuming after failures (partial pipeline reuse)","text":"<p>A common operational workflow is: - early steps succeed (and are cached) - a late step fails (e.g., ENOSPC / transient compute failure) - re-run should reuse upstream cached results and continue from the failure point</p> <p>This works when: - upstream outputs are logged as artifacts - downstream steps declare those artifacts as inputs - failed runs are not reused for caching (Consist only reuses completed runs)</p> <p>In practice: - re-running will cache-hit earlier steps - the failed step will re-execute because there is no valid completed run to reuse</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#pattern-4-hot-vs-cold-data-and-database-backed-recovery","level":2,"title":"Pattern 4: Hot vs. cold data, and database-backed recovery","text":"<p>Ingestion and hybrid views are covered in the dedicated guide: Ingestion &amp; Hybrid Views. That doc explains when to ingest, how schema validation works, and how DB fallback behaves when files are missing.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#what-exactly-happens-on-a-cache-hit","level":2,"title":"What exactly happens on a cache hit?","text":"<p>This is the most common source of confusion, so here is the explicit behavior.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#core-tracker-cache-hits-tasksscenarios","level":3,"title":"Core <code>Tracker</code> cache hits (tasks/scenarios)","text":"<p>On a cache hit, Consist: - Finds a matching completed prior run with the same signature. - By default, skips filesystem checks for cached outputs for speed.   - Pass <code>validate_cached_outputs=\"eager\"</code> if you want to require that output files exist (or are ingested). - Hydrates artifact objects for the cached outputs into the current run context.   - Paths are resolved on demand (e.g., <code>artifact.path</code>).   - <code>Artifact.path</code> lazily resolves via the active tracker; no filesystem checks happen until you access it.</p> <p>On a cache hit, Consist does not: - Copy files into a new run directory. - Recreate missing files from DuckDB automatically. - Guarantee that <code>artifact.path</code> exists on disk without an explicit load/materialization step.</p> <p>If you need bytes, you typically: - call <code>consist.load_df(...)</code> (or <code>consist.load(...)</code> for Relations) for tabular artifacts (disk first; see Ingestion &amp; Hybrid Views for DB fallback), or - implement explicit file copy/materialization for non-tabular artifacts.</p> <p>If you want to work with a DuckDB Relation directly, use <code>consist.load_relation(...)</code> as a context manager to ensure the underlying DuckDB connection is closed. If you keep Relations alive in memory, Consist will warn once the active relation count crosses the <code>CONSIST_RELATION_WARN_THRESHOLD</code> (default 100).</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#cache-misses-that-depend-on-cached-inputs","level":3,"title":"Cache misses that depend on cached inputs","text":"<p>If a run is not a cache hit but its inputs include artifacts produced by cached runs, those input artifacts are still referenced by portable URIs (e.g., <code>./outputs/foo.parquet</code>). By default, Consist resolves those URIs into the current run directory, which may not contain the bytes from the original run.</p> <p>To make “extend a scenario in a new run_dir” workflows work without rerunning cached steps, use the cache hydration policy:</p> <pre><code>with tracker.start_run(\n    \"next_step\",\n    model=\"step\",\n    inputs=[cached_artifact],\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n</code></pre> <p>With <code>cache_hydration=\"inputs-missing\"</code>, Consist will: - detect missing input paths in the current run_dir, - locate the original run directory recorded in provenance metadata, and - copy those input bytes into the current run_dir before execution.</p> <p>This keeps cache misses fast while still ensuring the executing step can read its inputs.</p> <p>If the original on-disk input is missing but the artifact was ingested, Consist will attempt to reconstruct the input from DuckDB for CSV and Parquet artifacts only. Other drivers raise a <code>ValueError</code> to avoid silent or lossy conversions.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#optional-cache-hit-materialization-copy-bytes-on-disk","level":3,"title":"Optional cache-hit materialization (copy bytes on disk)","text":"<p>Core Consist supports an opt-in policy to physically materialize cached outputs on cache hits by copying bytes from the cached artifact’s resolved path to a destination you provide.</p> <p>This is intentionally copy-only materialization (Phase 1): - It copies from the cached source path if it exists. - It does not reconstruct files from DuckDB.</p> <p>Enable it at run start:</p> <pre><code>with tracker.start_run(\n    \"r2\",\n    model=\"step\",\n    cache_hydration=\"outputs-all\",\n    materialize_cached_outputs_dir=Path(\"some/output/dir\"),\n):\n    ...\n</code></pre> <p>Or materialize only specific cached outputs (by artifact key):</p> <pre><code>with tracker.start_run(\n    \"r2\",\n    model=\"step\",\n    cache_hydration=\"outputs-requested\",\n    materialize_cached_output_paths={\"features\": Path(\"outputs/features.csv\")},\n):\n    ...\n</code></pre> <p>Equivalent with <code>consist.run(...)</code>:</p> <pre><code>result = consist.run(\n    fn=step,\n    cache_hydration=\"outputs-requested\",\n    output_paths={\"features\": Path(\"outputs/features.csv\")},\n)\n</code></pre> <p>Defaults remain unchanged: <code>cache_hydration=\"metadata\"</code> (artifact hydration only).</p> <p>Note: for <code>consist.run(...)</code> or <code>sc.run(...)</code>, use <code>output_paths={...}</code> with <code>cache_hydration=\"outputs-requested\"</code> to specify where requested outputs should be copied.</p> <p>Validation rules (error cases): - <code>outputs-requested</code> requires <code>materialize_cached_output_paths</code> (or <code>output_paths</code> for <code>consist.run(...)</code>) and rejects <code>materialize_cached_outputs_dir</code>. - <code>outputs-all</code> requires <code>materialize_cached_outputs_dir</code> and rejects <code>materialize_cached_output_paths</code>. - <code>metadata</code>/<code>inputs-missing</code> reject both materialization args.</p> <p>Operational notes: - <code>outputs-requested</code> will log a warning if you ask for keys that are not present in the cached outputs. - <code>outputs-all</code> raises <code>FileNotFoundError</code> if any cached output source file is missing.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#containers-integration-cache-hits-requested-outputs","level":3,"title":"Containers integration cache hits (requested outputs)","text":"<p>The containers API is intentionally different: on a cache hit it attempts to copy cached outputs to the requested host output paths so downstream tooling sees real files. Internally this now uses the same copy-based materialization helper as core runs (policy concept is shared), but containers default to “requested outputs” because the caller explicitly provided host output paths.</p> <p>This is an integration-level choice for ergonomics; it should not be assumed for core <code>Tracker</code> cache hits.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#hydration-policy-table","level":2,"title":"Hydration policy table","text":"Workflow need <code>cache_hydration</code> When bytes are copied Notes Fast cache hits, metadata only <code>metadata</code> Never Default; paths may not exist in new run dirs. Extend a scenario in a new <code>run_dir</code> <code>inputs-missing</code> On cache misses, for missing inputs only Ensures executing steps can read cached inputs. Materialize a few outputs on cache hits <code>outputs-requested</code> On cache hits, for requested outputs Requires <code>materialize_cached_output_paths</code> (or <code>output_paths</code> for <code>consist.run(...)</code>). Make all cached outputs exist locally <code>outputs-all</code> On cache hits, for all outputs Requires <code>materialize_cached_outputs_dir</code>. <p>Most users should stick with the default <code>metadata</code> mode. Use the other modes only when downstream tools require the actual files to exist on disk.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#practical-guidance","level":2,"title":"Practical Guidance","text":"","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#make-caching-predictable","level":3,"title":"Make caching predictable","text":"<ul> <li>Declare all true dependencies as inputs (<code>inputs=[...]</code> or task parameters).</li> <li>Keep configs deterministic (avoid embedding timestamps/randomness unless intended).</li> <li>Prefer stable code identity in tests (Consist test suite patches the code hash for determinism).</li> </ul>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#decide-what-to-ingest","level":3,"title":"Decide what to ingest","text":"<p>Ingest when you want: - queryable “hot” tables in DuckDB - schema profiling/validation - optional recovery if files are deleted</p> <p>Keep as cold files when: - you don’t want to duplicate storage in DuckDB - artifacts are huge or already live in an external system</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#safe-deletion-and-long-running-pipelines","level":3,"title":"Safe deletion and long-running pipelines","text":"<p>Deleting old artifacts can be safe when: - you’re confident they will not be loaded again, or - they were ingested and you intentionally allow DB recovery (usually by declaring them as inputs in a non-cached run, or by using <code>db_fallback=\"always\"</code> in explicit analysis tooling).</p> <p>If you want to aggressively GC the workspace while keeping provenance usable, ingestion + intentional DB fallback is the strategy to reach for.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#cache-hit-performance-and-diagnostics","level":2,"title":"Cache-Hit Performance and Diagnostics","text":"<p>Consist’s cache-hit path is optimized to minimize filesystem work by default. The main knobs are:</p> <ul> <li><code>validate_cached_outputs=\"lazy\"</code> (default) skips per-output existence checks.</li> <li><code>cache_hydration=\"metadata\"</code> (default) avoids copying bytes for cached outputs.</li> </ul>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#cache-and-hydration-knobs-summary","level":3,"title":"Cache and hydration knobs (summary)","text":"<ul> <li><code>cache_mode</code>:</li> <li><code>reuse</code> (default): reuse cached runs when signatures match.</li> <li><code>overwrite</code>: always execute and replace cached outputs.</li> <li><code>readonly</code>: read cache but avoid persisting new outputs.</li> <li><code>validate_cached_outputs</code>:</li> <li><code>lazy</code> (default): skip filesystem checks for cached outputs.</li> <li><code>eager</code>: require output files to exist (or be ingested) for a cache hit.</li> <li><code>cache_hydration</code>:</li> <li><code>metadata</code> (default): hydrate artifact objects only; no bytes copied.</li> <li><code>inputs-missing</code>: on cache misses, copy missing input bytes into the current <code>run_dir</code>.</li> <li><code>outputs-requested</code>: on cache hits, copy only specified cached outputs to requested paths.</li> <li><code>outputs-all</code>: on cache hits, copy all cached outputs into a target directory.</li> <li>Scenario default: pass <code>step_cache_hydration=\"inputs-missing\"</code> to <code>consist.scenario(...)</code>   to apply a default policy to all steps unless overridden.</li> <li><code>materialize_cached_output_paths</code>: required for <code>outputs-requested</code> (or <code>output_paths</code> for <code>consist.run(...)</code>).</li> <li><code>materialize_cached_outputs_dir</code>: required for <code>outputs-all</code>.</li> </ul> <p>For large iterative workflows, most cache-hit time is spent on:</p> <ul> <li>fetching cached outputs from the database</li> <li>hashing raw-file inputs (when inputs are not Consist-produced artifacts)</li> </ul> <p>You can enable lightweight diagnostics to understand where time is going:</p> <ul> <li><code>CONSIST_CACHE_TIMING=1</code> logs timing for cache-hit phases during <code>begin_run</code>   (signature prefetch, input hashing, cache lookup, validation, hydration).</li> <li><code>CONSIST_CACHE_DEBUG=1</code> logs cache hit/miss details and signatures.</li> </ul> <p>Example output with <code>CONSIST_CACHE_TIMING=1</code>:</p> <pre><code>[cache_timing] signature_prefetch=2.1ms input_hashing=145.8ms cache_lookup=3.4ms validate=0.6ms hydration=0.2ms\n</code></pre> <p>Interpretation: input hashing dominates, so consider ingesting large tabular inputs or passing Consist-produced artifacts instead of raw files.</p> <p>Example output with <code>CONSIST_CACHE_DEBUG=1</code>:</p> <pre><code>[cache_debug] hit=True signature=7e9c1c inputs=3 outputs=2 hydration=metadata\n</code></pre> <p>Interpretation: the cache hit returned metadata only; if downstream tools need files, select an outputs hydration mode.</p> <p>If you need strict safety for missing outputs, use <code>validate_cached_outputs=\"eager\"</code>, accepting the additional filesystem checks.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#pattern-5-shared-input-bundles-via-a-duckdb-file","level":2,"title":"Pattern 5: Shared input bundles via a DuckDB file","text":"<p>If you want collaborators to start with all required inputs already packaged in a standalone DuckDB file, use a dedicated \"bundle\" run that logs inputs as outputs and ingests them. Sharing the DB file is enough to reproduce inputs without shipping the original raw files.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#bundle-creation-producer","level":3,"title":"Bundle creation (producer)","text":"<pre><code>tracker = Tracker(run_dir=\"bundle_build\", db_path=\"inputs.duckdb\")\n\nwith tracker.start_run(\"inputs_bundle_v1\", model=\"input_bundle\", cache_mode=\"overwrite\"):\n    households = tracker.log_artifact(\"households.csv\", key=\"households\", direction=\"output\")\n    persons = tracker.log_artifact(\"persons.csv\", key=\"persons\", direction=\"output\")\n    tracker.ingest(households)\n    tracker.ingest(persons)\n</code></pre> <p>Packaging steps: 1. Run the bundle creation code once. 2. Share the resulting DuckDB file (<code>inputs.duckdb</code>) with collaborators. 3. Collaborators only need the DB file; they do not need the original CSV files.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#bundle-consumption-consumer","level":3,"title":"Bundle consumption (consumer)","text":"<pre><code>tracker = Tracker(run_dir=\"runs\", db_path=\"inputs.duckdb\")\n\nbundle_outputs = tracker.load_input_bundle(\"inputs_bundle_v1\")\ninputs = list(bundle_outputs.values())\n\nwith tracker.start_run(\n    \"simulate\",\n    model=\"simulate\",\n    inputs=inputs,\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n</code></pre> <p>Notes: - The bundle is identified only by run_id; no hard-coded hashes are needed. - <code>cache_hydration=\"inputs-missing\"</code> will reconstruct CSV/Parquet inputs from the   shared DuckDB file if the original bytes are missing.</p>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-and-hydration/#multi-model-workflows-from-one-bundle","level":3,"title":"Multi-model workflows from one bundle","text":"<p>Bundles can contain inputs for multiple downstream steps. Each run selects only the inputs it needs:</p> <pre><code>bundle_outputs = tracker.load_input_bundle(\"inputs_bundle_v1\")\n\nwith tracker.start_run(\n    \"model_a\",\n    model=\"model_a\",\n    inputs=[bundle_outputs[\"input_a\"]],\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n\nwith tracker.start_run(\n    \"model_b\",\n    model=\"model_b\",\n    inputs=[bundle_outputs[\"input_b\"]],\n    cache_hydration=\"inputs-missing\",\n):\n    ...\n</code></pre>","path":["Core Topics","Caching and Artifact Hydration Patterns"],"tags":[]},{"location":"caching-fundamentals/","level":1,"title":"How Caching Works","text":"<p>Consist uses intelligent caching to skip redundant computation. This page explains the core mechanism.</p>","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"caching-fundamentals/#the-basic-idea","level":2,"title":"The Basic Idea","text":"<p>Consist computes a fingerprint (signature) from three components:</p> <ol> <li>Your function's code – Git commit hash + tracked local Python modifications</li> <li>Configuration – The <code>config</code> dict you pass to <code>consist.run()</code></li> <li>Input files – Content or metadata hashes of files in the <code>inputs</code> dict (depends on <code>hashing_strategy</code>)</li> </ol> <p>If you run the same function with the same code, config, and inputs, the signature is identical. When Consist sees an identical signature, it returns the cached result from a previous run instead of re-executing.</p>","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"caching-fundamentals/#example","level":2,"title":"Example","text":"<pre><code>import consist\nfrom consist import Tracker, use_tracker\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\nwith use_tracker(tracker):\n    # First run\n    result1 = consist.run(\n        fn=clean_data,\n        inputs={\"raw_df\": \"data.csv\"},      # Hash of data.csv\n        config={\"threshold\": 0.5},          # Hash of this config\n        outputs=[\"cleaned\"],\n    )\n# Signature: SHA256(\"clean_data_v1\" + \"threshold:0.5\" + \"data.csv_hash\")\n# Output: executed, returned new result\n\n\n    # Second run with identical inputs/config\n    result2 = consist.run(\n        fn=clean_data,\n        inputs={\"raw_df\": \"data.csv\"},      # Same hash\n        config={\"threshold\": 0.5},          # Same hash\n        outputs=[\"cleaned\"],\n    )\n# Signature: same as above\n# Output: cache hit! Result returned instantly without execution\n\n\n    # Third run with different config\n    result3 = consist.run(\n        fn=clean_data,\n        inputs={\"raw_df\": \"data.csv\"},      # Same hash\n        config={\"threshold\": 0.8},          # Different hash!\n        outputs=[\"cleaned\"],\n    )\n# Signature: different (config changed)\n# Output: cache miss, function executes, new run recorded\n</code></pre>","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"caching-fundamentals/#what-changes-break-cache-hits","level":2,"title":"What Changes Break Cache Hits?","text":"What Changed Cache Hit? Why Input file content ❌ No File hash changes → signature changes Config value ❌ No Config hash changes → signature changes Function code ❌ No Code hash changes → signature changes <code>runtime_kwargs</code> ✅ Yes runtime_kwargs are NOT hashed; don't affect signature Output file names ✅ Yes Output names don't affect signature Comments in code Depends Committed comment changes affect the code hash; uncommitted changes mark the repo dirty and break cache.","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"caching-fundamentals/#what-does-cache-return","level":2,"title":"What Does Cache Return?","text":"<p>When there's a cache hit, Consist returns:</p> <ul> <li>Artifact metadata – Information about what run created the output, with what config</li> <li>File paths – Where the output was stored</li> <li>Optionally, file bytes – Depend on your cache hydration policy (see Caching &amp; Hydration for advanced options)</li> </ul> <p>Important: A cache hit returns metadata about the result, not necessarily a copy of the files.</p>","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"caching-fundamentals/#common-misconception","level":2,"title":"Common Misconception","text":"<p>\"Cache hit means the output files are copied to my new run directory.\"</p> <p>Not necessarily. A cache hit returns artifact metadata (provenance). Whether file bytes are copied depends on your cache hydration policy. By default: - File paths are preserved (you can access the original file) - Bytes are not copied (saves disk space)</p> <p>See Caching &amp; Hydration if you need to force file copying.</p>","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"caching-fundamentals/#when-caching-saves-time","level":2,"title":"When Caching Saves Time","text":"<p>Caching is most valuable in workflows with many runs and expensive computation. Here are realistic scenarios from scientific domains:</p> <p>Example 1: Land-Use Model Sensitivity Analysis</p> <p>Transportation planners use activity-based models to evaluate how pricing policies affect commute patterns. A sensitivity sweep tests 40 parameter combinations (toll levels: 0–$10, parking costs: $2-$15, transit pass subsidies: 0–50%).</p> <ul> <li>Each ActivitySim run: 30 minutes (generating synthetic population tours)</li> <li>Without caching: 40 runs × 30 min = 1200 minutes (20 hours)</li> <li>With caching: Base population synthesis (30 min, once) + 39 parameter tweaks with cache hits (5 min each, only trip mode choice re-run) = 30 + (39 × 5) = 225 minutes (3.75 hours)</li> <li>Time saved: 81% reduction in modeling time</li> </ul> <p>Example 2: ActivitySim Calibration Iteration</p> <p>Mode choice coefficients need iterative calibration against observed transit ridership. A modeler: 1. Runs population synthesis (45 minutes, computationally heavy) 2. Generates tours (20 minutes) 3. Runs mode choice with initial coefficients (10 minutes)</p> <p>After reviewing results, the coefficients are adjusted slightly and the model reruns.</p> <ul> <li>Without caching: Repeat all 3 steps = 75 minutes per iteration × 5 iterations = 375 minutes total</li> <li>With caching: Step 1–2 are cache hits (data unchanged), only step 3 re-executes = 45 + 20 (cached) + (10 × 5 iterations) = 115 minutes</li> <li>Time saved: 69% reduction; frees analyst time for interpretation</li> </ul> <p>Example 3: Climate Change Multi-Scenario Ensemble</p> <p>Climate researchers downscale global circulation models (GCMs) for regional impact studies. A baseline scenario and 8 future scenarios (4 emissions pathways × 2 time horizons) all share the same preprocessing pipeline.</p> <ul> <li>Baseline preprocessing (temperature interpolation, bias correction): 2 hours</li> <li>Each scenario-specific downscaling: 15 minutes</li> <li>Without caching: 9 × (2 hours + 15 min) = 20.25 hours</li> <li>With caching: Preprocessing once (2 hours), then 8 scenario runs hit cache on preprocessing = 2 hours + (8 × 15 min) = 3 hours</li> <li>Time saved: 85% reduction; enables rapid ensemble exploration for stakeholder analysis</li> </ul>","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"caching-fundamentals/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>See Caching &amp; Hydration for advanced policies (when/how to copy files, handling large datasets)</li> <li>See Configuration &amp; Facets to learn when to use <code>config</code> vs <code>facet</code></li> <li>See Usage Guide for multi-step workflow patterns</li> </ul>","path":["Getting Started","How Caching Works"],"tags":[]},{"location":"cli-reference/","level":1,"title":"CLI Reference","text":"<p>Consist provides command-line tools to inspect, query, and compare runs and artifacts without writing Python code. Use it to answer “what ran, with what inputs, and what changed?” directly from your provenance database.</p> <p>This is especially useful when you are SSH’d into a remote server or working in a headless environment: you can quickly explore runs, artifacts, and lineage from the shell without starting a Python session. </p> <p>In particular, the <code>consist shell</code> command creates a persistent <code>Tracker</code> object linked to a database and allows you to query run inputs and outputs and artifact metadata, producing nicely formatted tables and summaries in the terminal.</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#database-discovery","level":2,"title":"Database Discovery","text":"<p>The CLI looks for the provenance database in this order: 1. Explicit <code>--db-path</code> argument 2. <code>CONSIST_DB</code> environment variable 3. <code>provenance.duckdb</code> in the current directory 4. Common subdirectories (<code>./data/</code>, <code>./db/</code>, <code>./.consist/</code>)</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#commands","level":2,"title":"Commands","text":"","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-runs","level":3,"title":"consist runs","text":"<p>List recent runs with optional filters.</p> <pre><code>consist runs                          # Last 10 runs\nconsist runs --limit 20               # Last 20 runs\nconsist runs --model travel_demand    # Filter by model name\nconsist runs --status completed       # Filter by status\nconsist runs --tag simulation         # Filter by tag\nconsist runs --json                   # JSON output for scripting\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-show","level":3,"title":"consist show","text":"<p>Display detailed information about a specific run.</p> <pre><code>consist show &lt;run_id&gt;\n</code></pre> <p>Shows run metadata, configuration, status, duration, and any custom metadata fields.</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-artifacts","level":3,"title":"consist artifacts","text":"<p>List input and output artifacts for a run.</p> <pre><code>consist artifacts &lt;run_id&gt;\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-lineage","level":3,"title":"consist lineage","text":"<p>Trace the full provenance chain for an artifact.</p> <pre><code>consist lineage &lt;artifact_key_or_id&gt;\n</code></pre> <p>Displays a tree showing which runs and inputs produced the artifact.</p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-scenarios","level":3,"title":"consist scenarios","text":"<p>List all scenarios and their run counts.</p> <pre><code>consist scenarios\nconsist scenarios --limit 50\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-scenario","level":3,"title":"consist scenario","text":"<p>Show all runs within a specific scenario.</p> <pre><code>consist scenario &lt;scenario_id&gt;\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-search","level":3,"title":"consist search","text":"<p>Search runs by ID, model name, or scenario.</p> <pre><code>consist search \"baseline\"\nconsist search \"travel\" --limit 50\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-summary","level":3,"title":"consist summary","text":"<p>Display database statistics: total runs, artifacts, date range, and runs per model.</p> <pre><code>consist summary\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-preview","level":3,"title":"consist preview","text":"<p>Preview tabular artifacts (CSV, Parquet) without loading full data.</p> <pre><code>consist preview &lt;artifact_key&gt;\nconsist preview &lt;artifact_key&gt; --rows 10\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-validate","level":3,"title":"consist validate","text":"<p>Check that artifacts in the database exist on disk.</p> <pre><code>consist validate\nconsist validate --fix  # Mark missing artifacts\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#consist-shell","level":3,"title":"consist shell","text":"<p>Start an interactive session for exploring provenance.</p> <pre><code>consist shell\n</code></pre> <p>Inside the shell: <pre><code>(consist) runs --limit 5\n(consist) show abc123\n(consist) artifacts abc123\n(consist) scenarios\n(consist) exit\n</code></pre></p>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"cli-reference/#scripting-with-json-output","level":2,"title":"Scripting with JSON Output","text":"<p>Use <code>--json</code> for machine-readable output:</p> <pre><code>consist runs --json | jq '.[] | select(.status == \"completed\")'\n</code></pre>","path":["Getting Started","CLI Reference"],"tags":[]},{"location":"concepts/","level":1,"title":"Concepts","text":"<p>This page establishes the core mental model for Consist before diving into API details.</p>","path":["Getting Started","Concepts"],"tags":[]},{"location":"concepts/#core-abstractions","level":2,"title":"Core abstractions","text":"<ul> <li>Run: A single execution with tracked inputs, config, and outputs. Runs are identified by a signature that lets Consist reuse prior results.</li> <li>Artifact: A file produced or consumed by a run. Artifacts carry provenance metadata such as content hash and creator run.</li> <li>Scenario: A collection of related runs (an experiment or study) that lets you compare variants cleanly.</li> <li>Coupler: A lightweight helper that passes artifacts between steps in a scenario so each step records lineage without manual wiring.</li> </ul>","path":["Getting Started","Concepts"],"tags":[]},{"location":"concepts/#how-caching-works","level":2,"title":"How caching works","text":"<p>Consist computes a signature from the code version, config, and input artifact hashes:</p> <pre><code>signature = hash(code_version + config + input_hashes)\n</code></pre> <ul> <li>Same signature: return cached outputs.</li> <li>Different signature: execute and record new lineage.</li> </ul> <p>This means you can safely re-run a workflow with the same inputs and config without re-running work, while still getting new results when anything changes. On cache hits, Consist returns output artifact metadata without copying files; load or hydrate outputs only when you need bytes.</p>","path":["Getting Started","Concepts"],"tags":[]},{"location":"concepts/#the-config-vs-facet-distinction","level":2,"title":"The config vs facet distinction","text":"<p>This is a key concept for making runs both reproducible and queryable.</p> <p>When you configure a run, you make a choice: should this parameter trigger a re-run if it changes (cache invalidation), or should it be searchable so you can find \"all runs where this parameter had that value\"?</p> <p>Config (affects reproducibility): Parameters that change behavior. Consist hashes config values into your run's signature. If config changes, the signature changes, and cached results don't apply—you must re-run. Use this for anything that should invalidate the cache.</p> <p>Facet (enables filtering): Queryable metadata you want to search by. Facets are stored in the database without affecting caching. You can ask \"show all runs where year=2030 and scenario='baseline'\" without storing the entire config. Use this when you want analytics without cache invalidation.</p> <p>Practical example: Suppose you have a 100KB ActivitySim config file. Store the whole file as <code>config=...</code> (it hashes into the signature, so changes trigger re-runs). Then extract small, queryable pieces: <code>facet={\"year\": 2030, \"mode_choice_coefficient\": 0.5}</code>. This lets you search for runs where <code>coefficient &gt; 0.4</code> without bloating the database with raw csv inputs.</p> <p>Quick decision tree: - \"Should changing this parameter re-run my model?\" → <code>config</code> - \"Do I want to search/filter by this value later?\" → <code>facet</code> - \"Both?\" → Use both. A value can be in config (cache invalidation) and facet (queryable).</p> Parameter Affects Cache? Queryable? Use For <code>config=</code> Yes No (by default) Parameters that change behavior <code>facet=</code> No Yes (indexed) Metadata for filtering/grouping <code>facet_from=</code> — Yes Extract specific keys from config for querying","path":["Getting Started","Concepts"],"tags":[]},{"location":"concepts/#how-inputs-and-outputs-are-treated","level":2,"title":"How inputs and outputs are treated","text":"<ul> <li>Inputs are files or values that influence computation. File inputs are hashed by content or metadata depending on the hashing strategy (<code>full</code> vs <code>fast</code>).</li> <li>Outputs are named artifacts you declare when you call <code>consist.run(...)</code> (or <code>tracker.run(...)</code>). Consist stores their paths and provenance metadata for later lookup and querying.</li> </ul> <p>To keep outputs portable, write them under <code>tracker.run_dir</code> or a mounted <code>outputs://</code> root. That keeps artifact paths relative and consistent across machines.</p>","path":["Getting Started","Concepts"],"tags":[]},{"location":"concepts/#input-mappings-and-auto-loading","level":3,"title":"Input mappings and auto-loading","text":"<p>Inputs can be passed as a list (hash-only) or a mapping (hash + parameter injection). When you use a mapping, Consist matches input keys to function parameters and auto-loads those artifacts into the call by default. If you want to pass raw paths instead, set <code>load_inputs=False</code> and pass the paths explicitly (for example, via <code>runtime_kwargs</code>).</p> <p>Concrete example:</p> <pre><code>import pandas as pd\n\ndef summarize_trips(trips_df):\n    return trips_df[\"distance_miles\"].mean()\n\n# Auto-loading: mapping keys match function params, so Consist loads the DataFrame.\nresult = consist.run(\n    fn=summarize_trips,\n    inputs={\"trips_df\": trips_artifact},\n)\n\ndef summarize_trips_from_path(trips_path: str):\n    df = pd.read_parquet(trips_path)\n    return df[\"distance_miles\"].mean()\n\n# No auto-loading: you get the raw path and load it yourself.\nresult = consist.run(\n    fn=summarize_trips_from_path,\n    inputs={\"trips_path\": trips_artifact},\n    load_inputs=False,\n    runtime_kwargs={\"trips_path\": trips_artifact.path},\n)\n</code></pre>","path":["Getting Started","Concepts"],"tags":[]},{"location":"concepts/#when-to-use-each-pattern","level":2,"title":"When to use each pattern","text":"<ul> <li><code>consist.run(...)</code> (or <code>tracker.run(...)</code>) for standalone steps or when calling library functions.</li> <li><code>sc.trace(...)</code> for inline code blocks within a workflow.</li> <li><code>sc.run(...)</code> when you want cache-skip behavior (the function does not execute on cache hit).</li> </ul> <p>If you are unsure, start with <code>consist.run(...)</code> inside a <code>use_tracker(...)</code> scope. It is the simplest pattern and works well for most first-time use cases.</p>","path":["Getting Started","Concepts"],"tags":[]},{"location":"configs/","level":1,"title":"Configuration, Identity, and Facets","text":"<p>This guide explains how Consist handles configuration data, how it impacts run identity hashing, and how to make config data queryable in the database.</p>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#overview","level":2,"title":"Overview","text":"<p>Consist supports three complementary configuration channels per run:</p> <ol> <li>Identity config: <code>config=...</code> (hashed into the run signature)</li> <li>Queryable facet: <code>facet=...</code> (persisted in DuckDB and optionally indexed)</li> <li>Hash-only inputs: <code>hash_inputs=[...]</code> (file/dir digests folded into identity)</li> </ol> <p>Use <code>config</code> for the full run configuration, <code>facet</code> for a small subset of fields you want to query, and <code>hash_inputs</code> for external config trees or files that should affect caching without being stored as structured metadata.</p>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#api-summary","level":2,"title":"API Summary","text":"<p>You can pass config parameters via the high-level APIs:</p> <ul> <li><code>consist.run(...)</code> / <code>consist.trace(...)</code></li> <li><code>Tracker.run(...)</code> / <code>Tracker.trace(...)</code> (explicit tracker form)</li> <li><code>ScenarioContext.run(...)</code></li> <li><code>ScenarioContext.trace(...)</code></li> </ul> <p>Relevant arguments:</p> <ul> <li><code>config: dict | BaseModel | None</code></li> <li><code>config_plan: ConfigPlan | None</code></li> <li><code>facet: dict | BaseModel | None</code></li> <li><code>facet_from: list[str] | None</code></li> <li><code>hash_inputs: list[Path | str | (label, Path|str)] | None</code></li> <li><code>facet_schema_version: str|int|None</code></li> <li><code>facet_index: bool</code> (default <code>True</code>)</li> </ul>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#identity-hashing","level":2,"title":"Identity Hashing","text":"<p>A run signature is derived from:</p> <ul> <li>the config hash</li> <li>the input hash (logged inputs)</li> <li>the code hash</li> </ul> <p>The config hash includes:</p> <ul> <li>the <code>config</code> payload (normalized JSON)</li> <li>select run fields (<code>model</code>, <code>year</code>, <code>iteration</code>) under a reserved   <code>__consist_run_fields__</code> key to avoid cache hits across distinct runs</li> </ul> <p>This means changing <code>model</code>, <code>year</code>, or <code>iteration</code> will change the run signature even if <code>config</code> is unchanged.</p>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#identity-config-config","level":2,"title":"Identity Config (<code>config</code>)","text":"<ul> <li>Drives <code>Run.config_hash</code> and cache identity.</li> <li>Stored in the per-run <code>consist.json</code> snapshot under <code>config</code>.</li> <li>Not stored as structured/queryable data in DuckDB.</li> </ul> <p>If you pass a Pydantic model, it is serialized via <code>model_dump()</code> for hashing and JSON snapshotting.</p>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#facets-facet-and-facet_from","level":2,"title":"Facets (<code>facet</code> and <code>facet_from</code>)","text":"<p>Facets are compact, queryable config subsets persisted to DuckDB. They are intended to be small, stable, and useful for filtering.</p> <p>Facet sources:</p> <ul> <li>Explicit <code>facet=...</code></li> <li><code>facet_from=[\"key\", ...]</code> extracts top-level keys from <code>config</code></li> <li><code>to_consist_facet()</code> on a Pydantic config model (if implemented)</li> </ul> <p>Behavior notes:</p> <ul> <li>Facets are only persisted when explicitly provided, extracted via   <code>facet_from</code>, or produced by <code>to_consist_facet()</code>.</li> <li><code>facet_from</code> raises <code>KeyError</code> if any key is missing in <code>config</code>.</li> <li>If both <code>facet</code> and <code>facet_from</code> are provided, the extracted values are   merged into the explicit facet, and explicit keys win.</li> <li>If <code>facet_schema_version</code> is not provided and the config model exposes   <code>facet_schema_version</code>, Consist records that value automatically.</li> </ul> <p>Guardrails:</p> <ul> <li>Facets larger than 16 KB (canonical JSON) are not persisted.</li> <li>Facets that would produce more than 500 KV rows are not indexed.</li> </ul>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#hash-only-inputs-hash_inputs","level":2,"title":"Hash-Only Inputs (<code>hash_inputs</code>)","text":"<p>Use <code>hash_inputs</code> to include file or directory content in identity hashing without logging inputs in provenance.</p> <ul> <li>Files are hashed via the configured <code>hashing_strategy</code>.</li> <li>Directories are hashed deterministically via a sorted walk.</li> <li>Dotfiles are ignored by default.</li> </ul> <p><code>hash_inputs</code> is path-only (files or directories). Artifacts are intentionally excluded to keep identity hashing deterministic and avoid implicit hydration.</p> <p>Digests are recorded in:</p> <ul> <li><code>config[\"__consist_hash_inputs__\"]</code> (identity-only payload)</li> <li><code>run.meta[\"consist_hash_inputs\"]</code> (audit/debugging)</li> </ul>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#when-to-use-hash-only-inputs","level":3,"title":"When to Use Hash-Only Inputs","text":"<p><code>hash_inputs</code> solves a specific problem: Configuration files that must affect the cache key but are too large or unstructured to query in the database.</p> <p>The Trade-off</p> <p>Normally, you pass configuration two ways:</p> <ol> <li>Via <code>config=</code> — Stored in the JSON run snapshot and hashed into identity, but not queryable by default.</li> <li>Not tracked — Smaller footprint, but changes to external files don't invalidate the cache, risking incorrect cache hits.</li> </ol> <p><code>hash_inputs</code> is the middle ground: Hash the files so cache keys change when they do, but don't store the content.</p> <p>Example: ActivitySim Configuration</p> <p>ActivitySim projects use a 5MB+ directory of YAML and csv configuration files. The config affects simulation output, so it must change the cache signature. But the YAML is: - Too large to store as a queryable facet (exceeds the 16 KB limit) - Unstructured (hundreds of files with complex nested keys) - Not useful for filtering runs (you don't query \"find runs where beam.memory=180\")</p> <p>Instead, hash the config directory:</p> <pre><code>import consist\nfrom pathlib import Path\nfrom consist import use_tracker\n\nasim_config_dir = Path(\"./configs/activitysim\")\n\nwith use_tracker(tracker):\n    # First run with baseline config\n    result1 = consist.run(\n        fn=run_activitysim,\n        name=\"asim_baseline\",\n        config={\"scenario\": \"baseline\"},\n        hash_inputs=[(\"asim_config\", asim_config_dir)],\n    )\n\n# Later: You edit the YAML config (change a parameter)\n# The directory hash changes → signature changes → cache miss\n# ActivitySim re-runs with the new config\n\n    # Second run with updated config\n    result2 = consist.run(\n        fn=run_activitysim,\n        name=\"asim_baseline\",\n        config={\"scenario\": \"baseline\"},  # Same config dict\n        hash_inputs=[(\"asim_config\", asim_config_dir)],  # Different hash\n    )\n# Different signature (hash_inputs changed) → fresh run\n</code></pre> <p>Comparison: config vs hash_inputs</p> Approach Space Queryable Cache Behavior <code>config={\"yaml\": large_dict}</code> JSON snapshot only No (by default) Cache respects changes <code>hash_inputs=[path]</code> Minimal No Cache respects changes Ignore files Minimal N/A Cache miss if files change (BAD) <p>For large, unstructured configs like ActivitySim YAML, use <code>hash_inputs</code>.</p> <p>Audit Trail</p> <p>Even though the file content isn't stored, Consist records the hash for debugging:</p> <pre><code>run = tracker.get_run(\"asim_baseline_001\")\nprint(run.meta[\"consist_hash_inputs\"])\n# Output: {\"asim_config\": \"sha256:a1b2c3...\"}\n</code></pre> <p>This lets you correlate runs with specific configuration states without storing the full config.</p>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#querying-facets-in-duckdb","level":2,"title":"Querying Facets in DuckDB","text":"<p>Facet persistence creates two tables:</p> <ul> <li><code>config_facet</code>: deduplicated facet JSON blobs</li> <li><code>run_config_kv</code>: flattened key/value index for filtering</li> </ul> <p>Flattened keys use dot-notation, with dots in raw keys escaped as <code>\\.</code>.</p> <p>Examples:</p> <pre><code>tracker.find_runs_by_facet_kv(\n    key=\"beam.memory_gb\",\n    op=\"&gt;=\",\n    value_num=180,\n    namespace=\"beam\",\n)\n</code></pre> <p>You can also fetch facets directly:</p> <pre><code>facet = tracker.get_config_facet(facet_id)\n</code></pre>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#model-specific-config-adapters","level":2,"title":"Model-Specific Config Adapters","text":"<p>For complex, file-based configurations (such as ActivitySim YAML/CSV configs), Consist provides config adapters—model-specific modules that discover, canonicalize, and ingest configuration data into queryable tables.</p> <p>Unlike in-memory configs (dicts/Pydantic models), adapters handle: - Discovery: Locating config files and computing content hashes - Canonicalization: Converting config metadata into artifacts and ingestable schemas - Ingestion: Persisting calibration-sensitive parameters as queryable tables</p> <p>This decouples model-specific parsing logic from Consist core, making it easy to add new model types without coupling them to the framework.</p> <p>Available adapters:</p> <ul> <li>ActivitySim Config Adapter — Discover, canonicalize, and query ActivitySim YAML/CSV configurations</li> <li>BEAM Config Adapter — Canonicalize HOCON configs and query key/value parameters</li> </ul> <p>For detailed usage and API reference, see the Config Adapters Integration Guide.</p>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#examples","level":2,"title":"Examples","text":"","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#consistrun-with-config-facet","level":3,"title":"consist.run with config + facet","text":"<pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=run_beam_step,\n        name=\"beam\",\n        config=beam_cfg,\n        facet={\"memory_gb\": beam_cfg.memory_gb},\n        hash_inputs=[(\"beam_hocon\", beam_cfg_path)],\n    )\n</code></pre>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#consistrun-with-config_plan-config-adapters","level":3,"title":"consist.run with config_plan (config adapters)","text":"<pre><code>from consist.integrations.activitysim import ActivitySimConfigAdapter\n\nimport consist\nfrom consist import use_tracker\n\nadapter = ActivitySimConfigAdapter()\nplan = tracker.prepare_config(adapter, [overlay_dir, base_dir])\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=run_activitysim,\n        name=\"activitysim\",\n        config={\"scenario\": \"baseline\"},\n        config_plan=plan,\n        cache_mode=\"reuse\",\n    )\n</code></pre>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"configs/#scenariocontextrun-with-facet_from","level":3,"title":"ScenarioContext.run with facet_from","text":"<pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    with consist.scenario(\"demo\", tags=[\"travel\"]) as sc:\n        sc.run(\n            fn=run_activitysim,\n            name=\"activitysim\",\n            config=asim_cfg,\n            facet_from=[\"scenario\", \"sample_rate\"],\n            hash_inputs=[(\"asim_yaml\", asim_config_dir)],\n        )\n</code></pre>","path":["Core Topics","Configuration, Identity, and Facets"],"tags":[]},{"location":"containers-guide/","level":1,"title":"Container Integration Guide","text":"<p>Consist can execute containerized tools and models (Docker, Singularity) while automatically tracking provenance, inputs, outputs, and caching based on image digest and parameters.</p> <p>This guide covers when and how to use containers, with practical examples for ActivitySim, SUMO, and other tools.</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#why-containers","level":2,"title":"Why Containers?","text":"<p>Containers are ideal when:</p> <ul> <li>Wrapping existing tools (ActivitySim, SUMO, BEAM, R scripts, legacy code) without modifying source</li> <li>Tool has complex dependencies that are easier to package than to install</li> <li>Tool is non-deterministic or black-box (you can't inspect its internal caching)</li> <li>Tool expects specific file paths or directory structures</li> <li>Tool is in a different language (Python, R, Java, compiled binary)</li> <li>Tool is already containerized and you want to integrate it into a Consist workflow</li> </ul> <p>Not ideal for: - Simple Python functions (use <code>consist.run()</code> instead) - Workflows where you want fine-grained step caching (use <code>scenario()</code> + <code>consist.run()</code>) - Development/debugging (container startup adds latency during iteration, but can be fine for lightweight images)</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#how-caching-works","level":2,"title":"How Caching Works","text":"<p>When you execute a container, Consist computes a container signature from:</p> <ol> <li>Image identity: Full image digest (resolved from registry if <code>pull_latest=True</code>)</li> <li>Command: The exact command string and arguments</li> <li>Environment: All environment variables passed to the container</li> <li>Volumes: Container mount paths (but NOT host paths, which are run-specific)</li> <li>Input data: SHA256 hashes of input files</li> </ol> <p>If all these match a prior run, Consist returns cached outputs without executing the container.</p> <p>Key insight: Changing the image tag (e.g., <code>my-model:v1</code> → <code>my-model:v2</code>) invalidates the cache. If you use <code>my-model:latest</code>, consider setting <code>pull_latest=True</code> to refresh the digest from the registry and ensure cache invalidation when the tag updates (use it when <code>latest</code> is truly mutable).</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#basic-example-single-container-run","level":2,"title":"Basic Example: Single Container Run","text":"<pre><code>from consist import Tracker\nfrom consist.integrations.containers import run_container\nfrom pathlib import Path\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\n# Create input data\ninput_path = Path(\"./data/input.csv\")\ninput_path.parent.mkdir(exist_ok=True)\ninput_path.write_text(\"x,y\\n1,2\\n3,4\\n\")\n\n# Execute container\nresult = run_container(\n    tracker=tracker,\n    run_id=\"my_model_run\",\n    image=\"my-org/my-model:v1.0\",\n    command=[\"python\", \"process.py\", \"--input\", \"/inputs/input.csv\"],\n    volumes={\n        \"./data\": \"/inputs\",      # Host path → Container path\n        \"./outputs\": \"/outputs\",\n    },\n    inputs=[input_path],          # Files to hash (for cache key)\n    outputs=[\"./outputs/result.csv\"],  # Files to capture as artifacts\n    backend_type=\"docker\",\n)\n\n# Access results\nif result.cache_hit:\n    print(f\"Cache hit from {result.cache_source}\")\nelse:\n    print(\"Container executed\")\n\nfor key, artifact in result.artifacts.items():\n    print(f\"Output: {key} → {artifact.path}\")\n</code></pre> <p>What happened: 1. Consist created a signature from image + command + inputs 2. Checked if this signature exists in the database 3. If no prior run: executed the container, scanned outputs, logged them as artifacts 4. If prior run exists: copied cached artifacts to <code>./outputs/result.csv</code> (no container execution)</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#activitysim-integration","level":2,"title":"ActivitySim Integration","text":"<p>ActivitySim is a commonly used transportation demand modeling tool. Here's how to integrate it with Consist:</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#setup","level":3,"title":"Setup","text":"<ol> <li>Create a Docker image with ActivitySim installed:</li> </ol> <pre><code>FROM python:3.11\nRUN pip install activitysim numpy pandas\nCOPY . /workspace\nWORKDIR /workspace\n</code></pre> <p>Build and push to registry: <pre><code>docker build -t my-org/activitysim:v1.0 .\ndocker push my-org/activitysim:v1.0\n</code></pre></p> <ol> <li>Create ActivitySim config files (as usual) in a local directory: <pre><code>./configs/\n├── settings.yaml\n├── accessibility_coefficients.csv\n├── mode_choice_coefficients.csv\n└── ...\n</code></pre></li> </ol>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#running-a-scenario","level":3,"title":"Running a Scenario","text":"<pre><code>from consist import Tracker\nfrom consist.integrations.containers import run_container\nfrom pathlib import Path\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\ndef run_activitysim_scenario(scenario_name: str, configs_dir: Path):\n    \"\"\"Execute ActivitySim with Consist provenance.\"\"\"\n\n    # Ensure output directory exists\n    output_dir = Path(f\"./outputs/{scenario_name}\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    result = run_container(\n        tracker=tracker,\n        run_id=f\"activitysim_{scenario_name}\",\n        image=\"my-org/activitysim:v1.0\",\n        command=[\n            \"python\", \"-m\", \"activitysim.core.workflow\",\n            \"-c\", \"/configs\",\n            \"-o\", f\"/outputs/{scenario_name}\",\n        ],\n        volumes={\n            str(configs_dir): \"/configs\",\n            \"./outputs\": \"/outputs\",\n        },\n        inputs=[configs_dir],  # Hash all configs\n        outputs=[f\"./outputs/{scenario_name}\"],  # Capture all outputs\n        environment={\n            \"ACTIVITYSIM_CHUNK_SIZE\": \"100000\",  # Optional: tune performance\n        },\n        backend_type=\"docker\",\n    )\n\n    return result\n\n# Run baseline scenario\nresult_baseline = run_activitysim_scenario(\n    \"baseline\",\n    configs_dir=Path(\"./configs\"),\n)\n\n# Run scenario with modified coefficients\n# (New output directory and modified config) → new cache entry\nresult_modified = run_activitysim_scenario(\n    \"high_income_sensitivity\",\n    configs_dir=Path(\"./configs_modified\"),\n)\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#querying-results","level":3,"title":"Querying Results","text":"<p>After running scenarios, query outputs across runs:</p> <pre><code>import pandas as pd\nfrom pathlib import Path\n\n# Find all ActivitySim outputs\nfor run_subdir in Path(\"./runs\").glob(\"activitysim_*/\"):\n    output_artifacts = tracker.get_artifacts_for_run(run_subdir.name)\n    for key, artifact in output_artifacts.outputs.items():\n        if \"summary\" in key:\n            df = pd.read_csv(artifact.path)\n            print(f\"Scenario: {run_subdir.name}\")\n            print(df.head())\n</code></pre> <p>Or use Consist's database queries:</p> <pre><code>from sqlmodel import Session, select\n\nwith Session(tracker.engine) as session:\n    # Find all ActivitySim runs\n    runs = session.exec(\n        select(Run).where(Run.model == \"activitysim\")\n    ).all()\n\n    for run in runs:\n        artifacts = tracker.get_artifacts_for_run(run.id)\n        print(f\"Run: {run.id}, Outputs: {list(artifacts.outputs.keys())}\")\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#singularity-apptainer-support","level":2,"title":"Singularity / Apptainer Support","text":"<p>If using Singularity (common on HPC clusters):</p> <pre><code>from consist.integrations.containers import run_container\n\nresult = run_container(\n    tracker=tracker,\n    run_id=\"hpc_job\",\n    image=\"/path/to/my_model.sif\",  # Local .sif file path\n    command=[\"python\", \"model.py\", \"--param\", \"value\"],\n    volumes={\n        \"/scratch/inputs\": \"/inputs\",\n        \"/scratch/outputs\": \"/outputs\",\n    },\n    inputs=[Path(\"/scratch/inputs/data.csv\")],\n    outputs=[\"/scratch/outputs\"],\n    backend_type=\"singularity\",  # Use Singularity instead of Docker\n)\n</code></pre> <p>Key differences: - Image is typically a local file path (.sif) not a registry URL - Volume syntax is the same - Caching behavior is identical (based on image path digest)</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#volume-mounting-best-practices","level":2,"title":"Volume Mounting Best Practices","text":"","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#mount-paths","level":3,"title":"Mount Paths","text":"<p>Map host directories to container paths consistently:</p> <pre><code>volumes = {\n    \"/host/absolute/path\": \"/container/path\",  # Use absolute paths\n    \"./relative\": \"/container/relative\",        # Or relative (resolved against cwd)\n}\n</code></pre> <p>Important: Host paths in <code>volumes</code> are not part of the cache key (they're run-specific). Only <code>inputs</code> are hashed for the signature. If you want config changes to invalidate cache, pass config files in <code>inputs</code>:</p> <pre><code>result = run_container(\n    ...\n    volumes={\n        \"./config_dir\": \"/configs\",\n        \"./data\": \"/data\",\n    },\n    inputs=[Path(\"./config_dir/settings.yaml\"), Path(\"./data/input.csv\")],  # These are hashed\n    ...\n)\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#handling-outputs","level":3,"title":"Handling Outputs","text":"<p>Outputs can be specified as:</p> <ul> <li> <p>List of paths (logged with filename as key):   <pre><code>outputs=[\"./outputs/result.csv\", \"./outputs/logs.txt\"]\n# Artifact keys: \"result.csv\", \"logs.txt\"\n</code></pre></p> </li> <li> <p>Dict mapping keys to paths (custom artifact keys):   <pre><code>outputs={\n    \"main_result\": \"./outputs/result.csv\",\n    \"diagnostics\": \"./outputs/logs.txt\",\n}\n# Artifact keys: \"main_result\", \"diagnostics\"\n</code></pre></p> </li> </ul> <p>Consist scans output directories on the host after container exits. Files created inside the container at mounted paths are detected automatically.</p> <p>Warning: If the container doesn't create files at the expected paths, Consist logs a warning but doesn't fail (use <code>lineage_mode=\"full\"</code> to capture what was created).</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#environment-variables-configuration","level":2,"title":"Environment Variables &amp; Configuration","text":"<p>Pass environment variables to the container:</p> <pre><code>result = run_container(\n    ...\n    environment={\n        \"MODEL_PARAM_1\": \"value1\",\n        \"MODEL_PARAM_2\": \"value2\",\n        \"DEBUG\": \"true\",\n    },\n    ...\n)\n</code></pre> <p>These variables are part of the cache key. Changing them invalidates the cache.</p> <p>For large configurations, mount config files instead of passing via environment:</p> <pre><code># DON'T do this for large configs:\nenvironment={\"CONFIG\": json.dumps(huge_config)}  # Bad: unreadable, cache-unfriendly\n\n# DO this:\nvolumes={\"./config.json\": \"/app/config.json\"}\ninputs=[Path(\"./config.json\")]\ncommand=[\"python\", \"app.py\", \"--config\", \"/app/config.json\"]\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#cache-behavior-hydration","level":2,"title":"Cache Behavior &amp; Hydration","text":"","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#cache-hits","level":3,"title":"Cache Hits","text":"<p>On a cache hit:</p> <ol> <li>Consist finds a prior run with the same signature</li> <li>No container execution occurs</li> <li>Cached output files are copied to host paths</li> <li><code>result.cache_hit == True</code>, <code>result.cache_source</code> = prior run ID</li> </ol> <pre><code>result = run_container(...)\nif result.cache_hit:\n    print(f\"Cache hit from {result.cache_source}\")\n    # result.artifacts are materialized (files exist on disk)\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#cache-invalidation","level":3,"title":"Cache Invalidation","text":"<p>Cache is invalidated (new run executed) if any of these change:</p> <ul> <li>Image: <code>my-model:v1</code> → <code>my-model:v2</code> (or image pulled with different digest if <code>pull_latest=True</code>)</li> <li>Command: Arguments to the tool</li> <li>Environment: Any env var changes</li> <li>Inputs: Hash of input files changes</li> <li>Volumes: Container mount paths change</li> </ul>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#manual-cache-bypass","level":3,"title":"Manual Cache Bypass","text":"<p>To force re-execution even with matching signature:</p> <pre><code># No built-in flag, but you can:\n# 1. Change a trivial env var to force cache miss:\nenvironment={\"CACHE_BYPASS\": str(time.time())}\n\n# 2. Or use a cache_mode on the enclosing run:\ntracker.begin_run(..., cache_mode=\"overwrite\")\nresult = run_container(...)\ntracker.end_run()\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#nested-containers-inside-scenarios","level":2,"title":"Nested Containers (Inside Scenarios)","text":"<p>Use <code>run_container()</code> inside <code>scenario()</code> for multi-step workflows:</p> <pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom consist.integrations.containers import run_container\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\nwith use_tracker(tracker):\n    with consist.scenario(\"multi_model_workflow\") as sc:\n\n        # Step 1: Data preprocessing (Python)\n        preprocess_result = run_container(\n            tracker=tracker,\n            run_id=\"preprocess\",\n            image=\"my-org/preprocess:v1\",\n            command=[\"python\", \"preprocess.py\"],\n            volumes={\"./raw_data\": \"/data\"},\n            inputs=[Path(\"./raw_data\")],\n            outputs=[\"./preprocessed\"],\n        )\n        sc.coupler.set(\"preprocessed_data\", preprocess_result.output)\n\n        # Step 2: Model execution (ActivitySim)\n        model_result = run_container(\n            tracker=tracker,\n            run_id=\"activitysim\",\n            image=\"my-org/activitysim:v1\",\n            command=[\"python\", \"-m\", \"activitysim.core.workflow\"],\n            volumes={\n                \"./configs\": \"/configs\",\n                \"./model_outputs\": \"/outputs\",\n            },\n            inputs=[Path(\"./configs\")],\n            outputs=[\"./model_outputs\"],\n        )\n        sc.coupler.set(\"model_outputs\", model_result.output)\n\n        # Step 3: Analysis (Python)\n        with sc.trace(name=\"analysis\"):\n            # Load previous results\n            preprocessed = consist.load_df(sc.coupler.require(\"preprocessed_data\"))\n            model_output = consist.load_df(sc.coupler.require(\"model_outputs\"))\n            # ... analysis code ...\n</code></pre> <p>Each container step caches independently. If preprocessing input hasn't changed, skip it. If model config hasn't changed, skip it.</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#error-handling","level":2,"title":"Error Handling","text":"","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#container-execution-fails","level":3,"title":"Container Execution Fails","text":"<p>If the container exits with a non-zero code:</p> <pre><code>try:\n    result = run_container(...)\nexcept RuntimeError as e:\n    print(f\"Container failed: {e}\")\n    # Debug: check container logs, input paths, volume mounts\n</code></pre> <p>Debugging steps: 1. Run the container manually to check for errors:    <pre><code>docker run -it -v ./data:/inputs my-org/my-model:v1 python process.py\n</code></pre> 2. Check that input paths exist and are readable by the container 3. Ensure output directories are writable (containers often run as root) 4. Check volume mount paths are absolute or correctly resolved</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#output-files-not-found","level":3,"title":"Output Files Not Found","text":"<p>If Consist doesn't find expected outputs:</p> <pre><code>result = run_container(...)\n# Warning logged: \"Expected output not found: ./outputs/result.csv\"\n\n# Check what was actually created:\nimport subprocess\nsubprocess.run([\"docker\", \"run\", \"--rm\",\n                \"-v\", \"./outputs:/outputs\",\n                \"my-org/my-model:v1\",\n                \"ls\", \"-la\", \"/outputs\"])\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#image-pull-errors","level":3,"title":"Image Pull Errors","text":"<p>If the image cannot be pulled:</p> <pre><code>result = run_container(\n    ...\n    image=\"my-org/my-model:v1.0\",\n    pull_latest=False,  # Avoid registry roundtrip if not needed\n    backend_type=\"docker\",\n)\n# Check docker auth:\n# docker login\n# docker pull my-org/my-model:v1.0\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#performance-tuning","level":2,"title":"Performance Tuning","text":"","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#container-startup-overhead","level":3,"title":"Container Startup Overhead","text":"<p>Container creation/startup is ~1-2 seconds. For workflows with many short-lived steps, batch them:</p> <pre><code># DON'T do this (N containers, N startups):\nfor i in range(100):\n    run_container(...)\n\n# DO this (1 container, batch processing inside):\nrun_container(\n    command=[\"python\", \"batch_process.py\", \"--n\", \"100\"],\n    ...\n)\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#image-size-registry","level":3,"title":"Image Size &amp; Registry","text":"<p>Large images slow down pulls. Optimize: - Use slim base images (<code>python:3.11-slim</code> not <code>python:3.11</code>) - Only install required dependencies - Use multi-stage builds</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#persistent-caching","level":3,"title":"Persistent Caching","text":"<p>If you re-run the same container frequently, Consist's cache avoids re-execution. But if cache is disabled or cleared:</p> <pre><code># Cache is per (image_digest, command, inputs) signature\n# To maximize cache reuse:\n# 1. Pin image versions (don't use :latest)\n# 2. Use `pull_latest=False` unless you need latest code\n# 3. Log inputs consistently (same file paths, same hashes)\n</code></pre>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#lineage-mode-full-vs-none","level":2,"title":"Lineage Mode: \"full\" vs \"none\"","text":"<p>By default, <code>lineage_mode=\"full\"</code> performs provenance tracking. If you need to execute containers without Consist logging (for external tools), use <code>lineage_mode=\"none\"</code>:</p> <pre><code>result = run_container(\n    ...\n    lineage_mode=\"none\",  # Don't create a Consist run\n)\n# No provenance logged, but you still get manifest_hash for external tracking\n</code></pre> <p>This is useful if you're embedding Consist-executed containers inside a non-Consist workflow.</p>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"containers-guide/#see-also","level":2,"title":"See Also","text":"<ul> <li>Usage Guide: Pattern 3 (Container Integration)</li> <li>Architecture: Container Integration</li> <li>Troubleshooting: Container Execution</li> </ul>","path":["Core Topics","Container Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/","level":1,"title":"DLT Loader Integration Guide","text":"<p>The DLT (Data Load Tool) integration enables robust, schema-validated ingestion of data into DuckDB with automatic provenance column injection. This guide covers when to use DLT, how to configure schemas, and best practices for data quality.</p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#what-is-dlt","level":2,"title":"What is DLT?","text":"<p><code>dlt</code> is an open-source library for extracting and loading data. Consist uses it to:</p> <ul> <li>Ingest diverse formats (Parquet, CSV, JSON, Python objects) into DuckDB</li> <li>Auto-detect and enforce schemas (schema enforcement when provided)</li> <li>Handle data quality issues (type mismatches, missing values, duplicates)</li> <li>Inject Consist provenance columns (<code>consist_run_id</code>, <code>consist_artifact_id</code>, <code>consist_year</code>, etc.)</li> <li>Scale efficiently with streaming and batching</li> </ul>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#when-to-use-dlt-vs-direct-logging","level":2,"title":"When to Use DLT vs Direct Logging","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#use-dlt-for","level":3,"title":"Use DLT for:","text":"<ul> <li>Large datasets (100K+ rows) → streaming ingestion with DuckDB backend</li> <li>Transportation: 50 runs × 1M persons per run = 50M total household records across all scenarios</li> <li>Climate: Multi-year ensemble with 50 model runs, each producing regional summaries that you'll compare statistically</li> <li> <p>Urban Planning: Parcel-level zoning changes tracked across 20 scenario runs for cumulative analysis</p> </li> <li> <p>Schema evolution tracking → understand how data structure changes across runs</p> </li> <li>Transportation: New land use categories added in run 15; need to understand impact on mode choice modeling</li> <li>Climate: Regional variables refined across ensemble members; track when resolution changed</li> <li> <p>Urban Planning: Zoning class definitions evolved across policy iterations; audit the timeline</p> </li> <li> <p>Schema validation → enforce types, nullability, PKs before loading</p> </li> <li>Transportation: Catch mode=\"car_pool\" typos before querying mode choice; enforce person_id uniqueness</li> <li>Climate: Ensure temperature is numeric (not string from CSV parsing); reject runs with missing required variables</li> <li> <p>Urban Planning: Validate parcel_ids are unique across scenarios; catch mixed data types in zoning codes</p> </li> <li> <p>Complex data → nested structures, unions, evolving columns</p> </li> <li>Transportation: Person-trip links where one trip can have multiple stages; household composition variations</li> <li>Climate: Variable-length time series per location; nested metadata for model parameters</li> <li> <p>Urban Planning: Multi-level zoning (district → block → parcel) with hierarchical relationships</p> </li> <li> <p>Cross-run analysis → query data from multiple runs together with <code>tracker.views</code></p> </li> <li>Transportation: \"Compare average trip distance across all 50 scenarios in one SQL query\"</li> <li>Climate: \"Find 95<sup>th</sup> percentile precipitation across ensemble members by region\"</li> <li>Urban Planning: \"Count parcels zoned commercial across all scenarios, grouped by district\"</li> </ul> <p>Example: <pre><code>import consist\nfrom sqlmodel import select\n\n# DLT ingestion → can query across all runs with SQL\nVPerson = tracker.views.Person\nrows = consist.run_query(\n    select(VPerson.consist_run_id, VPerson.age),\n    tracker=tracker,\n)\n</code></pre></p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#use-direct-logging-for","level":3,"title":"Use Direct Logging for:","text":"<ul> <li>Small results (&lt;10K rows) → lightweight, no schema overhead</li> <li>Transportation: Single scenario's summary statistics (e.g., 10 lines showing mode split by district)</li> <li>Climate: One-off diagnostics (e.g., 50 error logs from model runs, not for re-analysis)</li> <li> <p>Urban Planning: Quick impact breakdown (e.g., 3 numbers showing acres zoned before/after one proposal)</p> </li> <li> <p>One-off analyses → not meant for cross-run queries</p> </li> <li>Transportation: Single debug run showing why trip generation failed in zone 42</li> <li>Climate: Quick sensitivity test to new parameter value; won't compare with baseline</li> <li> <p>Urban Planning: Ad-hoc impact assessment for one proposal that stakeholders asked about once</p> </li> <li> <p>Simple CSVs/Parquets → no schema validation needed</p> </li> <li>Transportation: Raw travel demand model export (you'll validate in downstream step)</li> <li>Climate: Raw gridded output from climate model (too large for DuckDB anyway; just store for archival)</li> <li> <p>Urban Planning: External zoning shapefile you imported (you don't own, can't enforce schema)</p> </li> <li> <p>External data → data you don't own or control</p> </li> <li>Transportation: Census data, transit schedules, road networks from external sources</li> <li>Climate: Observational data, boundary conditions from other modeling teams</li> <li>Urban Planning: Parcel maps from county assessor, regulatory texts from local government</li> </ul> <p>Example: <pre><code># Direct logging → no schema, just store the file\nwith tracker.start_run(\"log_result\", model=\"demo\"):\n    consist.log_artifact(result_path, key=\"my_result\")\n</code></pre></p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#decision-tree","level":3,"title":"Decision Tree","text":"<pre><code>Do you want to query this data across multiple runs in SQL?\n├─ YES → Use DLT (register a schema)\n├─ NO  → Do you trust the data format/types?\n│        ├─ YES → Use direct logging (tracker.log_artifact)\n│        └─ NO  → Use DLT with a schema\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#basic-dlt-workflow","level":2,"title":"Basic DLT Workflow","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-1-define-a-schema-sqlmodel","level":3,"title":"Step 1: Define a Schema (SQLModel)","text":"<pre><code>from sqlmodel import SQLModel, Field\nfrom typing import Optional\n\nclass Person(SQLModel, table=True):\n    \"\"\"Schema for person records.\"\"\"\n    person_id: int = Field(primary_key=True)\n    age: int\n    income: Optional[float]\n    name: str\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-2-create-tracker-with-schema","level":3,"title":"Step 2: Create Tracker with Schema","text":"<pre><code>from consist import Tracker\nfrom pathlib import Path\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person],  # Register schema\n)\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-3-log-data-with-schema","level":3,"title":"Step 3: Log Data with Schema","text":"<pre><code>import pandas as pd\n\n# Create data\ndf = pd.DataFrame({\n    \"person_id\": [1, 2, 3],\n    \"age\": [25, 30, 35],\n    \"income\": [50000.0, 60000.0, 70000.0],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\"],\n})\n\n# Log with schema (DLT ingestion)\nwith tracker.start_run(\"ingest_people\", model=\"demo\"):\n    tracker.log_dataframe(df, key=\"persons\", schema=Person)\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#step-4-query-across-runs","level":3,"title":"Step 4: Query Across Runs","text":"<pre><code>from sqlmodel import Session, select, func\n\n# Compute an aggregate over a single run_id (via the hybrid view).\nVPerson = tracker.views.Person\nwith Session(tracker.engine) as session:\n    avg_age = session.exec(\n        # Filter to one Consist run and take an average over the ingested rows.\n        select(func.avg(VPerson.age)).where(VPerson.consist_run_id == \"run_123\")\n    ).first()\n    print(f\"Average age in run_123: {avg_age}\")\n</code></pre> <p>Expected output:</p> <pre><code>Average age in run_123: 30.0\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#schema-definition-validation","level":2,"title":"Schema Definition &amp; Validation","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#basic-schema","level":3,"title":"Basic Schema","text":"<pre><code>from sqlmodel import SQLModel, Field\nfrom typing import Optional\n\nclass Trip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True)\n    person_id: int\n    origin: str\n    destination: str\n    distance_miles: float\n    mode: str  # \"car\", \"transit\", \"bike\"\n    departure_hour: Optional[int]\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#validation-required-vs-optional","level":3,"title":"Validation: Required vs Optional","text":"<pre><code>class StrictTrip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True, description=\"Unique trip ID\")\n    person_id: int = Field(gt=0)  # Must be positive\n    mode: str = Field(min_length=1)  # Non-empty\n    departure_hour: Optional[int] = Field(ge=0, le=23)  # 0-23 if present\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#foreign-keys-relationships","level":3,"title":"Foreign Keys (Relationships)","text":"<pre><code>from sqlmodel import SQLModel, Field, Relationship\n\nclass Person(SQLModel, table=True):\n    person_id: int = Field(primary_key=True)\n    name: str\n    # trips: List[\"Trip\"] = Relationship(back_populates=\"person\")\n\nclass Trip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True)\n    person_id: int = Field(foreign_key=\"person.person_id\")\n    distance_miles: float\n    # person: Person = Relationship(back_populates=\"trips\")\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#type-mapping","level":3,"title":"Type Mapping","text":"Python Type DuckDB Type Notes <code>int</code> <code>INTEGER</code> <code>float</code> <code>DOUBLE</code> <code>str</code> <code>VARCHAR</code> <code>bool</code> <code>BOOLEAN</code> <code>date</code> <code>DATE</code> Use <code>datetime.date</code> <code>datetime</code> <code>TIMESTAMP</code> Use <code>datetime.datetime</code> <code>Optional[T]</code> <code>T NULL</code> Allows NULL <code>List[T]</code> <code>T[]</code> Arrays (advanced)","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#logging-patterns","level":2,"title":"Logging Patterns","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#single-dataframe","level":3,"title":"Single DataFrame","text":"<pre><code>import consist\n\ndf = pd.read_csv(\"results.csv\")\nwith tracker.start_run(\"log_results\", model=\"demo\"):\n    consist.log_dataframe(\n        df,\n        key=\"results\",\n        schema=MySchema,  # Validate against schema\n    )\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#parquet-file","level":3,"title":"Parquet File","text":"<pre><code>with tracker.start_run(\"log_parquet\", model=\"demo\"):\n    tracker.log_artifact(\n        Path(\"results.parquet\"),\n        key=\"raw_results\",\n        schema=MySchema,  # Schema is stored; call tracker.ingest(...) to ingest\n    )\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#csv-file","level":3,"title":"CSV File","text":"<pre><code>with tracker.start_run(\"log_csv\", model=\"demo\"):\n    tracker.log_artifact(\n        Path(\"results.csv\"),\n        key=\"csv_results\",\n        schema=MySchema,\n    )\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#zarr-netcdf-matrix-data","level":3,"title":"Zarr / NetCDF (Matrix Data)","text":"<pre><code># Zarr metadata is ingested as catalog (not raw data)\nwith tracker.start_run(\"log_zarr\", model=\"demo\"):\n    tracker.log_artifact(\n        Path(\"simulation_output.zarr\"),\n        key=\"gridded_results\",\n        driver=\"zarr\",\n    )\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#data-quality-error-handling","level":2,"title":"Data Quality &amp; Error Handling","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#schema-enforcement-fail-on-issues","level":3,"title":"Schema Enforcement (Fail on Issues)","text":"<p>When you provide a schema, Consist enforces the column set and types. Extra columns raise a <code>ValueError</code>, and type mismatches are surfaced during ingestion.</p> <pre><code>with tracker.start_run(\"strict_ingest\", model=\"demo\"):\n    tracker.log_dataframe(\n        df,\n        key=\"strict_results\",\n        schema=MySchema,\n    )\n</code></pre> <p>If you want best-effort ingestion (no strict schema), omit the schema and let DLT infer the structure.</p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#type-coercion","level":3,"title":"Type Coercion","text":"<p>DLT attempts to coerce types:</p> <pre><code>df = pd.DataFrame({\n    \"trip_id\": [\"1\", \"2\", \"3\"],      # Strings\n    \"distance\": [1.5, 2.2, 3.1],     # Floats\n})\n\n# Ingested as:\n# trip_id: [1, 2, 3]  (coerced to int)\n# distance: [1.5, 2.2, 3.1]  (floats)\n</code></pre> <p>To avoid surprises, ensure input DataFrame matches schema types:</p> <pre><code>df = df.astype({\n    \"trip_id\": \"int64\",\n    \"distance\": \"float64\",\n})\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#handling-missing-data","level":3,"title":"Handling Missing Data","text":"<pre><code>class Trip(SQLModel, table=True):\n    trip_id: int = Field(primary_key=True)\n    person_id: int\n    departure_hour: Optional[int]  # Can be NULL\n    arrival_hour: Optional[int]\n</code></pre> <p>Missing values in Optional fields → NULL in DB. Missing in required fields → error or default depending on the schema and ingestion behavior.</p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#duplicate-handling","level":3,"title":"Duplicate Handling","text":"<p>Primary keys are not enforced automatically in all cases. If you need uniqueness, deduplicate before ingestion:</p> <pre><code>class Household(SQLModel, table=True):\n    household_id: int = Field(primary_key=True)\n    size: int\n\n# If DataFrame has duplicate household_id:\ndf = pd.DataFrame({\n    \"household_id\": [1, 2, 1],  # Duplicate!\n    \"size\": [4, 3, 5],\n})\n</code></pre> <p>To handle duplicates, deduplicate before ingestion:</p> <pre><code>df = df.drop_duplicates(subset=[\"household_id\"], keep=\"last\")\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#provenance-columns","level":2,"title":"Provenance Columns","text":"<p>Consist automatically injects system columns during ingestion:</p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#available-columns","level":3,"title":"Available Columns","text":"Column Type Description <code>consist_run_id</code> str ID of the run that created this data <code>consist_artifact_id</code> str Artifact ID of the source file <code>consist_scenario_id</code> str Scenario ID (available in hybrid views) <code>consist_year</code> int Year (if provided to run context) <code>consist_iteration</code> int Iteration count (if provided)","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#example-query","level":3,"title":"Example Query","text":"<pre><code>from sqlmodel import Session, select, func\n\nVPerson = tracker.views.Person\nwith Session(tracker.engine) as session:\n    # Count persons per run\n    results = session.exec(\n        select(\n            VPerson.consist_run_id,\n            func.count(VPerson.person_id).label(\"count\")\n        ).group_by(VPerson.consist_run_id)\n    ).all()\n\n    for run_id, count in results:\n        print(f\"Run {run_id}: {count} persons\")\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#filtering-by-provenance","level":3,"title":"Filtering by Provenance","text":"<pre><code># Get persons from a specific scenario year\nwith Session(tracker.engine) as session:\n    persons_2030 = session.exec(\n        select(VPerson).where(\n            VPerson.consist_year == 2030,\n            VPerson.consist_scenario_id == \"baseline\"\n        )\n    ).all()\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#advanced-patterns","level":2,"title":"Advanced Patterns","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#multi-step-ingestion-scenario","level":3,"title":"Multi-Step Ingestion (Scenario)","text":"<pre><code>import consist\nfrom consist import Tracker, use_tracker\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person, Trip, Household],\n)\n\nwith use_tracker(tracker):\n    with consist.scenario(\"model_run_2030\", year=2030) as sc:\n\n        # Step 1: Load persons\n        with sc.trace(name=\"load_persons\"):\n            df_persons = load_population_data()\n            consist.log_dataframe(\n                df_persons,\n                key=\"population\",\n                schema=Person,\n            )\n\n        # Step 2: Simulate trips\n        with sc.trace(name=\"simulate_trips\"):\n            df_trips = run_trip_simulation(df_persons)\n            consist.log_dataframe(\n                df_trips,\n                key=\"trips\",\n                schema=Trip,\n            )\n\n        # Step 3: Aggregate\n        with sc.trace(name=\"aggregate\"):\n            # Query previous step\n            with Session(tracker.engine) as session:\n                total_trips = session.exec(\n                    select(func.count(Trip.trip_id)).where(\n                        Trip.consist_scenario_id == \"model_run_2030\"\n                    )\n                ).first()\n            print(f\"Total trips: {total_trips}\")\n</code></pre> <p>All data from steps 1-3 is queryable together:</p> <pre><code># Cross-step query\nwith Session(tracker.engine) as session:\n    results = session.exec(\n        select(Person, Trip).join(\n            Trip, Person.person_id == Trip.person_id\n        ).where(Trip.consist_scenario_id == \"model_run_2030\")\n    ).all()\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#incremental-ingestion-batches","level":3,"title":"Incremental Ingestion (Batches)","text":"<p>For very large datasets, ingest in batches:</p> <pre><code>from pathlib import Path\n\n# Split large file into chunks\nimport pandas as pd\n\nchunk_size = 100000\nfor i, chunk in enumerate(pd.read_csv(\"large_file.csv\", chunksize=chunk_size)):\n    consist.log_dataframe(\n        chunk,\n        key=f\"data_chunk_{i}\",\n        schema=MySchema,\n    )\n</code></pre> <p>DuckDB automatically unions these into a single table when the schema/table name is the same (for example, when you pass <code>schema=MySchema</code> each time):</p> <pre><code># Query all chunks together\nwith Session(tracker.engine) as session:\n    count = session.exec(select(func.count(MySchema.id))).first()\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#late-arriving-data","level":3,"title":"Late-Arriving Data","text":"<p>If you ingest data, then later find more data to add:</p> <pre><code># Run 1: Initial data\nconsist.log_dataframe(df1, key=\"data\", schema=MySchema)\n\n# Run 2: Additional data (same key, same schema)\nconsist.log_dataframe(df2, key=\"data\", schema=MySchema)\n</code></pre> <p>Both sets are ingested and queryable:</p> <pre><code>with Session(tracker.engine) as session:\n    # Both df1 and df2 are included\n    results = session.exec(select(MySchema)).all()\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#performance-tuning","level":2,"title":"Performance Tuning","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#batch-size","level":3,"title":"Batch Size","text":"<p>DLT loads data in batches. For large files, tune batch size:</p> <pre><code># In tracker initialization (future feature):\n# tracker = Tracker(..., dlt_batch_size=50000)\n\n# Or split manually:\nfor chunk in pd.read_csv(\"file.csv\", chunksize=50000):\n    consist.log_dataframe(chunk, key=\"data\", schema=MySchema)\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#indexing","level":3,"title":"Indexing","text":"<p>After ingestion, create indexes for frequently queried columns:</p> <pre><code># Manual index (in DuckDB):\nwith tracker.engine.begin() as conn:\n    conn.exec_driver_sql(\n        \"CREATE INDEX idx_person_run ON global_tables.person(consist_run_id)\"\n    )\n</code></pre>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#file-format-choice","level":3,"title":"File Format Choice","text":"<ul> <li>Parquet: Faster loading, better compression, typed columns → preferred</li> <li>CSV: Human-readable, larger files, slower parsing → use for interchange</li> </ul>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#deduplication","level":3,"title":"Deduplication","text":"<p>If your pipeline generates duplicate records, deduplicate before ingestion:</p> <pre><code>df = df.drop_duplicates(subset=[\"id\"])\n</code></pre> <p>Consist does not currently deduplicate automatically.</p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#common-errors","level":2,"title":"Common Errors","text":"","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#schema-mismatch","level":3,"title":"Schema Mismatch","text":"<pre><code>Error: Column 'age' expected int, got str\n</code></pre> <p>Fix: Ensure DataFrame types match schema: <pre><code>df[\"age\"] = df[\"age\"].astype(\"int64\")\n</code></pre></p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#missing-required-column","level":3,"title":"Missing Required Column","text":"<pre><code>Error: Column 'person_id' required but missing\n</code></pre> <p>Fix: Add column or make it optional: <pre><code>df[\"person_id\"] = df.index + 1  # Add column\n# OR\nclass MySchema(SQLModel, table=True):\n    person_id: Optional[int]  # Make optional\n</code></pre></p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#primary-key-violation","level":3,"title":"Primary Key Violation","text":"<pre><code>Error: Duplicate primary key value\n</code></pre> <p>Fix: Deduplicate before ingestion: <pre><code>df = df.drop_duplicates(subset=[\"id\"])\n</code></pre></p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#dlt-not-installed","level":3,"title":"DLT Not Installed","text":"<pre><code>ImportError: No module named 'dlt'\n</code></pre> <p>Fix: Install the ingest extras: <pre><code>pip install \"consist[ingest]\"\n</code></pre></p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#null-in-non-optional-field","level":3,"title":"Null in Non-Optional Field","text":"<pre><code>Warning: Null value in non-optional field 'age'\n</code></pre> <p>Fix (with schema enforcement): Ensure no nulls: <pre><code>df = df.dropna(subset=[\"age\"])\n\n# OR make optional:\nclass MySchema(SQLModel, table=True):\n    age: Optional[int]\n</code></pre></p>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#comparison-dlt-vs-direct-logging","level":2,"title":"Comparison: DLT vs Direct Logging","text":"Feature DLT Direct Logging Schema validation ✅ Yes ❌ No Cross-run SQL queries ✅ Yes ❌ No Type enforcement ✅ Yes ❌ No Setup overhead ⚠️ Moderate ✅ Minimal Best for Analytics, large data Simple results Example use case Population, trips table Single analysis output","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"dlt-loader-guide/#see-also","level":2,"title":"See Also","text":"<ul> <li>Usage Guide: Ingestion &amp; Hybrid Views</li> <li>Schema Export</li> <li>Architecture: Data Virtualization</li> <li>dlt Documentation</li> </ul>","path":["Core Topics","DLT Loader Integration Guide"],"tags":[]},{"location":"examples/","level":1,"title":"Examples","text":"<p>Start with the quickstart notebook, then move into the two tutorial flows. Each notebook is runnable top-to-bottom and writes its outputs under <code>examples/runs/</code>.</p>","path":["Getting Started","Examples"],"tags":[]},{"location":"examples/#quickstart","level":2,"title":"Quickstart","text":"<p>Quickstart: 00_quickstart.ipynb</p> <ul> <li>Shortest \"happy path\" walkthrough.</li> <li>Basic runs, artifacts, and simple queries.</li> </ul>","path":["Getting Started","Examples"],"tags":[]},{"location":"examples/#tutorial-series","level":2,"title":"Tutorial series","text":"<p>Parameter sweep: 01_parameter_sweep_monte_carlo.ipynb</p> <ul> <li>Parameter sweep with provenance.</li> <li>Schema export to SQLModel stubs.</li> <li>Typed, cross-run analysis via views.</li> </ul> <p>Iterative workflow: 02_iterative_workflows.ipynb</p> <ul> <li>Scenario workflows with feedback loops.</li> <li>Provenance queries across iterations.</li> </ul> <p>Transportation workflow (detailed): 03_transportation_demand_modeling.ipynb</p> <ul> <li>End-to-end modeling pipeline example.</li> <li>Scenario comparison with cached steps.</li> </ul>","path":["Getting Started","Examples"],"tags":[]},{"location":"examples/#notes","level":2,"title":"Notes","text":"<ul> <li>For an overview of notebook structure and helper modules, see   examples/README.md.</li> <li>The notebooks require <code>pip install -e \".[dev]\"</code> and Jupyter.</li> </ul>","path":["Getting Started","Examples"],"tags":[]},{"location":"glossary/","level":1,"title":"Glossary","text":"<p>This page defines key terms used in Consist documentation.</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#artifact","level":2,"title":"Artifact","text":"<p>A file (CSV, Parquet, HDF5, etc.) produced or used by a run, with metadata attached to track its origin and integrity. Think of it as a \"named, tracked file\" that Consist remembers who created it and can verify hasn't been corrupted.</p> <p>Artifacts record: - The file path and format (CSV, Parquet, HDF5, etc.) - Which run created or used it (for inputs) - A content hash (SHA256) for integrity checking - Optional ingestion status (whether it was stored in DuckDB)</p> <p>Research example: When you publish results (a transportation demand forecast, climate projections, or zoning capacity map), each output file is an Artifact. You can ask \"who created this file?\" (<code>consist lineage traffic_volumes</code>), verify it hasn't been corrupted, and trace it back to the exact code version and config that produced it.</p> <p>See also: Run, Provenance, Ingestion</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#cache-hit","level":2,"title":"Cache Hit","text":"<p>When Consist skips execution because it finds a previous run with an identical signature. The cached results (artifact metadata and optionally file copies) are returned instantly.</p> <p>Example: You run a function with config <code>{\"threshold\": 0.5}</code> on input <code>data.csv</code>, then re-run with the exact same config and input. Second execution is a cache hit—no computation happens.</p> <p>Research example: In a parameter sweep (testing 20 demand elasticity values), the first run re-executes your demand model. Runs 2-20 are all cache hits for preprocessing (same input data, same code) but misses for the demand model (different elasticity). Consist skips 19 preprocessing steps, saving hours.</p> <p>Opposite: Cache miss (run must execute)</p> <p>See also: Signature, Cache Miss</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#cache-miss","level":2,"title":"Cache Miss","text":"<p>When Consist cannot find a matching cached result, so the function must execute. The outputs are recorded as a new run and new artifacts are created.</p> <p>See also: Cache Hit, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#canonical-hashing","level":2,"title":"Canonical Hashing","text":"<p>Converting configuration data (dicts, YAML, etc.) into a single, consistent fingerprint, regardless of field order or how numbers are formatted. This ensures <code>{\"a\": 1, \"b\": 2}</code> and <code>{\"b\": 2, \"a\": 1}</code> produce the same hash, so Consist treats them as identical configurations.</p> <p>Why it matters: Without canonical hashing, the same config in different orders would produce different cache keys, breaking reproducibility.</p> <p>See also: Signature, Config</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#config","level":2,"title":"Config","text":"<p>Dictionary of parameters that affect computation, and are included in the cache signature. Example:</p> <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=my_model,\n        config={\"year\": 2030, \"scenario\": \"baseline\"},  # Part of signature\n        inputs={...},\n        outputs=[...]\n    )\n</code></pre> <p>Changing config invalidates cache and triggers re-execution. Config is hashed (not stored as-is) to allow large nested dictionaries.</p> <p>Research example: Your config might be a 50MB ActivitySim parameter file. Consist hashes it into the cache key, so if a colleague changes a mode choice coefficient, Consist automatically knows to re-run your demand model (cache miss). Without this, you'd have to manually remember which config changes require re-running which steps.</p> <p>See also: Facet, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#coupler","level":2,"title":"Coupler","text":"<p>A Consist pattern for orchestrating multi-step workflows. A coupler passes artifacts from one step's outputs to the next step's inputs, linking them through scenario trees. Useful for complex loops and iterative computations.</p> <p>Example: In a transportation model with feedback loops (trip distribution → mode choice → assignment → congestion update), couplers link each step's outputs to the next step's inputs.</p> <p>See also: Scenario, Run, Trace</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#dlt-data-load-tool","level":2,"title":"DLT (Data Load Tool)","text":"<p>An optional Python library for loading data into data warehouses. Consist integrates with DLT to materialize artifacts into DuckDB with provenance columns (run_id, artifact_id, etc.). Using DLT is optional; you can ingest manually or load files directly.</p> <p>See also: Ingestion, Materialization</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#facet","level":2,"title":"Facet","text":"<p>A small, queryable subset of configuration (e.g., <code>{\"year\": 2030, \"scenario\": \"baseline\"}</code>) that's indexed in DuckDB so you can filter runs. Use facets when you want to ask \"show me all runs where year=2030\" without storing the entire 50MB config file.</p> <p>Unlike identity config (which is hashed into the cache key), facets are stored queryably in the database for filtering and analysis.</p> <p>Use case: Your config is a 10 MB YAML file (too large to store). You extract <code>{\"year\": 2030, \"parking_cost\": 5.0}</code> as facets and query runs by year/parking_cost.</p> <p>Example: <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=my_model,\n        config={\"huge_model_config\": ...},  # Not queryable directly (too large)\n        facet={\"year\": 2030, \"scenario\": \"baseline\"},  # Indexed and queryable\n        inputs={...},\n        outputs=[...]\n    )\n\n# Later: query all 2030 runs\ndf = tracker.find_runs(facet_year=2030)\n</code></pre></p> <p>Research example: You're running demand models for 10 years (2020-2050) × 3 scenarios (baseline, transit-friendly, congestion pricing). You set <code>facet={\"year\": 2030, \"scenario\": \"transit-friendly\"}</code> for each run. Later, your colleague asks \"show me all 2040 sensitivity tests.\" You query <code>facet_year=2040</code> and get 50 runs instantly—no manual searching through 500 run directories.</p> <p>See also: Config, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#ghost-mode","level":2,"title":"Ghost Mode","text":"<p>Consist's ability to recover artifacts that exist only in the provenance database (DuckDB), not as physical files. If you delete the original file but it was ingested, you can still load it via <code>consist.load(artifact)</code> (returns a DuckDB Relation) or <code>consist.load_df(artifact)</code>.</p> <p>Use case: Re-running a pipeline against archived input data. If inputs were previously ingested, Consist recovers them from the database instead of requiring the original files.</p> <p>See also: Ingestion, Materialization</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#hydration","level":2,"title":"Hydration","text":"<p>Recovering the metadata and location information about a previous run's output without copying the file bytes. On a cache hit, Consist \"hydrates\" the output artifact so you know where it came from and can access it, but doesn't necessarily copy it to your current run directory.</p> <p>Hydration ≠ copying files. A hydrated artifact has provenance metadata but may not have file bytes copied to the new run's directory. By default, Consist recovers the information but doesn't copy files (saves disk space). You opt in to copying files when needed.</p> <p>See also: Materialization, Cache Hit</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#identity-config","level":2,"title":"Identity Config","text":"<p>The full set of configuration parameters that affect a run's cache signature; if identity config changes, the run must re-execute. Unlike facets (which are just for querying), identity config is hashed into the cache key to ensure cache invalidation when parameters change.</p> <p>Example: If your <code>config</code> dict contains <code>{\"year\": 2030, \"mode_choice_coefficient\": 0.5}</code>, both values are hashed into the signature. Changing either value invalidates cache.</p> <p>See also: Config, Facet, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#ingestion","level":2,"title":"Ingestion","text":"<p>Loading artifact data into DuckDB for SQL-native analysis. Optional; you can use Consist without ingesting (just track files).</p> <p>Use case: You want to query 50 Parquet files across 50 runs in SQL without loading them all into memory.</p> <p>Process: 1. Artifact is created/logged by a run 2. <code>tracker.ingest(artifact, data=df)</code> stores the data in DuckDB 3. Later: query in SQL across all ingested data</p> <p>See also: Materialization, Ghost Mode, Hybrid View</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#lineage","level":2,"title":"Lineage","text":"<p>The complete dependency chain showing where a result came from. Lineage tracks: which run created an artifact, which inputs that run used, which runs created those inputs, etc.</p> <p>Example: <code>consist lineage traffic_volumes</code> shows: <pre><code>traffic_volumes (artifact)\n├── created by: traffic_simulation run\n│   ├── input: assigned_trips\n│   │   └── created by: assignment run\n│   │       └── input: trip_tables\n│   │           └── created by: mode_choice run\n│   │               └── ...\n</code></pre></p> <p>See also: Provenance, Run, Artifact</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#merkle-dag","level":2,"title":"Merkle DAG","text":"<p>A chain of computations where each step's inputs are linked to the outputs of previous steps, creating an unbreakable record of data lineage. Like a railroad consist (the specific order of locomotives and cars), each simulation year depends on the previous year's output.</p> <p>Why it matters: This structure enables Consist to detect when cached results are valid (all upstream inputs haven't changed) and to recover missing data if needed.</p> <p>See also: Signature, Lineage, Artifact</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#materialization","level":2,"title":"Materialization","text":"<p>Saving the actual bytes of a data file into DuckDB (the provenance database) so it's recoverable even if the original file gets deleted. If you ingest a 10GB result file, DuckDB stores a copy so you can retrieve it later without the original file.</p> <p>Materialization = copying bytes into the database Hydration = recovering metadata without bytes</p> <p>See also: Hydration, Ingestion, Ghost Mode</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#provenance","level":2,"title":"Provenance","text":"<p>Complete history of where a result came from: code version, configuration, input data, and compute environment. Consist records provenance automatically for every run.</p> <p>Why it matters: Reproducibility (\"Can I re-run this exactly?\"), Accountability (\"Which config made this figure?\"), Debugging (\"Why did this change?\")</p> <p>Research example: You published a land-use forecast that shows 10% job growth in downtown. A policy maker asks \"is that Figure 3a or Figure 3b from the report?\" You run <code>consist show &lt;run_id&gt;</code> and instantly see: code version (commit SHA), exact config parameters (zoning policy, density cap), which parcel survey data, and when it ran. You can reproduce it exactly or change one parameter and show the difference. Without provenance, you'd spend hours reconstructing assumptions.</p> <p>See also: Artifact, Lineage, Signature</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#run","level":2,"title":"Run","text":"<p>A single execution of a tracked function or workflow step. A run records: - Input artifacts and configuration - Execution status (completed, failed, cached) - Output artifacts - Timing (start/end times) - Tags and metadata</p> <p>Example: <pre><code>import consist\nfrom consist import use_tracker\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean_data,\n        inputs={\"raw_path\": \"raw.csv\"},\n        config={\"threshold\": 0.5},\n        outputs=[\"cleaned\"],\n    )\n</code></pre></p> <p>This creates a Run with one input artifact, one config dict, and one output artifact.</p> <p>Research example: In climate modeling, each year's downscaling (e.g., 2030 temperature downscaling) is a separate Run. The provenance database stores 20 runs (2031-2050) all under one scenario, linked by data flow (year 2030's output feeds year 2031's input). You can query: \"which runs used GCM model X?\" or \"what was the total compute time across all 2050 runs?\" or \"trace 2050's data back to the original global model.\"</p> <p>See also: Artifact, Scenario</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#scenario","level":2,"title":"Scenario","text":"<p>A grouping of related runs. Scenarios are useful for organizing multi-variant studies or iterative workflows.</p> <p>Example: \"baseline_2030\" scenario contains 5 related runs: - Year 2030, baseline policy, iteration 0 - Year 2030, baseline policy, iteration 1 - Year 2030, baseline policy, iteration 2 - ...</p> <p>Important: Consist uses \"scenario\" differently from policy modeling jargon. In Consist, a scenario is a parent run grouping; in transportation modeling, \"baseline scenario\" and \"growth scenario\" are policy variants. Don't confuse the two.</p> <p>See also: Run, Coupler, Trace</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#signature","level":2,"title":"Signature","text":"<p>A unique fingerprint of a run created by hashing together your code version, configuration parameters, and input data. It's like a \"run ID\" that Consist compares across executions to detect when the same inputs, code, and config have been run before.</p> <p>How it works: 1. Function code is hashed (git commit SHA + modified files) 2. Config dict is hashed deterministically (canonical hashing) 3. Input file hashes are computed 4. All three are combined: <code>signature = SHA256(code + config + inputs)</code> 5. This signature is the cache key</p> <p>Why: If you re-run with the same signature, Consist knows the result will be the same, so it returns the cached version instantly. Identical signatures = identical outputs (assuming deterministic functions).</p> <p>See also: Cache Hit, Config, Artifact, Canonical Hashing</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#trace","level":2,"title":"Trace","text":"<p>The execution path through a multi-step workflow, showing which runs were executed, which were cache hits, and what artifacts were passed between them.</p> <p>See also: Scenario, Coupler, Lineage</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#virtualization-data-virtualization","level":2,"title":"Virtualization (Data Virtualization)","text":"<p>Querying multiple artifacts as if they were a single table, without loading all data into memory. DuckDB handles data movement lazily.</p> <p>Example: Query 50 Parquet files across 50 runs: <pre><code>SELECT year, mode, COUNT(*) as trips\nFROM consist_view_trips\nWHERE scenario IN ('baseline', 'high_growth')\nGROUP BY year, mode\n</code></pre></p> <p>Consist creates a virtual SQL view that queries each file as needed, not loading all at once.</p> <p>See also: Hybrid View, Ingestion</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"glossary/#hybrid-view","level":2,"title":"Hybrid View","text":"<p>A SQL view that combines: 1. Hot data: Ingested artifacts stored in DuckDB 2. Cold data: Raw files (Parquet, CSV) queried on-the-fly</p> <p>Hybrid views let you query across runs without requiring all data to be ingested, reducing storage overhead.</p> <p>See also: Ingestion, Virtualization</p>","path":["Support & Reference","Glossary"],"tags":[]},{"location":"ingestion-and-hybrid-views/","level":1,"title":"Ingestion &amp; Hybrid Views","text":"<p>Consist supports two storage modes for tabular data:</p> <ul> <li>Cold data: keep files on disk and load from their paths.</li> <li>Hot data: ingest into DuckDB for fast queries, schema tracking, and optional recovery.</li> </ul> <p>Hybrid views let you query both modes through a single SQLModel view.</p>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#when-to-ingest","level":2,"title":"When to ingest","text":"<p>Ingest when you want: - Fast, SQL-native analytics over many runs. - Schema tracking and exportable SQLModel stubs. - The ability to recover tabular data if files are missing.</p> <p>Stay cold when you want: - Minimal DB size (data stays in files, not duplicated in DuckDB). - Large binary formats that are not tabular (e.g., rasters, binary blobs).</p> <p>“Cold” means the data lives only on disk paths; you keep provenance metadata, but you cannot query it in SQL or include it in hybrid views until you ingest it.</p>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#ingesting-artifacts","level":2,"title":"Ingesting artifacts","text":"<p>Ingestion uses the optional DLT dependency:</p> <pre><code>pip install \"consist[ingest]\"\n</code></pre> <p>Basic usage:</p> <pre><code>from consist import Tracker\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\nwith tracker.start_run(\"ingest_example\", model=\"demo\"):\n    artifact = tracker.log_artifact(\"data/people.csv\", key=\"people\", driver=\"csv\")\n    tracker.ingest(artifact)\n</code></pre> <p>You can also ingest explicit data payloads (dicts, DataFrames, generators):</p> <pre><code>with tracker.start_run(\"ingest_inline\", model=\"demo\"):\n    artifact = tracker.log_artifact(\"inputs/people.json\", key=\"people\")\n    tracker.ingest(artifact, data=[{\"id\": 1, \"name\": \"A\"}])\n</code></pre>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#strict-schema-ingestion","level":2,"title":"Strict schema ingestion","text":"<p>Provide a SQLModel schema to validate incoming data:</p> <pre><code>from sqlmodel import SQLModel, Field\n\nclass Person(SQLModel, table=True):\n    __tablename__ = \"people\"\n    id: int = Field(primary_key=True)\n    name: str\n\nwith tracker.start_run(\"strict_ingest\", model=\"demo\"):\n    artifact = tracker.log_artifact(\"data/people.csv\", key=\"people\", schema=Person)\n    tracker.ingest(artifact, schema=Person)\n</code></pre> <p>Strict mode rejects incompatible data rather than silently coercing it.</p>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#provenance-columns","level":2,"title":"Provenance columns","text":"<p>Ingested tables include Consist system columns such as: - <code>consist_run_id</code>, <code>consist_year</code>, <code>consist_iteration</code> - <code>consist_artifact_id</code></p> <p>These enable joins across runs and allow hybrid views to filter by scenario or year.</p>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#hybrid-views-hot-cold","level":2,"title":"Hybrid views (hot + cold)","text":"<p>Hybrid views are SQL views that union: - Hot data ingested into DuckDB tables. - Cold data read directly from files (CSV/Parquet) on disk.</p> <p>To activate a view, register a SQLModel schema:</p> <pre><code>from sqlmodel import SQLModel, Field\n\nclass Person(SQLModel, table=True):\n    __tablename__ = \"people\"\n    person_id: int = Field(primary_key=True)\n    age: int\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person],\n)\n\nVPerson = tracker.views.Person\n</code></pre> <p>Once registered, you can query across all runs:</p> <pre><code>from sqlmodel import select, func\nimport consist\n\nquery = (\n    select(VPerson.consist_year, func.avg(VPerson.age).label(\"avg_age\"))\n    .group_by(VPerson.consist_year)\n)\nrows = consist.run_query(query, tracker=tracker)\n</code></pre>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#schema-export-workflow","level":2,"title":"Schema export workflow","text":"<p>If you want a curated schema with explicit PK/FK constraints:</p> <p>1) Ingest tabular data. 2) Run schema export to generate a SQLModel stub. 3) Edit the stub and register it for views.</p> <p>See <code>docs/schema-export.md</code> for details.</p>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#loading-behavior-and-db-fallback","level":2,"title":"Loading behavior and DB fallback","text":"<p><code>consist.load(...)</code> normally reads from the filesystem. If files are missing and the artifact was ingested, DB fallback can recover data depending on the policy:</p> <ul> <li><code>db_fallback=\"inputs-only\"</code> (default): only inside an active run and for declared inputs.</li> <li><code>db_fallback=\"always\"</code>: allow recovery whenever the artifact is ingested.</li> <li><code>db_fallback=\"never\"</code>: disable recovery.</li> </ul> <p>Example:</p> <pre><code>df = consist.load_df(artifact, tracker=tracker, db_fallback=\"always\")\n</code></pre>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#loader-kwargs-for-tabular-artifacts","level":2,"title":"Loader kwargs for tabular artifacts","text":"<p><code>consist.load(...)</code> validates loader kwargs for tabular drivers and will raise a <code>ValueError</code> if you pass unsupported options.</p> <p>Supported kwargs by driver: - parquet: <code>columns</code> - csv: <code>columns</code>, <code>delimiter</code>, <code>header</code> (alias: <code>sep</code> -&gt; <code>delimiter</code>) - json: <code>orient</code>, <code>dtype</code>, <code>convert_axes</code>, <code>convert_dates</code>, <code>precise_float</code>,   <code>date_unit</code>, <code>encoding</code>, <code>lines</code>, <code>compression</code>, <code>typ</code> - h5_table: <code>columns</code>, <code>where</code>, <code>start</code>, <code>stop</code></p> <p>Use <code>consist.load_relation(...)</code> if you want a DuckDB Relation and will manage its lifecycle explicitly. Use <code>consist.load_df(...)</code> for a DataFrame and automatic cleanup.</p>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#hdf5-native-support-roadmap","level":2,"title":"HDF5 native support roadmap","text":"<p>Today, <code>h5_table</code> uses a staging bridge (<code>pandas.read_hdf(...)</code> + <code>conn.from_df(...)</code>). Native HDF5 support will replace that staging step with DuckDB's HDF5 extension so DuckDB can query HDF5 tables directly.</p> <p>Still needed for native HDF5: - Switch <code>h5_table</code> loading to <code>hdf5_read(...)</code> in DuckDB once the extension is stable. - Validate behavior and version pinning for the DuckDB HDF5 extension. - Handle PyTables/object dtype edge cases (or document explicit fallbacks). - Update schema capture to run <code>DESCRIBE</code> on the native relation path. - Add performance/regression testing for large HDF5 tables.</p> <p>What native HDF5 enables: - Zero-copy, streaming reads from HDF5 into DuckDB (no pandas materialization). - Lower memory usage and faster queries for large HDF5 tables. - Consistent relation-first SQL workflows across Parquet/CSV/HDF5.</p>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"ingestion-and-hybrid-views/#best-practices","level":2,"title":"Best practices","text":"<ul> <li>Ingest key tabular outputs you will compare across runs (e.g., transportation: trip tables or skim summaries; climate: aggregated metrics like monthly averages; urban planning: parcel or zoning summaries). Rule of thumb: if it is &lt;1GB and you expect to compare it, ingest it.</li> <li>Keep raw binary outputs cold, and use mounts for portability.</li> <li>Register SQLModel schemas for any concept you want to query via views.</li> <li>Use schema export to bootstrap models, then curate them.</li> </ul>","path":["Core Topics","Ingestion &amp; Hybrid Views"],"tags":[]},{"location":"mounts-and-portability/","level":1,"title":"Mounts &amp; Portability","text":"<p>Consist stores portable URIs instead of absolute filesystem paths so runs can move between machines without breaking lineage. This page explains how mounts, workspace URIs, and historical path resolution work.</p>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#getting-started-with-mounts","level":2,"title":"Getting Started with Mounts","text":"<p>When you set up Consist for your research project, you'll define mounts — mappings from short names to directories on your filesystem. These mounts let each team member keep their data in different locations while sharing a common provenance database.</p> <p>The Workflow</p> <ol> <li>Identify your data directories: Where are your inputs? Where do you write outputs? Where is temporary scratch space?</li> <li>Assign mount names: Choose memorable names like <code>inputs</code>, <code>outputs</code>, <code>scratch</code>, <code>shared</code>.</li> <li>Define mounts in your Tracker: Map those names to real paths on each person's machine.</li> <li>Log artifacts under mounts: Instead of absolute paths, use URIs like <code>inputs://land_use.csv</code>.</li> </ol> <p>Example for a Research Team</p> <p>Suppose your team shares an ActivitySim project:</p> <pre><code>from consist import Tracker\nfrom pathlib import Path\n\n# Shared setup (agreed upon by the team)\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/shared/data/activitysim_inputs\",      # Shared NFS mount\n        \"outputs\": \"/local/activitysim_outputs\",          # Local SSD for speed\n        \"scratch\": \"/scratch/users/YOUR_USERNAME\",        # Temporary workspace\n    },\n)\n</code></pre> <p>On each team member's machine, the paths differ but mount names stay the same:</p> <pre><code># Alice's setup\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/mnt/nfs/activitysim_inputs\",          # Alice's NFS mount point\n        \"outputs\": \"/home/alice/activitysim_outputs\",\n        \"scratch\": \"/scratch/alice\",\n    },\n)\n\n# Bob's setup\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/data/nfs/inputs\",                     # Bob's mount point\n        \"outputs\": \"/var/cache/bob/outputs\",\n        \"scratch\": \"/tmp/bob_scratch\",\n    },\n)\n</code></pre> <p>When Alice runs a simulation and logs an output (inside a run context):</p> <pre><code>with tracker.start_run(\"asim_baseline\", model=\"activitysim\"):\n    consist.log_artifact(\n        Path(\"/home/alice/activitysim_outputs/results.parquet\"),\n        key=\"results\",\n        direction=\"output\",\n    )\n</code></pre> <p>Consist detects the mount and stores a portable URI:</p> <pre><code>outputs://results.parquet\n</code></pre> <p>When Bob retrieves this artifact, Consist resolves it using his mount configuration:</p> <pre><code>outputs:// → /var/cache/bob/outputs/ → /var/cache/bob/outputs/results.parquet\n</code></pre> <p>Benefits</p> <ul> <li>Portability: Runs move between machines without breaking lineage.</li> <li>Flexibility: Each user can customize their local paths for their hardware (local SSD, NFS, cloud storage).</li> <li>Shared provenance: A single <code>provenance.duckdb</code> file shared across the team still works even if underlying filesystems differ.</li> <li>Isolation: Temporary outputs stay local; shared data stays on shared infrastructure.</li> </ul>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#mounts-at-a-glance","level":2,"title":"Mounts at a glance","text":"<p>Mounts map a short scheme name to a real path on disk:</p> <pre><code>from consist import Tracker\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    mounts={\n        \"inputs\": \"/shared/inputs\",\n        \"scratch\": \"/scratch/users/MY_USERNAME\",\n    },\n)\n</code></pre> <p>When you log a path under a mount, Consist stores a URI such as:</p> <pre><code>inputs://land_use.csv\nscratch://temp/output.parquet\n</code></pre> <p>This keeps provenance portable and lets each user remap mounts on their machine.</p>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#workspace-uris-run-local-outputs","level":2,"title":"Workspace URIs (run-local outputs)","text":"<p>Paths under the run directory are stored relative to the active run:</p> <pre><code>./outputs/&lt;run_id&gt;/model.csv\n</code></pre> <p>Internally this is treated as a workspace URI and resolved using the run's <code>_physical_run_dir</code> metadata. That field records the absolute run directory used when the run executed.</p> <p>Implications: - Cache hits can hydrate artifacts even when the current run directory differs. - Moving or deleting the original run directory will break byte-level access,   but metadata-only cache hits still work.</p>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#historical-path-resolution","level":2,"title":"Historical path resolution","text":"<p>When Consist needs bytes from a historical run (e.g., cache hydration or <code>inputs-missing</code>), it resolves paths in this order:</p> <p>1) If the URI uses <code>workspace://</code> or <code>./</code>, resolve relative to the original run’s    <code>_physical_run_dir</code>. 2) If the URI uses a mount scheme (e.g., <code>inputs://</code>), resolve using the current    tracker mounts. 3) Otherwise, treat the URI as an absolute path.</p> <p>If a mount is missing or points somewhere else, materialization will warn and skip missing files rather than crashing (unless explicitly set to raise).</p>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#sharing-a-database-across-machines","level":2,"title":"Sharing a database across machines","text":"<p>Sharing a DuckDB provenance file across a team is supported, but you must keep mounts consistent in intent even if the physical paths differ.</p> <p>Recommended practice: - Agree on mount names (<code>inputs</code>, <code>outputs</code>, <code>scratch</code>, <code>shared</code>). - Each user maps those names to their local filesystem. - Store the DB in a shared location with write access controls.</p> <p>If a user hits a cache hit but cannot access the source filesystem, Consist will log a warning and proceed without materializing the files.</p>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#best-practices","level":2,"title":"Best practices","text":"<ul> <li>Prefer mounts for shared data directories; avoid absolute paths in artifacts.</li> <li>Keep run directories local and disposable; treat cached outputs as rehydratable.</li> <li>Use <code>cache_hydration=\"outputs-requested\"</code> for only the outputs you need.</li> <li>Use <code>cache_hydration=\"inputs-missing\"</code> to backfill inputs when a run moves   across machines or directories.</li> </ul>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"mounts-and-portability/#troubleshooting","level":2,"title":"Troubleshooting","text":"<ul> <li>Missing file on cache hit: Check that mounts map to the correct root and the   original run directory still exists for workspace URIs.</li> <li>Moved run directory: Cache metadata is still valid, but byte materialization   will warn because <code>_physical_run_dir</code> no longer points to the original location.</li> <li>Permission denied: Consist warns and continues; adjust mount permissions or   use a shared accessible path for cached outputs you need to materialize.</li> </ul>","path":["Core Topics","Mounts &amp; Portability"],"tags":[]},{"location":"schema-export/","level":1,"title":"Schema Export (SQLModel Stubs)","text":"<p>Consist can capture the observed (post-ingest) schema of tabular artifacts and export that schema as a static SQLModel class stub you can commit and edit. This reduces “retype the table definition” friction while keeping Consist honest about what it can and cannot infer.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#what-you-get","level":2,"title":"What You Get","text":"<ul> <li>A Python 3.11 file containing one SQLModel class stub (exported as <code>table=True</code>).</li> <li>Columns with:</li> <li>a best-effort type mapping from DuckDB logical types</li> <li>nullability (<code>Optional[...] = None</code> only when nullable)</li> <li>deterministic ordering (prefers <code>ordinal_position</code>)</li> <li>“Hints” as comments (enums/stats) when available (on by default).</li> <li>Messy column names are handled: the generated attribute name is sanitized, but the original DB column name is preserved via <code>Field(sa_column=Column(\"original\", ...))</code> when needed.</li> <li>Foreign keys are preserved if they were provided in a curated SQLModel schema (rendered as <code>Field(foreign_key=\"table.column\")</code>).</li> </ul>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#abstract-vs-concrete-exports","level":2,"title":"Abstract vs Concrete Exports","text":"<p>By default, Consist exports SQLModel classes as abstract (<code>__abstract__ = True</code>). This keeps the stub importable immediately, because SQLAlchemy requires a primary key to map a concrete table/view, and many analysis tables don’t have an obvious primary key.</p> <ul> <li>Default (recommended): abstract export, safe to import, great for view schemas.</li> <li>Concrete export (advanced): export with <code>--concrete</code> (or <code>abstract=False</code> in Python) and then add a primary key before importing, otherwise SQLAlchemy will raise an error like “could not assemble any primary key columns”.</li> </ul>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#what-you-still-do-manually","level":2,"title":"What You Still Do Manually","text":"<p>Consist does not infer semantic intent. You typically edit the stub to add:</p> <ul> <li>primary keys / foreign keys</li> <li>indexes / uniqueness constraints</li> <li>relationships</li> <li>renames / normalization decisions</li> </ul> <p>If you pass a curated SQLModel schema to <code>log_artifact(..., schema=YourModel)</code> and it includes <code>foreign_key=...</code>, those FKs are persisted and will be re-emitted during export.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#prerequisites","level":2,"title":"Prerequisites","text":"<p>Schema export uses the schema captured from a DuckDB-ingested table. In practice:</p> <ol> <li>You run with a database configured (<code>db_path=...</code>).</li> <li>You ingest tabular data into DuckDB (hot data).</li> <li>Consist profiles the ingested table and stores a deduped schema referenced by <code>artifact.meta[\"schema_id\"]</code>.</li> </ol> <p>If you already see <code>schema_id</code> in an artifact’s <code>meta</code>, you’re ready to export.</p> <p>You can also capture schemas without ingestion for CSV/Parquet artifacts by enabling lightweight file profiling at log time (<code>profile_file_schema=True</code>, optional <code>file_schema_sample_rows=</code>). This writes the same <code>schema_id</code> pointer into <code>artifact.meta</code>, allowing schema export even if the original file is later missing or moved.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#exporting-from-the-cli","level":2,"title":"Exporting from the CLI","text":"<p>Export by artifact UUID (recommended UX for now):</p> <pre><code>python -m consist.cli schema export \\\n  --artifact-id 00000000-0000-0000-0000-000000000000 \\\n  --out your_pkg/models/persons.py\n</code></pre> <p>Or export by schema id (hash):</p> <pre><code>python -m consist.cli schema export \\\n  --schema-id &lt;schema-hash&gt; \\\n  --out your_pkg/models/persons.py\n</code></pre> <p>Useful flags:</p> <ul> <li><code>--class-name Persons</code> to override the generated class name</li> <li><code>--table-name persons</code> to override <code>__tablename__</code></li> <li><code>--include-system-cols</code> to include system/ingestion columns like <code>consist_*</code> and <code>_dlt_*</code></li> <li><code>--no-stats-comments</code> to omit stats/enum hint comments</li> <li><code>--concrete</code> to export a non-abstract mapped class (you must add a primary key)</li> </ul> <p>If <code>--out</code> is omitted, the stub is printed to stdout (so it can be piped or redirected).</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#exporting-from-python","level":2,"title":"Exporting from Python","text":"<p>You can generate the code (and optionally write it) from a <code>Tracker</code>:</p> <pre><code>from pathlib import Path\nfrom consist import Tracker\n\ntracker = Tracker(run_dir=\".\", db_path=\"provenance.duckdb\")\n\ncode = tracker.export_schema_sqlmodel(\n    artifact_id=\"00000000-0000-0000-0000-000000000000\",\n    out_path=Path(\"your_pkg/models/persons.py\"),\n    abstract=True,\n)\n</code></pre> <p>The method returns the generated string regardless of whether you write it.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#besteffort-foreign-key-enforcement","level":2,"title":"Best‑Effort Foreign Key Enforcement","text":"<p>If you want DuckDB to attempt to enforce persisted foreign keys, run:</p> <pre><code>python -m consist.cli schema apply-fks\n</code></pre> <p>This step is best‑effort: if a constraint can’t be added (e.g., due to missing parent rows), Consist logs a warning and continues.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#where-to-put-the-generated-file","level":2,"title":"Where to Put the Generated File","text":"<p>Place it anywhere importable by your project (on <code>PYTHONPATH</code>), for example:</p> <ul> <li><code>your_pkg/models/</code></li> <li><code>your_pkg/schemas/</code></li> </ul> <p>If it’s a package directory, include an <code>__init__.py</code> so you can <code>import your_pkg.models.persons</code>.</p> <p>Consist does not require a special directory layout; it only needs you to import the class and register it for views.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#using-the-generated-model-with-views","level":2,"title":"Using the Generated Model with Views","text":"<p>Consist’s hybrid views are created from your SQLModel schema. To activate a view:</p> <p>1) Import the model class. 2) Register it with the <code>Tracker</code> (at init time or after).</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#register-at-tracker-initialization","level":3,"title":"Register at Tracker initialization","text":"<pre><code>from consist import Tracker\nfrom your_pkg.models.persons import Persons\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\", schemas=[Persons])\nVPersons = tracker.views.Persons\n</code></pre>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#register-later","level":3,"title":"Register later","text":"<pre><code>from your_pkg.models.persons import Persons\n\nVPersons = tracker.view(Persons)  # registers + creates/refreshes the hybrid view\n</code></pre>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#querying","level":3,"title":"Querying","text":"<pre><code>from sqlmodel import select\nimport consist\n\nVPersons = tracker.views.Persons\nrows = consist.run_query(select(VPersons).limit(5), tracker=tracker)\n</code></pre> <p>Views automatically include Consist metadata columns (e.g. <code>consist_run_id</code>, <code>consist_year</code>, <code>consist_scenario_id</code>) for filtering and grouping.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#column-name-rules-important","level":2,"title":"Column Name Rules (Important)","text":"<p>The generated stub aggressively normalizes attribute names to reduce runtime errors:</p> <ul> <li>invalid characters become <code>_</code></li> <li>leading digits are prefixed with <code>_</code></li> <li>Python keywords get a trailing <code>_</code></li> <li>collisions become <code>__2</code>, <code>__3</code>, ...</li> </ul> <p>When the attribute name differs from the real column name, the stub uses:</p> <pre><code>mass_kg: float | None = Field(default=None, sa_column=Column(\"Mass(kg)\", ...))\n</code></pre> <p>This matters for views and empty-view typing: Consist now uses the original DB column name when creating typed empty views, so schemas with renamed attributes still behave correctly.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#matching-artifacts-tables-and-__tablename__","level":2,"title":"Matching Artifacts, Tables, and <code>__tablename__</code>","text":"<p>For views to “pick up” your data, the model’s <code>__tablename__</code> should match the concept key you are querying:</p> <ul> <li>for ingested (“hot”) data, this is typically the DuckDB table name used at ingest time</li> <li>for file-based (“cold”) data, this is typically the <code>Artifact.key</code></li> </ul> <p>If you want your curated model name to differ from the ingestion key, you can:</p> <ul> <li>override <code>--table-name</code> / <code>table_name=...</code> during export, or</li> <li>set <code>__tablename__ = \"...\"</code> manually after editing</li> </ul>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#system-and-ingestion-columns","level":2,"title":"System and Ingestion Columns","text":"<p>By default, exported stubs omit system/ingestion columns:</p> <ul> <li><code>consist_*</code> (Consist provenance columns)</li> <li><code>_dlt_*</code> (common ingestion/system columns)</li> </ul> <p>Use <code>--include-system-cols</code> if you need a faithful “as-ingested” representation.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"schema-export/#notes-on-wide-sparse-tables","level":2,"title":"Notes on Wide / Sparse Tables","text":"<p>Very wide tables can cause the stored JSON profile to truncate. Consist still persists per-field rows so schema export continues to work, and emits a warning when truncation occurs.</p> <p>If your upstream workflow produces sparse wide tables, consider reshaping to a long format before ingestion (or as a dedicated transformation step). Future Consist work may add first-class “pre/post ingestion transforms”, but schema export is designed to work even when the JSON blob is truncated.</p>","path":["Core Topics","Schema Export (SQLModel Stubs)"],"tags":[]},{"location":"troubleshooting/","level":1,"title":"Troubleshooting Guide","text":"<p>This guide covers common errors, their root causes, and solutions.</p>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#cache-provenance-issues","level":2,"title":"Cache &amp; Provenance Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#relation-leak-warnings","level":3,"title":"\"Relation leak warnings\"","text":"<p>Symptom: Warning: <code>Consist has N active DuckDB relations...</code></p> <p>Root Cause: Relations returned by <code>consist.load(...)</code> (tabular artifacts) keep a DuckDB connection open until you close them.</p> <p>Solution:</p> <ul> <li>Prefer <code>consist.load_df(...)</code> if you only need a pandas DataFrame.</li> <li>Use <code>consist.load_relation(...)</code> as a context manager to ensure connections are closed.</li> <li>If you're intentionally holding many Relations, increase the warning threshold:   <code>CONSIST_RELATION_WARN_THRESHOLD=500</code>.</li> </ul>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#old-dbs-no-longer-load-after-the-relation-first-refactor","level":3,"title":"\"Old DBs no longer load after the Relation-first refactor\"","text":"<p>Symptom: Errors when reading artifacts or querying the DB after upgrading.</p> <p>Root Cause: The artifact schema changed: - <code>Artifact.uri</code> → <code>Artifact.container_uri</code> - <code>Artifact.table_path</code> added (nullable) for container formats (HDF5 tables) - <code>Artifact.array_path</code> added (nullable) for array formats - <code>meta[\"table_path\"]</code> is no longer used</p> <p>Solution:</p> <p>Reset your Consist database(s) and re-run workflows:</p> <pre><code>rm ./provenance.duckdb\nrm ./test_db.duckdb\n</code></pre> <p>Then update any code that referenced <code>artifact.uri</code> or <code>artifact.meta[\"table_path\"]</code>:</p> <pre><code># Before\nartifact.uri\nartifact.meta.get(\"table_path\")\n\n# After\nartifact.container_uri\nartifact.table_path\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#cache-hit-but-output-files-are-missing","level":3,"title":"\"Cache hit but output files are missing\"","text":"<p>Symptom: <code>cache_hit=True</code> but <code>artifact.path</code> doesn't exist on disk.</p> <p>Root Cause: Consist returned a cache hit but didn't materialize the files to disk.</p> <p>Why this happens: Consist defaults to metadata-only cache hits to keep cache checks fast and avoid duplicating large files. You explicitly opt in to file copying via hydration/materialization when you need bytes on disk.</p> <p>Solution:</p> <p>Use cache hydration to copy files:</p> <pre><code>result = consist.run(\n    fn=my_function,\n    inputs={...},\n    cache_hydration=\"outputs-all\",  # Copy all cached outputs\n    ...\n)\n</code></pre> <p>Or materialize manually:</p> <pre><code>from pathlib import Path\n\nresult = consist.run(...)\nif result.cache_hit:\n    from consist.core.materialize import materialize_artifacts\n    artifacts_to_load = [\n        (artifact, Path(tracker.run_dir) / \"rehydrated\" / artifact.path.name)\n        for artifact in result.outputs.values()\n    ]\n    materialize_artifacts(tracker, artifacts_to_load)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#same-inputsconfig-but-cache-not-found","level":3,"title":"\"Same inputs/config but cache not found\"","text":"<p>Symptom: Code hasn't changed, inputs haven't changed, but run re-executes instead of hitting cache.</p> <p>Root Cause: Signature mismatch. Something in the cache key changed.</p> <p>Solution:</p> <p>Debug the signature:</p> <pre><code>from pathlib import Path\n\nidentity = tracker.identity\ncode_hash = identity.get_code_version()\n# If you want to match Consist's exact run hash, include model/year/iteration:\n# config_hash = identity.compute_run_config_hash(config={\"param\": value}, model=\"my_model\", year=2030)\nconfig_hash = identity.compute_config_hash({\"param\": value})\ninput_hash = identity.compute_file_checksum(Path(\"input.csv\"))\n\nprint(f\"Code: {code_hash}\")\nprint(f\"Config: {config_hash}\")\nprint(f\"Inputs: {input_hash}\")\n\n# Check if these match a prior run\nprior_runs = tracker.find_runs()\nfor run in prior_runs:\n    print(f\"Run {run.id}: signature={run.signature}\")\n</code></pre> <p>Common causes: - Code changed: Check git status, function definitions - Config changed: Check parameter types (0 vs 0.0, \"0\" vs 0) - Input file changed: Check file modification time, content hash - Run fields changed: <code>model</code>, <code>year</code>, or <code>iteration</code> are folded into the config hash - Dependencies changed: Installed package versions can affect behavior</p>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#how-do-i-clearreset-cache","level":3,"title":"\"How do I clear/reset cache?\"","text":"<p>Solution:</p> <p>Delete the database file:</p> <pre><code>rm ./provenance.duckdb\n</code></pre> <p>This clears all run history and cache. Next run will re-execute everything.</p> <p>To keep history but force re-execution:</p> <pre><code>result = consist.run(\n    fn=your_fn,\n    inputs={...},\n    outputs=[...],\n    cache_mode=\"overwrite\",\n)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#database-locked-error","level":3,"title":"\"Database locked\" error","text":"<p>Symptom: <code>database is locked</code> or similar error when running multiple Consist processes.</p> <p>Root Cause: DuckDB locks the database during writes. Concurrent write attempts fail.</p> <p>Solution:</p> <ol> <li> <p>Run sequentially (recommended):    <pre><code>python workflow1.py\npython workflow2.py\n</code></pre></p> </li> <li> <p>Use separate databases per process: <pre><code>tracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=f\"./provenance_{process_id}.duckdb\",  # Unique per process\n)\n</code></pre></p> </li> <li> <p>Increase lock timeout: <pre><code>import duckdb\nconn = duckdb.connect(\"provenance.duckdb\", timeout=30)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#mount-path-issues","level":2,"title":"Mount &amp; Path Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#mount-not-resolving-container-integration","level":3,"title":"\"Mount not resolving\" (Container integration)","text":"<p>Symptom: Container runs but <code>/inputs</code> is empty or doesn't exist.</p> <p>Root Cause: Volume mount paths don't exist or are incorrect.</p> <p>Solution:</p> <ol> <li> <p>Check paths exist on host: <pre><code>from pathlib import Path\nfor host_path in volumes.keys():\n    assert Path(host_path).exists(), f\"Missing: {host_path}\"\n</code></pre></p> </li> <li> <p>Use absolute paths: <pre><code># DON'T:\nvolumes={\"./data\": \"/inputs\"}\n\n# DO:\nvolumes={str(Path(\"./data\").resolve()): \"/inputs\"}\n</code></pre></p> </li> <li> <p>Check permissions: <pre><code>ls -la ./data\n# Must be readable by your user (and by Docker if using docker-in-docker)\n</code></pre></p> </li> <li> <p>Debug mount: <pre><code>docker run -it -v ./data:/inputs my-image ls -la /inputs\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#uri-resolution-failed","level":3,"title":"\"URI resolution failed\"","text":"<p>Symptom: Error like <code>Cannot resolve URI: outputs://key/file.csv</code></p> <p>Root Cause: URI scheme not recognized or mount not registered.</p> <p>Solution:</p> <p>Use absolute paths instead of URI schemes for file operations:</p> <pre><code># DON'T:\nartifact_uri = \"outputs://key/result.csv\"\ndf = pd.read_csv(artifact_uri)  # Fails\n\n# DO:\nwith tracker.start_run(\"resolve_uri\", model=\"example\"):\n    artifact = tracker.log_artifact(result_path, key=\"key\", direction=\"output\")\n    df = pd.read_csv(artifact.path)  # Use .path property\n</code></pre> <p>Or resolve URI explicitly:</p> <pre><code>resolved_path = tracker.resolve_uri(\"outputs://key/result.csv\")\ndf = pd.read_csv(resolved_path)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#working-directory-changed-between-runs","level":3,"title":"\"Working directory changed between runs\"","text":"<p>Symptom: File paths work in first run but fail in second run (re-run from different directory).</p> <p>Root Cause: Relative paths depend on current working directory.</p> <p>Solution:</p> <p>Use absolute paths everywhere:</p> <pre><code># DON'T:\noutput_file = \"results.csv\"  # Relative to cwd\n\n# DO:\noutput_file = Path(tracker.run_dir) / \"results.csv\"  # Absolute\n</code></pre> <p>Or use artifact URIs:</p> <pre><code>with tracker.start_run(\"log_output\", model=\"example\"):\n    tracker.log_artifact(result, key=\"output\", direction=\"output\")\n# Later, access via:\nartifact = tracker.get_artifacts_for_run(\"run_id\").outputs[\"output\"]\nprint(artifact.path)  # Absolute path\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#data-schema-issues","level":2,"title":"Data &amp; Schema Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#schema-mismatch-during-ingestion","level":3,"title":"\"Schema mismatch during ingestion\"","text":"<p>Symptom: Error like <code>Column 'age' expected int, got str</code></p> <p>Root Cause: DataFrame column type doesn't match schema definition.</p> <p>Solution:</p> <p>Convert DataFrame types before ingestion:</p> <pre><code>from your_pkg.models import MySchema\n\n# Check types\nprint(df.dtypes)\n\n# Convert if needed\ndf = df.astype({\n    \"age\": \"int64\",\n    \"income\": \"float64\",\n    \"name\": \"object\",\n})\n\nwith tracker.start_run(\"ingest_data\", model=\"example\"):\n    tracker.log_dataframe(df, key=\"data\", schema=MySchema)\n</code></pre> <p>Or use Pandas casting:</p> <pre><code>df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")  # Convert with fallback\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#null-in-non-optional-field","level":3,"title":"\"Null in non-optional field\"","text":"<p>Symptom: Warning like <code>Null value in non-optional field 'age'</code></p> <p>Root Cause: DataFrame has NaN/None in a field that schema requires non-null.</p> <p>Solution:</p> <ol> <li> <p>Drop nulls: <pre><code>df = df.dropna(subset=[\"age\"])\n</code></pre></p> </li> <li> <p>Fill nulls: <pre><code>df[\"age\"] = df[\"age\"].fillna(0)  # Default value\n</code></pre></p> </li> <li> <p>Make field optional: <pre><code>from typing import Optional\n\nclass MySchema(SQLModel, table=True):\n    age: Optional[int]  # Can be None\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#duplicate-primary-keys","level":3,"title":"\"Duplicate primary keys\"","text":"<p>Symptom: Error like <code>Primary key violation: duplicate ID</code></p> <p>Root Cause: DataFrame has duplicate values in the primary key column.</p> <p>Solution:</p> <p>Deduplicate before ingestion:</p> <pre><code># Keep last occurrence (or \"first\")\ndf = df.drop_duplicates(subset=[\"id\"], keep=\"last\")\n\n# Or remove all duplicates\ndf = df[~df.duplicated(subset=[\"id\"], keep=False)]\n\nwith tracker.start_run(\"ingest_deduped\", model=\"example\"):\n    tracker.log_dataframe(df, key=\"data\", schema=MySchema)\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#cant-query-across-runs","level":3,"title":"\"Can't query across runs\"","text":"<p>Symptom: <code>tracker.views.MySchema</code> doesn't exist or returns empty results.</p> <p>Root Cause: Schema not registered or data not ingested with schema.</p> <p>Solution:</p> <ol> <li> <p>Register schema on Tracker creation: <pre><code>tracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person, Trip],  # Register here\n)\n</code></pre></p> </li> <li> <p>Ingest with schema: <code>python with tracker.start_run(\"ingest_persons\", model=\"example\"):     tracker.log_dataframe(df, key=\"persons\", schema=Person)</code></p> </li> <li> <p>Verify schema exists: <pre><code>print(tracker.views.Person)  # Should not raise AttributeError\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#container-execution-issues","level":2,"title":"Container Execution Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#container-execution-failed","level":3,"title":"\"Container execution failed\"","text":"<p>Symptom: Error: <code>RuntimeError: Container execution failed</code></p> <p>Root Cause: Container exited with non-zero code.</p> <p>Solution:</p> <ol> <li> <p>Test container manually: <pre><code>docker run -it -v ./data:/inputs my-image python script.py\n</code></pre></p> </li> <li> <p>Check logs: <pre><code>docker logs &lt;container_id&gt;\n</code></pre></p> </li> <li> <p>Add verbose output: <pre><code>result = run_container(\n    ...\n    environment={\"DEBUG\": \"1\"},  # Enable debug output in container\n)\n</code></pre></p> </li> <li> <p>Verify input paths: <pre><code>from pathlib import Path\nfor input_path in inputs:\n    assert Path(input_path).exists(), f\"Missing: {input_path}\"\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#output-files-not-found-after-container","level":3,"title":"\"Output files not found after container\"","text":"<p>Symptom: Warning: <code>Expected output not found: ./outputs/result.csv</code></p> <p>Root Cause: Container didn't create output at expected location.</p> <p>Solution:</p> <ol> <li> <p>Verify container creates outputs: <pre><code>docker run -it -v ./outputs:/outputs my-image sh -c \"ls -la /outputs &amp;&amp; echo 'done'\"\n</code></pre></p> </li> <li> <p>Check output paths in container: <pre><code>run_container(\n    ...\n    command=[\"python\", \"script.py\"],  # Ensure script creates output\n)\n</code></pre></p> </li> <li> <p>Use correct host paths: <pre><code>output_dir = Path(\"./outputs\").mkdir(parents=True, exist_ok=True)\nresult = run_container(\n    ...\n    outputs=[str(output_dir / \"result.csv\")],\n)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#image-pull-failed","level":3,"title":"\"Image pull failed\"","text":"<p>Symptom: Error: <code>Error pulling image: authentication required</code></p> <p>Root Cause: Docker can't access the image registry.</p> <p>Solution:</p> <ol> <li> <p>Authenticate: <pre><code>docker login\n</code></pre></p> </li> <li> <p>Use public images: <pre><code>run_container(\n    image=\"ubuntu:latest\",  # Public image\n    ...\n)\n</code></pre></p> </li> <li> <p>Check image exists locally: <pre><code>docker images | grep my-image\n</code></pre></p> </li> <li> <p>Disable pull: <pre><code>run_container(\n    ...\n    pull_latest=False,  # Use local image if available\n)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#permission-denied-in-container","level":3,"title":"\"Permission denied in container\"","text":"<p>Symptom: <code>Permission denied</code> when container writes to mounted volume.</p> <p>Root Cause: Container user doesn't have write permission on host mount.</p> <p>Solution:</p> <ol> <li> <p>Make directory writable: <pre><code>chmod 777 ./outputs\n</code></pre></p> </li> <li> <p>Run container as current user: <pre><code># (Requires custom Dockerfile or user configuration)\n# docker run --user $(id -u):$(id -g) ...\n</code></pre></p> </li> <li> <p>Create output directory with correct permissions: <pre><code>output_dir = Path(\"./outputs\")\noutput_dir.mkdir(parents=True, exist_ok=True, mode=0o777)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#performance-issues","level":2,"title":"Performance Issues","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#runs-are-very-slow","level":3,"title":"\"Runs are very slow\"","text":"<p>Symptom: Each run takes much longer than expected.</p> <p>Root Cause: Several possibilities:</p> <ol> <li>No cache hits: Check if signature is changing unexpectedly.</li> <li>File I/O bottleneck: Large artifact materialization.</li> <li>Database queries slow: Too many cross-run queries.</li> <li>Container startup overhead: Each container run adds 1-2 seconds.</li> </ol> <p>Solution:</p> <ol> <li> <p>Profile execution: <pre><code>import time\nstart = time.time()\nresult = consist.run(...)\nprint(f\"Elapsed: {time.time() - start}s\")\nprint(f\"Cache hit: {result.cache_hit}\")\n</code></pre></p> </li> <li> <p>Avoid unnecessary materialization: <pre><code># Don't materialize if you don't need it\nresult = consist.run(..., cache_hydration=\"none\")\n</code></pre></p> </li> <li> <p>Use Parquet instead of CSV (faster parsing):    <pre><code>df.to_parquet(\"output.parquet\")  # Instead of .to_csv()\n</code></pre></p> </li> <li> <p>Batch containers to reduce startup overhead: <pre><code># DON'T:\nfor i in range(100):\n    run_container(...)  # 100 containers, 100 startups\n\n# DO:\nrun_container(\n    command=[\"python\", \"process_batch.py\", \"--n\", \"100\"],\n    ...\n)\n</code></pre></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#database-is-huge-and-slow","level":3,"title":"\"Database is huge and slow\"","text":"<p>Symptom: Queries are slow, database file is large.</p> <p>Root Cause: Too much data ingested or too many runs.</p> <p>Solution:</p> <ol> <li> <p>Vacuum database: <pre><code>with tracker.engine.begin() as conn:\n    conn.exec_driver_sql(\"VACUUM\")\n</code></pre></p> </li> <li> <p>Archive old runs: <pre><code># Move old database\nmv provenance.duckdb provenance.backup.duckdb\n# Start fresh\n</code></pre></p> </li> <li> <p>Use selective ingestion: <code>python    # Don't ingest everything, just what you need with tracker.start_run(\"sample_ingest\", model=\"example\"):     tracker.log_dataframe(df.head(1000), key=\"sample\")  # Sample instead of all</code></p> </li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#debugging-tools","level":2,"title":"Debugging Tools","text":"","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#enable-logging","level":3,"title":"Enable Logging","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(\"consist\")\nlogger.setLevel(logging.DEBUG)\n</code></pre> <p>This prints detailed provenance tracking, signature computation, and cache decisions.</p>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#inspect-run-metadata","level":3,"title":"Inspect Run Metadata","text":"<pre><code>run = tracker.get_run(\"run_id\")\nprint(f\"Signature: {run.signature}\")\nprint(f\"Code hash: {run.git_hash}\")\nprint(f\"Meta: {run.meta}\")\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#inspect-database","level":3,"title":"Inspect Database","text":"<pre><code>import duckdb\n\nconn = duckdb.connect(\"provenance.duckdb\")\nprint(conn.query(\"SELECT * FROM run LIMIT 5\").df())\nprint(conn.query(\"SELECT * FROM artifact LIMIT 5\").df())\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#check-file-hashes","level":3,"title":"Check File Hashes","text":"<pre><code>from pathlib import Path\n\nwith tracker.start_run(\"hash_input\", model=\"example\"):\n    artifact = tracker.log_artifact(Path(\"input.csv\"), key=\"input\", direction=\"input\")\n    print(f\"Path: {artifact.path}\")\n    print(f\"Hash: {artifact.hash}\")\n    print(f\"Size: {artifact.path.stat().st_size}\")\n</code></pre>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#getting-help","level":2,"title":"Getting Help","text":"<p>If you hit an issue not covered here:</p> <ol> <li> <p>Check the logs: <pre><code>logging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> <li> <p>Inspect database: <pre><code>duckdb provenance.duckdb \".schema\"\n</code></pre></p> </li> <li> <p>File an issue on GitHub with:</p> </li> <li>Error message and traceback</li> <li>Minimal reproducible example</li> <li>Output of <code>consist runs</code> (recent run history)</li> <li>Output of logging (with DEBUG enabled)</li> </ol>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"troubleshooting/#see-also","level":2,"title":"See Also","text":"<ul> <li>Container Integration</li> <li>DLT Loader</li> <li>Architecture (for implementation details)</li> <li>CLI Reference (for debugging commands)</li> </ul>","path":["Support & Reference","Troubleshooting Guide"],"tags":[]},{"location":"usage-guide/","level":1,"title":"Usage Guide","text":"<p>Consist provides flexible patterns for tracking provenance in scientific workflows. This guide walks you through the main usage patterns, from simple single-step runs to complex multi-year simulations. Each section is written to help: - Developers integrating Consist into a simulation tool - Practitioners running tools and wanting clearer inputs/outputs - Researchers managing multi-stage pipelines and reproducibility</p> <p>New to Consist? Start with the quickstart notebook, then work through the examples below in order.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#choosing-your-pattern","level":2,"title":"Choosing Your Pattern","text":"<p>Choose based on what you're building and how much structure you need:</p> Your Workflow Pattern Why Single data processing step (clean, transform, aggregate) <code>run</code> Simple: caches the entire function call with low overhead. Use for self-contained operations. Multi-step workflow (preprocessing → simulation → analysis) <code>scenario</code> Groups related steps, shares state via coupler, per-step caching. Use when steps have dependencies or shared configuration. Existing tool/model (subprocess, legacy code, container) <code>container</code> or <code>depends_on</code> Wraps external executables, tracks container digest as cache key. Use for black-box tools. Parameter sweep / sensitivity analysis <code>scenario</code> + loop Run the same step with different configs, compare results. Multi-year simulation <code>scenario</code> + loop Runs in years, each year caches independently, all years share scenario context.","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#pattern-1-single-step-runs-run","level":2,"title":"Pattern 1: Single-Step Runs (<code>run()</code>)","text":"<p>Use <code>run()</code> when you have a self-contained operation: data cleaning, transformation, aggregation, or any callable that takes inputs and produces outputs. Consist caches the entire function call based on code version + config + input data.</p> <p>When to use: - Simple data transformations (filter, aggregate, merge) - Expensive computations that don't depend on other runs - One-off analyses - You want the simplest mental model</p> <p>When NOT to use: - Multi-step workflows with dependencies between steps (use <code>scenario()</code> instead) - Existing tools that write to directories (use <code>depends_on</code> or <code>container</code>)</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#simple-example-data-cleaning","level":3,"title":"Simple Example: Data Cleaning","text":"<p>Here's the basic pattern:</p> <pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom pathlib import Path\nimport pandas as pd\n\n# 1. Create tracker\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\n# 2. Define your function\ndef clean_data(raw: pd.DataFrame, threshold: float = 0.5) -&gt; pd.DataFrame:\n    df = raw[raw[\"value\"] &gt; threshold]  # Filter\n    return df\n\n# 3. Run it with tracker context\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean_data,\n        inputs={\"raw\": Path(\"raw.csv\")},\n        config={\"threshold\": 0.5},  # Part of cache key\n        outputs=[\"cleaned\"],\n    )\n\n# 4. Access results\ncleaned_artifact = result.outputs[\"cleaned\"]\ncleaned_df = consist.load_df(cleaned_artifact)\nprint(f\"Output: {cleaned_artifact.path}\")\n</code></pre> <p>If you run it again with the same inputs and config, you should get a cache hit and no re-execution.</p> Alternative: keep raw file paths (no auto-load)  Use this when your function needs a `Path` and manages I/O directly (common for legacy tools or file-based APIs).  <pre><code>def clean_data(raw_file: Path, _consist_ctx) -&gt; None:\n    df = pd.read_csv(raw_file)\n    out_path = _consist_ctx.run_dir / \"cleaned.parquet\"\n    df.to_parquet(out_path)\n    _consist_ctx.log_output(out_path, key=\"cleaned\")\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean_data,\n        inputs={\"raw_file\": Path(\"raw.csv\")},  # hashed input\n        load_inputs=False,\n        runtime_kwargs={\"raw_file\": Path(\"raw.csv\")},\n        inject_context=True,\n    )\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#example-with-config","level":3,"title":"Example with Config","text":"<p>Use Pydantic models for structured configs:</p> <pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom pydantic import BaseModel\nfrom pathlib import Path\nimport pandas as pd\n\nclass CleaningConfig(BaseModel):\n    threshold: float = 0.5\n    remove_outliers: bool = True\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\ndef clean_data(raw: pd.DataFrame, config: CleaningConfig) -&gt; pd.DataFrame:\n    df = raw\n    df = df[df[\"value\"] &gt;= config.threshold]\n    if config.remove_outliers:\n        df = df[df[\"value\"] &lt; df[\"value\"].quantile(0.95)]\n    return df\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=clean_data,\n        inputs={\"raw\": Path(\"raw.csv\")},\n        config=CleaningConfig(threshold=0.5, remove_outliers=True),\n        outputs=[\"cleaned\"],\n    )\n</code></pre> <p>Each distinct config → separate cache entries. Change the threshold? Only that run re-executes.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#wrapping-legacy-or-black-box-tools","level":3,"title":"Wrapping Legacy or Black-Box Tools","text":"<p>If you have existing code that writes files to a directory, use the injected run context to capture outputs:</p> <pre><code>from pathlib import Path\n\ndef run_legacy_model(upstream, _consist_ctx) -&gt; None:\n    import legacy_model\n\n    output_dir = _consist_ctx.run_dir / \"legacy_outputs\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    with _consist_ctx.capture_outputs(output_dir, pattern=\"*.csv\"):\n        legacy_model.run(upstream, output_dir=output_dir)\n\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=run_legacy_model,\n        inputs={\"upstream\": Path(\"input.csv\")},\n        depends_on=[Path(\"config.yaml\"), Path(\"parameters.json\")],  # Hash these files too\n        load_inputs=False,\n        runtime_kwargs={\"upstream\": Path(\"input.csv\")},  # Pass raw paths at runtime\n        inject_context=True,\n    )\n</code></pre> <p>Captured outputs are keyed by filename stem (for example, <code>results.csv</code> -&gt; <code>results</code>).</p> Alternative: capture a fixed output directory  If the tool always writes to a known folder and returns `None`, you can capture it directly:  <pre><code>with use_tracker(tracker):\n    def run_legacy_model(upstream) -&gt; None:\n        import legacy_model\n\n        legacy_model.run(upstream, output_dir=\"outputs\")\n\n    result = consist.run(\n        fn=run_legacy_model,\n        inputs={\"upstream\": Path(\"input.csv\")},  # hashed input\n        depends_on=[Path(\"config.yaml\")],\n        load_inputs=False,\n        runtime_kwargs={\"upstream\": Path(\"input.csv\")},\n        capture_dir=Path(\"outputs\"),\n        capture_pattern=\"*.csv\",\n    )\n</code></pre> How input mappings become function arguments  When `inputs` is a mapping, Consist matches keys to function parameters and auto-loads those artifacts (for example, a CSV becomes a DataFrame) into the call by default. If you keep `load_inputs=True`, `inputs={\"upstream\": ...}` becomes the `upstream` argument automatically.  If you want raw paths instead of auto-loaded data, set `load_inputs=False` and pass paths explicitly via `runtime_kwargs`.  <p>The <code>depends_on</code> files are hashed as part of the cache key, so changing config.yaml invalidates the cache.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#pattern-2-multi-step-workflows-scenario","level":2,"title":"Pattern 2: Multi-Step Workflows (<code>scenario()</code>)","text":"<p>Use <code>scenario()</code> when you have multiple interdependent steps that share state or configuration. Scenarios group steps into a coherent unit (a \"run scenario\"), while each step caches independently.</p> <p>When to use: - Multi-step pipelines (preprocess → simulate → analyze) - Steps have data dependencies (output of step 1 is input to step 2) - Multi-year simulations (year 2020 → 2030 → 2040) - Parameter sweeps where you want to compare across variants - Shared configuration across multiple steps</p> <p>Benefits over <code>run()</code>: - Steps cache independently—skip re-executing steps whose inputs haven't changed - Use the coupler to pass data between steps with automatic provenance tracking - Group runs into scenarios for easy cross-scenario queries</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#understanding-the-coupler","level":3,"title":"Understanding the Coupler","text":"<p>The coupler is your scenario-scoped artifact registry. When you log an artifact with a key, it's automatically stored in the coupler, making data flow between steps explicit and traceable. This is especially useful when: - You need clean handoffs between tools (developer workflows) - You want a clear list of outputs by name (practitioner workflows) - You want auditable step-to-step lineage (research workflows)</p> <p>Etymology: A coupler (like the library name \"consist\" from railroad terminology) is the mechanism that links train cars together. In Consist, the coupler links your workflow steps by storing and threading their outputs.</p> <p>Key behaviors: - When you log an artifact with a <code>key</code>, it's automatically synced to the coupler - You retrieve artifacts with <code>coupler.require(key)</code> or via <code>inputs=</code> declarations - The coupler persists across all steps in a scenario - Each scenario has its own coupler; they don't share data - On cache hits, cached outputs are pre-synced to the coupler before your step runs</p> <p>You interact with the coupler when: - Accessing inputs in trace blocks: <code>coupler.require(\"population\")</code> - Declaring inputs to <code>sc.run()</code>: <code>inputs=[\"population\"]</code> - Validating that outputs were produced: <code>sc.require_outputs(...)</code></p> <p>Live-sync (automatic): When you log an artifact, it's immediately available in the coupler—you don't need to manually call <code>coupler.set()</code>.</p> <p>For optional-Consist workflows: If you're using Consist in optional mode (with fallback to Path objects or artifact-like objects), use <code>coupler.set_from_artifact(key, value)</code> instead of <code>coupler.set()</code>. It handles both real Artifacts and artifact-like objects (Paths, strings, noop artifacts) transparently.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#simple-example-two-step-workflow","level":3,"title":"Simple Example: Two-Step Workflow","text":"<pre><code>import consist\nfrom consist import Tracker, use_tracker\nfrom pathlib import Path\nimport pandas as pd\n\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\n# Define steps as regular functions\ndef preprocess_data(raw: pd.DataFrame) -&gt; pd.DataFrame:\n    df = raw[raw[\"value\"] &gt; 0.5]\n    return df\n\ndef analyze_data(preprocessed: pd.DataFrame) -&gt; pd.DataFrame:\n    summary = preprocessed.groupby(\"category\", as_index=False)[\"value\"].mean()\n    return summary\n\n# Execute as a scenario\nwith use_tracker(tracker):\n    with consist.scenario(\"my_analysis\") as sc:\n        sc.run(\n            name=\"preprocess\",\n            fn=preprocess_data,\n            inputs={\"raw\": Path(\"raw.csv\")},\n            outputs=[\"preprocessed\"],\n        )\n\n        sc.run(\n            name=\"analyze\",\n            fn=analyze_data,\n            inputs=[\"preprocessed\"],\n            load_inputs=True,\n            outputs=[\"analysis\"],\n        )\n</code></pre> <p>All steps have <code>scenario_id=\"my_analysis\"</code>, making it easy to query together. Change preprocess logic? The preprocess step re-runs; the analyze step re-runs only if the preprocessed artifact changes. Change raw.csv? Both re-execute.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#declaring-inputs-mapping-form-vs-list-form","level":3,"title":"Declaring Inputs: Mapping Form vs List Form","text":"<p>The <code>inputs=</code> parameter supports two different forms for different situations:</p> <p>Mapping form (explicit paths from disk): <pre><code>sc.run(\n    name=\"preprocess\",\n    fn=preprocess_data,\n    inputs={\"raw\": Path(\"raw.csv\")},  # Load from disk\n    outputs=[\"preprocessed\"],\n)\n</code></pre> Use this when you're loading data from disk for the first time.</p> <p>List form (resolve from coupler): <pre><code>sc.run(\n    name=\"analyze\",\n    fn=analyze_data,\n    inputs=[\"preprocessed\"],          # Resolve from coupler\n    load_inputs=True,                 # Auto-load as function parameters\n    outputs=[\"analysis\"],\n)\n</code></pre> Use this when the artifact is already in the coupler from a prior step. With <code>load_inputs=True</code>, each key becomes a function parameter with the loaded data. For example, <code>inputs=[\"preprocessed\"]</code> means your function receives a <code>preprocessed</code> parameter with the loaded DataFrame.</p> <p>Note: If you have existing code using <code>input_keys=</code>, it continues to work for backward compatibility. <code>inputs=</code> is the preferred name for new code.</p> Alternative: inline steps with sc.trace  Use this when you want inline code blocks (they always execute, even on cache hits). This can be useful for lightweight validation or logging.  <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"my_analysis\") as sc:\n        with sc.trace(name=\"preprocess\", inputs={\"raw\": Path(\"raw.csv\")}):\n            df = pd.read_csv(\"raw.csv\")\n            consist.log_dataframe(df, key=\"preprocessed\")\n\n        with sc.trace(name=\"analyze\", inputs=[\"preprocessed\"]):\n            df = consist.load_df(sc.coupler.require(\"preprocessed\"))\n            summary = df.groupby(\"category\", as_index=False)[\"value\"].mean()\n            consist.log_dataframe(summary, key=\"analysis\")\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#passing-data-between-steps-with-the-coupler","level":3,"title":"Passing Data Between Steps with the Coupler","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\", model=\"travel_demand\") as sc:\n        coupler = sc.coupler\n\n        # Step 1: Load and prepare\n        with sc.trace(name=\"initialize\", run_id=\"baseline_init\"):\n            df_pop = load_population()\n            consist.log_dataframe(df_pop, key=\"population\")\n\n        # Step 2: Simulate for each year\n        for year in [2020, 2030, 2040]:\n            with sc.trace(\n                name=\"simulate\",\n                run_id=f\"baseline_{year}\",\n                year=year,\n                inputs=[\"population\"],  # Declare dependency\n            ):\n                # Get data from coupler (with automatic cache detection)\n                df_pop = consist.load_df(coupler.require(\"population\"))\n                df_result = run_model(year, df_pop)\n\n                # Log output and store for downstream steps\n                consist.log_dataframe(df_result, key=\"persons\")\n</code></pre> <p>What <code>inputs</code> (list form) does: - Declares that this step depends on \"population\" artifact - Consist tracks this as part of the cache key - If the population artifact hasn't changed, this step's cache is still valid</p> <p>Simpler alternative (auto-load inputs with <code>sc.run</code>)</p> <p>If your step can be expressed as a function, <code>sc.run</code> can auto-load inputs into function parameters so you don't need to call <code>coupler.require(...)</code> manually:</p> <pre><code>def simulate_year(population: pd.DataFrame, config: dict) -&gt; pd.DataFrame:\n    year = config[\"year\"]\n    return run_model(year, population)\n\nwith use_tracker(tracker):\n    with consist.scenario(\"baseline\", model=\"travel_demand\") as sc:\n        with sc.trace(name=\"initialize\", run_id=\"baseline_init\"):\n            df_pop = load_population()\n            consist.log_dataframe(df_pop, key=\"population\")\n\n        for year in [2020, 2030, 2040]:\n            sc.run(\n                name=f\"simulate_{year}\",\n                fn=simulate_year,\n                inputs=[\"population\"],\n                outputs=[\"persons\"],\n                load_inputs=True,\n                config={\"year\": year},\n            )\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#output-validation","level":3,"title":"Output Validation","text":"<p>Consist can validate that your workflow produces expected outputs, catching typos or missing data early. You have three patterns to choose from based on your workflow. Most users just need Pattern A.</p> <p>Pattern A: Declare required outputs (Static workflows) — START HERE</p> <p>Use this when you know all your workflow outputs upfront.</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"workflow\") as sc:\n        sc.require_outputs(\n            \"zarr_skims\",\n            \"synthetic_population\",\n        )\n\n        # Run steps—at scenario exit, missing outputs raise RuntimeError\n        compile_result = sc.run(\"compile\", fn=asim_compile_runner.run)\n</code></pre> <p>When to use: Static workflows where steps always produce the same outputs (most common).</p> <p>Benefits: Simple, clear contract. Missing outputs are caught at scenario exit. Typos are caught immediately.</p> <p>Optional: Add guardrails for typos <pre><code>sc.require_outputs(\n    \"zarr_skims\",\n    \"synthetic_population\",\n    warn_undocumented=True,  # Warn if you set other keys by mistake\n    description={\n        \"zarr_skims\": \"Zone-to-zone travel times in Zarr format\",\n        \"synthetic_population\": \"Synthetic population with activity schedules\",\n    }\n)\n</code></pre></p> <p>Shortcut: Pass required outputs directly to <code>scenario()</code>: <pre><code>with consist.scenario(\n    \"workflow\",\n    require_outputs=[\"zarr_skims\", \"synthetic_population\"],\n) as sc:\n    ...\n</code></pre></p> <p>Pattern B: Runtime-declared validation (Dynamic/optional outputs)</p> <p>Use this when outputs are dynamic or optional (e.g., optional debug outputs, or conditional branching).</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"workflow\") as sc:\n        # Declare outputs as you build them\n        sc.declare_outputs(\n            \"zarr_skims\", \"synthetic_population\",\n            required={\"zarr_skims\": True, \"synthetic_population\": True}\n        )\n\n        # Add more outputs later if needed\n        if debugging:\n            sc.declare_outputs(\"debug_report\", required={\"debug_report\": False})\n\n        # Run steps—missing required outputs raise RuntimeError at exit\n        compile_result = sc.run(\"compile\", fn=asim_compile_runner.run)\n</code></pre> <p>When to use: Workflows with per-key control over required vs optional, or conditional outputs.</p> <p>Benefits: Granular per-key control. Mix required and optional outputs. Add outputs dynamically.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#selective-output-collection-with-collect_by_keys","level":3,"title":"Selective Output Collection with <code>collect_by_keys()</code>","text":"<p>By default, when a step produces multiple outputs, all are automatically synced to the coupler. Use <code>collect_by_keys()</code> when you need to: - Select only specific outputs from many (ignore others) - Namespace outputs by year or scenario (prefix them)</p> <pre><code># Select specific outputs\nresult = sc.run(\"step\", fn=some_func)  # Produces: persons, households, jobs\nsc.collect_by_keys(result.outputs, \"persons\", \"households\")\n# coupler now has: \"persons\", \"households\" (jobs ignored)\n\n# Namespace by year (useful in multi-year simulations)\nfor year in [2020, 2030, 2040]:\n    result = sc.run(f\"forecast_{year}\", fn=forecast_fn)\n    sc.collect_by_keys(result.outputs, \"population\", \"skims\", prefix=f\"{year}_\")\n\n# coupler now has: \"2020_population\", \"2020_skims\", \"2030_population\", etc.\n</code></pre> <p>Bulk logging with metadata: <pre><code>with consist.scenario(\"outputs\") as sc:\n    with sc.trace(name=\"export_outputs\"):\n        # Log multiple files at once with explicit keys\n        outputs = consist.log_artifacts(\n            {\n                \"persons\": \"results/persons.parquet\",\n                \"households\": \"results/households.parquet\",\n                \"jobs\": \"results/jobs.parquet\"\n            },\n            metadata_by_key={\n                \"households\": {\"role\": \"primary_unit\"},\n                \"jobs\": {\"role\": \"employment_proxy\"}\n            },\n            year=2030,\n            scenario_name=\"base\"\n        )\n        # All get year=2030, scenario_name=\"base\"\n        # households also gets role=\"primary_unit\"\n</code></pre></p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#example-parameter-sweep-in-a-scenario","level":3,"title":"Example: Parameter Sweep in a Scenario","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"sensitivity_analysis\") as sc:\n        coupler = sc.coupler\n\n        # Load once, use for all variants\n        with sc.trace(name=\"setup\"):\n            df = pd.read_csv(\"data.csv\")\n            consist.log_dataframe(df, key=\"data\")\n\n        # Test different thresholds\n        for threshold in [0.3, 0.5, 0.7]:\n            with sc.trace(\n                name=\"analyze\",\n                run_id=f\"threshold_{threshold}\",\n                threshold=threshold,\n                inputs=[\"data\"],\n            ):\n                df = consist.load_df(coupler.require(\"data\"))\n                filtered = df[df[\"value\"] &gt; threshold]\n                consist.log_dataframe(filtered, key=\"filtered\")\n</code></pre> <p>Each threshold creates a separate run with its own cache entry. Re-run later? Consist returns cached results for matching thresholds, skips setup (since input unchanged).</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#pattern-3-container-integration","level":2,"title":"Pattern 3: Container Integration","text":"<p>Use containers when you have existing tools, models, or legacy code that runs as a subprocess or Docker container. The image digest becomes part of the cache key.</p> <p>When to use: - Running ActivitySim, SUMO, BEAM, or other external models - Legacy code you don't want to refactor - Python/R/Java executables that you invoke as subprocesses - Tools that expect specific file paths or output directories</p> <pre><code>from consist.integrations.containers import run_container\nfrom pathlib import Path\n\nhost_inputs = Path(\"data/inputs\").resolve()\nhost_outputs = Path(\"data/outputs\").resolve()\n\nresult = run_container(\n    tracker=tracker,\n    run_id=\"model_2030\",\n    image=\"travel-model:v2.1\",  # Image digest becomes cache key\n    command=[\"python\", \"run.py\", \"--year\", \"2030\"],\n    volumes={\n        str(host_inputs): \"/inputs\",\n        str(host_outputs): \"/outputs\",\n    },\n    inputs=[host_inputs / \"input.csv\"],\n    outputs={\"results\": host_outputs / \"results.parquet\"},\n)\n\noutput_artifact = result.artifacts[\"results\"]\n</code></pre> <p>Change the image version? Cache invalidates. Same image + inputs? Returns cached results.</p> Alternative: use consist.run with executor=\"container\"  If you prefer the same API as `consist.run`, you can use the container executor:  <pre><code>import consist\n\nwith consist.use_tracker(tracker):\n    result = consist.run(\n        name=\"model_2030\",\n        executor=\"container\",\n        inputs=[host_inputs / \"input.csv\"],\n        output_paths={\"results\": host_outputs / \"results.parquet\"},\n        container={\n            \"image\": \"travel-model:v2.1\",\n            \"command\": [\"python\", \"run.py\", \"--year\", \"2030\"],\n            \"volumes\": {str(host_inputs): \"/inputs\", str(host_outputs): \"/outputs\"},\n        },\n    )\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#advanced-patterns","level":2,"title":"Advanced Patterns","text":"","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#cache-hits-and-the-coupler","level":3,"title":"Cache Hits and the Coupler","text":"<p>When Consist detects a cache hit (same step, same code version, same config and inputs):</p> <ol> <li>In <code>sc.run()</code>: Your function is skipped entirely. Cached outputs are returned and automatically synced to the coupler.</li> <li>In <code>sc.trace()</code>: Cached outputs are pre-synced to the coupler BEFORE your trace body runs, so you can access them with <code>coupler.require()</code> immediately. Your code still executes (unlike <code>sc.run()</code>).</li> </ol> <p>This means your code doesn't need to handle cache hits differently—the coupler is populated automatically in both cases.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#cache-hydration","level":3,"title":"Cache Hydration","text":"<p>By default, Consist returns metadata-only cache hits (no file copies). You can opt in to materializing cached files when needed:</p> <ul> <li><code>outputs-requested</code>: Copy only specific cached outputs to paths you provide</li> <li><code>outputs-all</code>: Copy all cached outputs into a target directory</li> <li><code>inputs-missing</code>: When a cache miss occurs, backfill missing inputs from prior runs before executing</li> </ul> <p>Note: <code>outputs-requested</code> requires <code>output_paths=...</code> so Consist knows where to write the files. <code>inputs-missing</code> only works for inputs that are tracked artifacts (not raw paths), so Consist can find the prior run's files or reconstruct ingested tables.</p> <p>Set per-run via <code>cache_hydration=...</code> or for scenario steps via <code>step_cache_hydration=...</code>:</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\", step_cache_hydration=\"inputs-missing\") as sc:\n        coupler = sc.coupler\n        # This step will backfill inputs if they're missing from disk\n        with sc.trace(name=\"simulate\", inputs=[\"population\"]):\n            df_pop = consist.load_df(coupler.require(\"population\"))\n            # ... rest of simulation ...\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#mixing-runs-and-scenarios","level":3,"title":"Mixing Runs and Scenarios","text":"<p>Call <code>consist.run(...)</code> inside a scenario when a step should cache independently:</p> <pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        coupler = sc.coupler\n        # Run expensive preprocessing independently\n        preprocess = consist.run(\n            fn=expensive_preprocessing,\n            inputs={\"network_file\": Path(\"network.geojson\")},\n            outputs=[\"processed\"],\n        )\n        # Add outputs to coupler for downstream steps\n        coupler.update(preprocess.outputs)\n\n        # Later steps can use the preprocessed output\n        with sc.trace(name=\"simulate\", inputs=[\"processed\"]):\n            network = consist.load_df(coupler.require(\"processed\"))\n            # ... simulation ...\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#when-does-code-execute-understanding-scrun-vs-sctrace","level":2,"title":"When Does Code Execute? Understanding <code>sc.run()</code> vs <code>sc.trace()</code>","text":"<p>When building multi-step scientific workflows, a critical question arises: Does my Python code run every time, or only when inputs change? This section clarifies the difference between <code>sc.run()</code> and <code>sc.trace()</code>, two fundamental Consist patterns that differ in execution behavior on cache hits.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#the-core-distinction","level":3,"title":"The Core Distinction","text":"<p>On a cache hit (when Consist finds previously-cached results for this step with the same inputs):</p> <ul> <li> <p><code>sc.trace(...)</code> — Your Python block always executes. Consist returns cached outputs, but your code still runs. Use this for logging, diagnostics, or steps that must track intermediate state every run.</p> </li> <li> <p><code>sc.run(...)</code> — Your Python function only executes on cache miss. On a cache hit, Consist skips calling your function entirely and returns the cached output. Use this for expensive operations like scientific simulations, data processing, or model fitting.</p> </li> </ul>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#why-this-matters-performance-side-effects","level":3,"title":"Why This Matters: Performance &amp; Side Effects","text":"<p>Consider an expensive simulation that takes 2 hours. Running it 100 times with the same inputs would normally take 200 hours of compute. With Consist:</p> <ul> <li>If you use <code>sc.trace()</code>: code runs 100 times (200 hours) — caching provides metadata only</li> <li>If you use <code>sc.run()</code>: code runs once (2 hours), then 99 cache hits retrieve results instantly</li> </ul> <p>Side effects also differ. If your step writes temporary files, updates external systems, or has other side effects, <code>sc.trace()</code> repeats them on every run, while <code>sc.run()</code> skips them on cache hits.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#example-activitysim-style-land-use-simulation","level":3,"title":"Example: ActivitySim-Style Land Use Simulation","text":"<p>Here's a realistic example showing the difference:</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#using-sctrace-always-runs","level":4,"title":"Using <code>sc.trace()</code> (Always Runs)","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        year = 2030\n        # This block executes every time, even on cache hits\n        with sc.trace(\n            name=\"prepare_land_use\",\n            inputs={\"geojson\": Path(\"land_use.geojson\")},\n            year=year\n        ):\n            # This code ALWAYS runs—useful if you log, print status, etc.\n            print(f\"Processing land use for year {year}\")\n            zones = load_zones(\"land_use.geojson\")\n\n            # But Consist returns cached output if it exists\n            df_zones = pd.DataFrame(zones)\n            consist.log_dataframe(df_zones, key=\"zones\")\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#using-scrun-skips-on-cache-hit","level":4,"title":"Using <code>sc.run()</code> (Skips on Cache Hit)","text":"<pre><code>with use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        def prepare_land_use(geojson_path: Path) -&gt; pd.DataFrame:\n            # This function ONLY runs on cache miss\n            # On cache hit, Consist returns cached output without calling it\n            print(f\"Processing land use\")  # Only prints on first run\n            zones = load_zones(geojson_path)\n            return pd.DataFrame(zones)\n\n        result = sc.run(\n            name=\"prepare_land_use\",\n            fn=prepare_land_use,\n            inputs={\"geojson_path\": Path(\"land_use.geojson\")},\n            outputs=[\"zones\"],\n            load_inputs=True,  # Auto-load Path → argument\n        )\n        # Outputs are synced to the coupler automatically\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#which-should-you-use","level":3,"title":"Which Should You Use?","text":"<p>Choose based on your workflow needs:</p> Scenario Use Why Expensive simulation, model fitting, or large data transformation <code>sc.run()</code> Skip re-execution on cache hits; critical for 2+ hour runtimes or iterative analysis Steps that log, print diagnostics, or validate state on every run <code>sc.trace()</code> Need to see side effects repeated; cheaper operations that re-run quickly Multi-year simulation where early years are cached, new years execute <code>sc.run()</code> Each year has independent cache entry; skip re-running 2020 when computing 2030 Mixed: some expensive, some diagnostic Both in same scenario Use <code>sc.run()</code> for expensive steps, <code>sc.trace()</code> for cheap validation","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#practical-guidance","level":3,"title":"Practical Guidance","text":"<p>For most scientific workflows, prefer <code>sc.run()</code> when: - Your function is deterministic (no randomness unless seeded in config) - It doesn't have important side effects outside the outputs you log - You can structure it as a pure function (inputs → outputs)</p> <p>Use <code>sc.trace()</code> when: - You need to run initialization or setup code that triggers external systems - You want explicit control over what happens every run vs. only on cache miss - Your step is fast enough that re-execution overhead doesn't matter</p> <p>On large file inputs: If your function receives multi-GB files, set <code>load_inputs=False</code> and use <code>cache_hydration=\"inputs-missing\"</code> to ensure input files are available on cache misses without re-loading on every run.</p> Alternative: log file outputs inside the step  If your function writes files, log them with the injected context (and read inputs yourself if needed):  <pre><code>def beam_preprocess(data_file, _consist_ctx) -&gt; None:\n    out_path = _consist_ctx.run_dir / \"beam_inputs.parquet\"\n    # ... write out_path ...\n    _consist_ctx.log_output(out_path, key=\"beam_inputs\")\n\nwith use_tracker(tracker):\n    with consist.scenario(\"baseline\") as sc:\n        sc.run(\n            name=\"beam_preprocess\",\n            fn=beam_preprocess,\n            inputs={\"data_file\": Path(\"data.parquet\")},\n            load_inputs=False,\n            runtime_kwargs={\"data_file\": Path(\"data.parquet\")},\n            inject_context=True,\n        )\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#query-facets-with-pivot_facets","level":3,"title":"Query Facets with <code>pivot_facets</code>","text":"<p>Log small, queryable config values (facets) and pivot them into a wide table for analysis. This is a simple way to compare many runs side‑by‑side (for example, a sensitivity analysis across parameters):</p> <pre><code>from sqlmodel import select\nimport consist\n\n# Pivot config facets into columns\nparams = consist.pivot_facets(\n    namespace=\"simulate\",\n    keys=[\"alpha\", \"beta\", \"mode\"],\n    value_columns={\"mode\": \"value_str\"},\n)\n\nrows = consist.run_query(\n    select(params.c.run_id, params.c.alpha, params.c.beta, params.c.mode),\n    tracker=tracker,\n)\n\n# Result: list of rows with columns [run_id, alpha, beta, mode] for comparison\n</code></pre> <p>See Concepts for when to use <code>config</code> vs <code>facet</code>.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#querying-results","level":2,"title":"Querying Results","text":"","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#finding-runs","level":3,"title":"Finding Runs","text":"<p>See: Example notebooks.</p> <pre><code>import consist\n\n# Find a specific run\nrun = consist.find_run(\n    tracker=tracker,\n    parent_id=\"baseline\",  # Scenario ID\n    year=2030,\n    model=\"simulate\"\n)\n\n# Get multiple runs indexed by a field\nruns_by_year = consist.find_runs(\n    tracker=tracker,\n    parent_id=\"baseline\",\n    model=\"simulate\",\n    index_by=\"year\"\n)\nresult_2030 = runs_by_year[2030]\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#loading-artifacts","level":3,"title":"Loading Artifacts","text":"<p>See: Example notebooks.</p> <pre><code># Get artifacts for a run\nartifacts = tracker.get_artifacts_for_run(run.id)\npersons_artifact = artifacts.outputs[\"persons\"]\n\n# Load the data\ndf = consist.load_df(persons_artifact)\n</code></pre>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#cross-run-queries-with-views","level":3,"title":"Cross-Run Queries with Views","text":"<p>See: Example notebooks.</p> <p>Register schemas to enable SQL queries across all runs:</p> <pre><code>import consist\nfrom sqlmodel import SQLModel, Field, select, func\nfrom consist import Tracker\n\nclass Person(SQLModel, table=True):\n    person_id: int = Field(primary_key=True)\n    age: int\n    number_of_trips: int\n\ntracker = Tracker(\n    run_dir=\"./runs\",\n    db_path=\"./provenance.duckdb\",\n    schemas=[Person]\n)\n\n# After running scenarios...\nVPerson = tracker.views.Person\n\nquery = (\n    select(\n        VPerson.consist_scenario_id,\n        VPerson.consist_year,\n        func.avg(VPerson.number_of_trips).label(\"avg_trips\")\n    )\n    .where(VPerson.consist_scenario_id.in_([\"baseline\", \"high_gas\"]))\n    .group_by(VPerson.consist_scenario_id, VPerson.consist_year)\n)\n\nresults = consist.run_query(query, tracker=tracker)\n</code></pre> <p>Views automatically include <code>consist_scenario_id</code>, <code>consist_year</code>, and other metadata columns for filtering and grouping. For more on ingestion and hybrid views, see Ingestion &amp; Hybrid Views.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"usage-guide/#generating-schemas-from-captured-data","level":3,"title":"Generating Schemas from Captured Data","text":"<p>If you ingest tabular data into DuckDB, Consist can capture the observed schema and export an editable SQLModel stub so you can curate PK/FK constraints and then register the model for views. This is useful when you want a stable, documented schema for downstream analysis or audits.</p> <p>You can also opt into lightweight file schema capture when logging CSV/Parquet artifacts by passing <code>profile_file_schema=True</code> (and optionally <code>file_schema_sample_rows=</code>) to <code>log_artifact</code>. These captured schemas are stored in the provenance DB and remain available even if the original files move or are deleted. If you already have a content hash (e.g., after copying or moving a file), pass <code>content_hash=</code> to <code>log_artifact</code> to reuse it without re-hashing the file. For safety, Consist will not overwrite an existing, different hash unless you pass <code>force_hash_override=True</code>. To verify the hash against disk, use <code>validate_content_hash=True</code>.</p> <p>See <code>docs/schema-export.md</code> for the full workflow (CLI + Python) and column-name/<code>__tablename__</code> guidelines. See Ingestion &amp; Hybrid Views for ingestion tradeoffs and DB fallback behavior.</p>","path":["Getting Started","Usage Guide"],"tags":[]},{"location":"api/","level":1,"title":"API reference","text":"<p>This section documents Consist's public API and key helpers. Start with the public surface area, then drill into the core classes and supporting utilities.</p>","path":["API Reference","API reference"],"tags":[]},{"location":"api/#start-here","level":2,"title":"Start here","text":"<ul> <li>Public API</li> <li>Helpers</li> </ul>","path":["API Reference","API reference"],"tags":[]},{"location":"api/#core-classes","level":2,"title":"Core classes","text":"<ul> <li>Tracker</li> <li>Artifact</li> <li>Run</li> <li>Indexing</li> <li>Identity Manager</li> <li>Workflow Contexts</li> <li>Views</li> <li>Matrix Views</li> <li>Materialization</li> </ul>","path":["API Reference","API reference"],"tags":[]},{"location":"api/api_helpers/","level":1,"title":"API Helpers","text":"","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.RelationConnectionLeakWarning","level":2,"title":"<code>RelationConnectionLeakWarning</code>","text":"<p>               Bases: <code>RuntimeWarning</code></p> <p>Warning emitted when relation connections appear to accumulate.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.view","level":2,"title":"<code>view(model, name=None)</code>","text":"<p>Create a SQLModel class backed by a Consist hybrid view.</p> <p>This is a convenience wrapper around the view factory that lets you define a SQLModel schema for a concept (e.g., a canonicalized config table, an ingested dataset, or a computed artifact view) and then query it as a normal SQLModel table. The returned class is a dynamic subclass with <code>table=True</code> that points at a database view, so you can use it in <code>sqlmodel.Session</code> queries without creating a physical table.</p> <p>If you need explicit control over view naming or want to create multiple named views for the same concept, use <code>Tracker.create_view(...)</code> or <code>Tracker.view(...)</code> directly.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[T]</code> <p>Base SQLModel describing the schema (columns and types).</p> required <code>name</code> <code>Optional[str]</code> <p>Optional override for the generated view name (defaults to the model's table name).</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[T]</code> <p>SQLModel subclass with <code>table=True</code> pointing at the hybrid view.</p> <p>Examples:</p> <pre><code>from sqlmodel import Session, select\nfrom consist import view\nfrom consist.models.activitysim import ActivitySimConstantsCache\n\n# Create a dynamic view class for querying constants\nConstantsView = view(ActivitySimConstantsCache)\n\nwith Session(tracker.engine) as session:\n    rows = session.exec(\n        select(ConstantsView)\n        .where(ConstantsView.key == \"sample_rate\")\n    ).all()\n</code></pre>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.use_tracker","level":2,"title":"<code>use_tracker(tracker)</code>","text":"<p>Set a fallback (default) tracker for Consist API entrypoints.</p> <p>This configures which tracker is used by consist.run(), consist.start_run(), etc. when called outside an active run context (i.e., when the tracker stack is empty). Once inside a run, the tracker becomes \"active\" via push_tracker() and is accessed by logging functions like consist.log_artifact().</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.run","level":2,"title":"<code>run(fn=None, name=None, *, tracker=None, **kwargs)</code>","text":"<p>Execute a function as a tracked Consist run.</p> <p>This is a high-level entrypoint that wraps a function call in a Consist run. It automatically handles run start/end and result capturing.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Optional[Callable]</code> <p>The function to execute.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>A semantic name for the run. Defaults to the function name.</p> <code>None</code> <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to use.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>Tracker.run</code>, such as <code>inputs</code>, <code>tags</code>, or <code>runtime_kwargs</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>RunResult</code> <p>An object containing the function's return value and the recorded <code>Run</code> record.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.trace","level":2,"title":"<code>trace(name, *, tracker=None, **kwargs)</code>","text":"<p>Create a nested tracing context within an active run.</p> <p>Use <code>trace</code> to break down a large run into smaller, logical steps. Each traced step is recorded as a sub-run with its own inputs and outputs.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Semantic name for this step/trace.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to use.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata or parameters for this trace.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The active tracker instance.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.start_run","level":2,"title":"<code>start_run(run_id, model, tracker=None, **kwargs)</code>","text":"<p>Initiate and manage a Consist run.</p> <p>This context manager marks the beginning of a discrete unit of work (a \"run\"). All artifacts logged within this context will be associated with this run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>A unique identifier for this run.</p> required <code>model</code> <code>str</code> <p>The name of the model or system being run.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to use. Defaults to the active global tracker.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for the run, such as <code>tags</code>, <code>config</code>, or <code>inputs</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The active tracker instance.</p> Example <p>with consist.start_run(\"run_123\", \"my_model\"): ...     consist.log_artifact(\"data.csv\", \"input_data\")</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.define_step","level":2,"title":"<code>define_step(*, outputs=None, tags=None, description=None)</code>","text":"<p>Attach metadata to a function without changing execution behavior.</p> <p>This decorator lets you attach defaults such as <code>outputs</code> or <code>tags</code> to a function. <code>Tracker.run</code> and <code>ScenarioContext.run</code> read this metadata.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.scenario","level":2,"title":"<code>scenario(name, tracker=None, *, enabled=True, **kwargs)</code>","text":"<p>Proxy for <code>Tracker.scenario</code> to avoid importing the tracker directly.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the scenario (used for the header run ID).</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to use; defaults to the active global tracker.</p> <code>None</code> <code>enabled</code> <code>bool</code> <p>If False, returns a noop scenario context that executes without provenance tracking while preserving Coupler/RunResult ergonomics.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments forwarded to <code>Tracker.scenario</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>ScenarioContext</code> <p>Scenario context manager.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.single_step_scenario","level":2,"title":"<code>single_step_scenario(name, step_name=None, tracker=None, **kwargs)</code>","text":"<p>Convenience wrapper that exposes a single step scenario.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the scenario header.</p> required <code>step_name</code> <code>Optional[str]</code> <p>Name for the single step; defaults to <code>name</code>.</p> <code>None</code> <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker to execute the scenario; defaults to the active tracker.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments forwarded to <code>Tracker.scenario</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>ScenarioContext</code> <p>Scenario context manager for the single step.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.current_tracker","level":2,"title":"<code>current_tracker()</code>","text":"<p>Retrieves the active <code>Tracker</code> instance from the global context.</p> <p>If no run is active, this function falls back to the default tracker (if set via <code>consist.use_tracker</code> or <code>consist.set_current_tracker</code>).</p> <p>Returns:</p> Type Description <code>Tracker</code> <p>The <code>Tracker</code> instance currently active in the execution context.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no <code>Tracker</code> is active in the current context and no default tracker has been configured.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.current_run","level":2,"title":"<code>current_run()</code>","text":"<p>Return the active <code>Run</code> record if one is in progress, otherwise <code>None</code>.</p> <p>A <code>Run</code> record contains metadata about the current execution, such as its unique ID, model name, and start time.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.current_consist","level":2,"title":"<code>current_consist()</code>","text":"<p>Return the active <code>ConsistRecord</code> if one is in progress, otherwise <code>None</code>.</p> <p>The <code>ConsistRecord</code> is the internal state object that tracks the active run's inputs, outputs, and metadata during execution.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.cached_artifacts","level":2,"title":"<code>cached_artifacts(direction='output')</code>","text":"<p>Returns hydrated cached artifacts for the active run, if any.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>str</code> <p>\"output\" or \"input\".</p> <code>\"output\"</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping from artifact key to Artifact, or empty dict if no cache hit.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.cached_output","level":2,"title":"<code>cached_output(key=None)</code>","text":"<p>Fetch a hydrated cached output artifact for the active run.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[str]</code> <p>Specific artifact key to look up; defaults to the first available artifact.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>Cached artifact instance or <code>None</code> if no cache hit exists.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.get_artifact","level":2,"title":"<code>get_artifact(run_id, key=None, key_contains=None, direction='output')</code>","text":"<p>Retrieve a single artifact from a historical run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Identifier of the run that produced the artifact.</p> required <code>key</code> <code>Optional[str]</code> <p>Exact artifact key to match.</p> <code>None</code> <code>key_contains</code> <code>Optional[str]</code> <p>Substring filter for artifact keys.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Either \"input\" or \"output\".</p> <code>\"output\"</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>Matching artifact or <code>None</code> if not found.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_artifact","level":2,"title":"<code>log_artifact(path, key=None, direction='output', schema=None, driver=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', *, enabled=True, **meta)</code>","text":"<p>Logs an artifact (file or data reference) to the currently active run.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.log_artifact</code>. It automatically links the artifact to the current run context, handles path virtualization, and performs lineage discovery.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact (e.g., \"households\"). Required if <code>path</code> is a path-like (str/Path).</p> <code>None</code> <code>direction</code> <code>str</code> <p>Specifies whether the artifact is an \"input\" or \"output\" for the current run. Defaults to \"output\".</p> <code>\"output\"</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the artifact's data. Its name will be stored in artifact metadata.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify the driver (e.g., 'h5_table'). If None, the driver is inferred from the file extension.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>reuse_if_unchanged</code> <code>bool</code> <p>If True and logging an output, reuse a prior artifact row when the content hash matches.</p> <code>False</code> <code>reuse_scope</code> <code>(same_uri, any_uri)</code> <p>Scope for output reuse checks. \"same_uri\" restricts reuse to the same URI, while \"any_uri\" allows reuse across different URIs with the same hash.</p> <code>\"same_uri\"</code> <code>enabled</code> <code>bool</code> <p>If False, returns a noop artifact object without requiring an active run.</p> <code>True</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's flexible <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If <code>key</code> is not provided when <code>path</code> is a path-like (str/Path).</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_artifacts","level":2,"title":"<code>log_artifacts(outputs, *, direction='output', driver=None, metadata_by_key=None, reuse_if_unchanged=False, reuse_scope='same_uri', enabled=True, **shared_meta)</code>","text":"<p>Log multiple artifacts in a single call for efficiency.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.log_artifacts</code>. It logs a batch of related artifacts with optional per-key metadata customization.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Mapping[str, ArtifactRef]</code> <p>Mapping of key -&gt; path/Artifact to log. Keys must be strings and explicitly chosen by the caller (not inferred from filenames).</p> required <code>direction</code> <code>str</code> <p>\"input\" or \"output\" for the current run context.</p> <code>\"output\"</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify driver for all artifacts. If None, inferred from file extension.</p> <code>None</code> <code>metadata_by_key</code> <code>Optional[Mapping[str, Dict[str, Any]]]</code> <p>Per-key metadata overrides applied on top of shared metadata.</p> <code>None</code> <code>enabled</code> <code>bool</code> <p>If False, returns noop artifact objects without requiring an active run.</p> <code>True</code> <code>**shared_meta</code> <code>Any</code> <p>Metadata key-value pairs applied to ALL logged artifacts.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[str, ArtifactLike]</code> <p>Mapping of key -&gt; logged Artifact-like objects.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If metadata_by_key contains keys not in outputs, or if any value is None.</p> <code>TypeError</code> <p>If mapping keys are not strings.</p> <p>Examples:</p> <p>Log multiple outputs with shared metadata: <pre><code>artifacts = consist.log_artifacts(\n    {\n        \"persons\": \"results/persons.parquet\",\n        \"households\": \"results/households.parquet\",\n        \"jobs\": \"results/jobs.parquet\"\n    },\n    year=2030,\n    scenario=\"base\"\n)\n# All three artifacts get year=2030 and scenario=\"base\"\n</code></pre></p> <p>Mix shared and per-key metadata: <pre><code>artifacts = consist.log_artifacts(\n    {\n        \"persons\": \"output/persons.parquet\",\n        \"households\": \"output/households.parquet\"\n    },\n    metadata_by_key={\n        \"households\": {\"role\": \"primary_unit\", \"weight\": 1.0}\n    },\n    dataset_version=\"v2\",\n    simulation_id=\"run_001\"\n)\n# persons gets: dataset_version=\"v2\", simulation_id=\"run_001\"\n# households gets: dataset_version=\"v2\", simulation_id=\"run_001\",\n#                  role=\"primary_unit\", weight=1.0\n</code></pre></p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_input","level":2,"title":"<code>log_input(path, key=None, *, schema=None, driver=None, content_hash=None, force_hash_override=False, validate_content_hash=False, enabled=True, **meta)</code>","text":"<p>Log an input artifact to the active run.</p> <p>An input artifact represents a data source that the current run reads from. Logging it creates a lineage link, allowing Consist to track which versions of data produced which results.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>File path (str/Path) or an existing <code>Artifact</code> object.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact (e.g. \"raw_households\"). Required if <code>path</code> is a file path.</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Optional SQLModel class defining the data structure.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicit format driver (e.g. \"parquet\"). Inferred from extension if None.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed hash to avoid re-hashing large files.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>Overwrite existing hash in the database if different from <code>content_hash</code>.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>Re-hash the file on disk to ensure it matches the provided <code>content_hash</code>.</p> <code>False</code> <code>enabled</code> <code>bool</code> <p>If False, returns a dummy artifact object for disconnected execution.</p> <code>True</code> <code>**meta</code> <code>Any</code> <p>Additional metadata fields to store with the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ArtifactLike</code> <p>The logged artifact reference.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_output","level":2,"title":"<code>log_output(path, key=None, *, schema=None, driver=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', enabled=True, **meta)</code>","text":"<p>Log an output artifact produced by the current run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>File path (str/Path) or an existing <code>Artifact</code> object.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact (e.g. \"processed_results\"). Required if <code>path</code> is a file path.</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Optional SQLModel class defining the data structure.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicit format driver (e.g. \"parquet\"). Inferred from extension if None.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed hash to avoid re-hashing large files.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>Overwrite existing hash in the database if different from <code>content_hash</code>.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>Re-hash the file on disk to ensure it matches the provided <code>content_hash</code>.</p> <code>False</code> <code>reuse_if_unchanged</code> <code>bool</code> <p>If True, and the content hash matches a previous run's output, Consist may reuse that historical artifact record.</p> <code>False</code> <code>reuse_scope</code> <code>(same_uri, any_uri)</code> <p>Whether to restrict reuse to the exact same file path or any path with the same hash.</p> <code>\"same_uri\"</code> <code>enabled</code> <code>bool</code> <p>If False, returns a dummy artifact object.</p> <code>True</code> <code>**meta</code> <code>Any</code> <p>Additional metadata fields to store.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ArtifactLike</code> <p>The logged artifact reference.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_dataframe","level":2,"title":"<code>log_dataframe(df, key, schema=None, direction='output', tracker=None, path=None, driver=None, meta=None, **to_file_kwargs)</code>","text":"<p>Serialize a DataFrame, log it as an artifact, and trigger optional ingestion.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data to persist.</p> required <code>key</code> <code>str</code> <p>Logical artifact key.</p> required <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Schema used for ingestion, if provided.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Artifact direction relative to the run.</p> <code>\"output\"</code> <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to use; defaults to the active tracker. If None and no active run context exists, raises RuntimeError.</p> <code>None</code> <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Output path; defaults to <code>&lt;run_dir&gt;/outputs/&lt;run_subdir&gt;/&lt;key&gt;.&lt;driver&gt;</code> for the active run as determined by the tracker's run subdir configuration.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>File format driver (e.g., \"parquet\" or \"csv\").</p> <code>None</code> <code>meta</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the artifact.</p> <code>None</code> <code>**to_file_kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>pd.DataFrame.to_parquet</code> or <code>to_csv</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The artifact logged for the written dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested driver is unsupported.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.log_meta","level":2,"title":"<code>log_meta(**kwargs)</code>","text":"<p>Updates the active run's metadata with the provided key-value pairs.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.log_meta</code>. It allows users to log additional information about the current run, such as performance metrics, experimental parameters, or tags, directly to the run's metadata. This information is then persisted to both the JSON log and the DuckDB database.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary key-value pairs to merge into the <code>meta</code> dictionary of the current run. Existing keys will be updated, and new keys will be added.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called when no <code>Tracker</code> is active in the current context.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.ingest","level":2,"title":"<code>ingest(artifact, data=None, schema=None, run=None)</code>","text":"<p>Ingests data associated with an <code>Artifact</code> into the active run's database.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.ingest</code>. It materializes data into the DuckDB, leveraging <code>dlt</code> for efficient loading and injecting provenance information. This is part of the \"Hot Data Strategy\".</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact object representing the data being ingested.</p> required <code>data</code> <code>Optional[Union[Iterable[Dict[str, Any]], Any]]</code> <p>The data to ingest. Can be an iterable of dictionaries (rows), a file path (str or Path), a Pandas DataFrame, or any other data type that <code>dlt</code> can handle. If <code>None</code>, Consist attempts to read the data directly from the artifact's resolved file path.</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the ingested data. If provided, <code>dlt</code> will use this for strict validation and schema inference.</p> <code>None</code> <code>run</code> <code>Optional[Run]</code> <p>The <code>Run</code> context for ingestion. If provided, the data will be tagged with this run's ID (Offline Mode). If <code>None</code>, it defaults to the currently active run (Online Mode).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result information from the underlying <code>dlt</code> ingestion process.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no database is configured for the active <code>Tracker</code> or if <code>ingest</code> is called outside of an active run context.</p> <code>Exception</code> <p>Any exception raised by the underlying <code>dlt</code> ingestion process.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.register_views","level":2,"title":"<code>register_views(*models)</code>","text":"<p>Register hybrid view models for the active tracker.</p> <p>Parameters:</p> Name Type Description Default <code>*models</code> <code>Type[SQLModel]</code> <p>SQLModel classes describing the schema of hybrid views.</p> <code>()</code> <p>Returns:</p> Type Description <code>Dict[str, Type[SQLModel]]</code> <p>Mapping from model class name to the generated view model.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.find_run","level":2,"title":"<code>find_run(tracker=None, **filters)</code>","text":"<p>Convenience proxy for <code>Tracker.find_run</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to query; defaults to the active tracker.</p> <code>None</code> <code>**filters</code> <code>Any</code> <p>Filter values forwarded to <code>Tracker.find_run</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>Matching run or <code>None</code> when no match exists.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.find_runs","level":2,"title":"<code>find_runs(tracker=None, **filters)</code>","text":"<p>Convenience proxy for <code>Tracker.find_runs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance to query; defaults to the active tracker.</p> <code>None</code> <code>**filters</code> <code>Any</code> <p>Filter values forwarded to <code>Tracker.find_runs</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[list[Run], Dict[Hashable, Run]]</code> <p>Results returned by <code>Tracker.find_runs</code>.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.db_session","level":2,"title":"<code>db_session(tracker=None)</code>","text":"<p>Provide a SQLModel <code>Session</code> connected to the tracker's database.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance supplying the engine; defaults to active tracker.</p> <code>None</code> <p>Yields:</p> Type Description <code>Session</code> <p>SQLModel session bound to the tracker engine.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.run_query","level":2,"title":"<code>run_query(query, tracker=None)</code>","text":"<p>Execute a SQLModel/SQLAlchemy query via the tracker engine.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Executable</code> <p>Query object (<code>select</code>, etc.).</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker instance supplying the engine; defaults to the active tracker.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>Results of the executed query.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.pivot_facets","level":2,"title":"<code>pivot_facets(*, namespace, keys, value_column='value_num', value_columns=None, label_prefix='', label_map=None, run_id_label='run_id', table=RunConfigKV)</code>","text":"<p>Build a pivoted facet subquery keyed by run_id.</p> <p>This is a convenience helper for turning flattened config facets (<code>run_config_kv</code>) into a wide table suitable for joins.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Optional[str]</code> <p>Facet namespace to filter by (typically the model name). If <code>None</code>, the namespace filter is skipped.</p> required <code>keys</code> <code>Iterable[str]</code> <p>Facet keys to pivot into columns.</p> required <code>value_column</code> <code>str</code> <p>Default value column to read from. Must be one of: <code>value_num</code>, <code>value_str</code>, <code>value_bool</code>, <code>value_json</code>.</p> <code>\"value_num\"</code> <code>value_columns</code> <code>Optional[Mapping[str, str]]</code> <p>Optional per-key override of <code>value_column</code>.</p> <code>None</code> <code>label_prefix</code> <code>str</code> <p>Optional prefix for pivoted column labels.</p> <code>\"\"</code> <code>label_map</code> <code>Optional[Mapping[str, str]]</code> <p>Optional per-key label overrides.</p> <code>None</code> <code>run_id_label</code> <code>str</code> <p>Label to use for the run id column in the returned subquery.</p> <code>\"run_id\"</code> <code>table</code> <code>Type[RunConfigKV]</code> <p>Table/model providing the facet KV rows.</p> <code>RunConfigKV</code> <p>Returns:</p> Type Description <code>Any</code> <p>A SQLAlchemy subquery with columns: <code>run_id</code> and one column per key.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.capture_outputs","level":2,"title":"<code>capture_outputs(directory, pattern='*', recursive=False)</code>","text":"<p>Context manager to automatically capture and log new or modified files in a directory within the current active run context.</p> <p>This function is a convenient proxy to <code>consist.core.tracker.Tracker.capture_outputs</code>. It watches a specified <code>directory</code> for any file changes (creations or modifications) that occur within its <code>with</code> block. These changes are then automatically logged as output artifacts of the current Consist run.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>The path to the directory to monitor for new or modified files.</p> required <code>pattern</code> <code>str</code> <p>A glob pattern (e.g., \".csv\", \"data_.parquet\") to filter which files are captured within the specified directory. Defaults to all files.</p> <code>\"*\"</code> <code>recursive</code> <code>bool</code> <p>If True, the capture will recursively scan subdirectories within <code>directory</code> for changes.</p> <code>False</code> <p>Yields:</p> Type Description <code>OutputCapture</code> <p>An <code>OutputCapture</code> object containing a list of <code>Artifact</code> objects that were captured and logged after the <code>with</code> block finishes.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>capture_outputs</code> is used outside of an active <code>start_run</code> context.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.load","level":2,"title":"<code>load(artifact, tracker=None, *, db_fallback='inputs-only', **kwargs)</code>","text":"<pre><code>load(artifact: ZarrArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; xarray.Dataset\n</code></pre><pre><code>load(artifact: NetCdfArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; xarray.Dataset\n</code></pre><pre><code>load(artifact: OpenMatrixArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; xarray.Dataset\n</code></pre><pre><code>load(artifact: SpatialArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; geopandas.GeoDataFrame\n</code></pre><pre><code>load(artifact: HdfStoreArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; pd.HDFStore\n</code></pre><pre><code>load(artifact: DataFrameArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; duckdb.DuckDBPyRelation\n</code></pre><pre><code>load(artifact: JsonArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; duckdb.DuckDBPyRelation\n</code></pre><pre><code>load(artifact: TabularArtifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; duckdb.DuckDBPyRelation\n</code></pre><pre><code>load(artifact: Artifact, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; LoadResult\n</code></pre><pre><code>load(artifact: ArtifactLike, tracker: Optional[Tracker] = None, *, db_fallback: str = 'inputs-only', **kwargs: Any) -&gt; LoadResult\n</code></pre> <p>Smart loader that retrieves data for an artifact from the best available source.</p> <p>This function attempts to load the data associated with an <code>Artifact</code> object. It prioritizes loading from disk (raw format) if the file exists. If the file is missing but the artifact is marked as ingested, it can optionally recover the data from the Consist DuckDB database (\"Ghost Mode\"). By default, DB recovery is only allowed when the artifact is an input to an active, non-cached run.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The Consist <code>Artifact</code> object whose data is to be loaded.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>The <code>Tracker</code> instance to use for path resolution and database access. If <code>None</code>, the function attempts to use the active global tracker context. Explicitly passing a <code>tracker</code> is recommended for clarity or when no global context is available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the underlying data loader function (e.g., <code>pd.read_parquet</code>, <code>pd.read_csv</code>, <code>xr.open_zarr</code>, <code>pd.read_sql</code>).</p> <code>{}</code> <code>db_fallback</code> <code>str</code> <p>Controls when the loader is allowed to fall back to DuckDB (\"Ghost Mode\") when the file is missing but the artifact is marked as ingested.</p> <ul> <li>\"inputs-only\": allow DB fallback only if the artifact is declared as an input   to the current active run AND the current run is not a cache hit.</li> <li>\"always\": allow DB fallback whenever <code>artifact.meta[\"is_ingested\"]</code> is true and   a tracker with a DB connection is available.</li> <li>\"never\": disable DB fallback entirely.</li> </ul> <code>\"inputs-only\"</code> <p>Returns:</p> Type Description <code>LoadResult</code> <p>The loaded data, typically a DuckDB Relation for tabular data, an xarray Dataset for array formats, or another data object depending on the artifact's <code>driver</code> and the data format.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no <code>Tracker</code> instance can be resolved (neither provided nor active in context) and the artifact's absolute path is not directly resolvable. Also if the artifact is marked as ingested but no tracker with a DB connection is available.</p> <code>FileNotFoundError</code> <p>If the artifact's data cannot be found on disk or recovered from the database.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.load_df","level":2,"title":"<code>load_df(artifact, tracker=None, *, db_fallback='inputs-only', close=True, **kwargs)</code>","text":"<p>Load a tabular artifact and return a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to load.</p> required <code>tracker</code> <code>Optional[Tracker]</code> <p>Tracker to use for resolving paths or DB fallback.</p> <code>None</code> <code>db_fallback</code> <code>str</code> <p>Controls when DB recovery is allowed for ingested artifacts.</p> <code>\"inputs-only\"</code> <code>close</code> <code>bool</code> <p>Whether to close the underlying DuckDB connection after materialization when the load returns a Relation.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional loader options.</p> <code>{}</code>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.load_relation","level":2,"title":"<code>load_relation(artifact, tracker=None, *, db_fallback='inputs-only', **kwargs)</code>","text":"<p>Context manager that yields a DuckDB Relation and ensures the underlying connection is closed on exit.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.to_df","level":2,"title":"<code>to_df(relation, *, close=True)</code>","text":"<p>Convert a DuckDB Relation to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>relation</code> <code>DuckDBPyRelation</code> <p>Relation to materialize into a DataFrame.</p> required <code>close</code> <code>bool</code> <p>Whether to close the underlying DuckDB connection after materialization. Use <code>close=False</code> if you plan to continue using the relation.</p> <code>True</code>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.active_relation_count","level":2,"title":"<code>active_relation_count()</code>","text":"<p>Return the number of active DuckDB relations tracked by Consist.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.set_current_tracker","level":2,"title":"<code>set_current_tracker(tracker)</code>","text":"<p>Set the default (fallback) tracker used by Consist entrypoints.</p> <p>Entrypoints like <code>consist.run()</code>, <code>consist.start_run()</code>, and <code>consist.scenario()</code> use this tracker if they are called outside of an active run context and no explicit <code>tracker=</code> argument is provided.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Optional[Tracker]</code> <p>The tracker instance to set as the default, or <code>None</code> to clear.</p> required <p>Returns:</p> Type Description <code>Optional[Tracker]</code> <p>The previously configured default tracker, if any.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.noop_scenario","level":2,"title":"<code>noop_scenario(name, **kwargs)</code>","text":"<p>Creates a scenario context that executes without provenance tracking.</p> <p>This is useful for debugging or running simulations where you want the ergonomics of the Consist scenario API (like the Coupler and RunResult) but do not want to record any metadata or artifacts to the database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the scenario (for display/logging purposes).</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments forwarded to the noop context.</p> <code>{}</code>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_dataframe_artifact","level":2,"title":"<code>is_dataframe_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to tabular types (parquet, csv, h5_table).</p> <p>Use this to enable type-safe loading and IDE autocomplete:</p> <pre><code>if is_dataframe_artifact(artifact):\n    rel = load(artifact)  # Type checker knows return is Relation\n    df.head()  # IDE autocomplete works!\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is parquet, csv, or h5_table.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_tabular_artifact","level":2,"title":"<code>is_tabular_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to any tabular format (parquet, csv, h5_table, json).</p> <p>Note: This is broader than <code>is_dataframe_artifact()</code>, so <code>load()</code> returns a Relation for tabular artifacts. Use <code>load_df()</code> for a pandas escape hatch.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver produces tabular data.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_json_artifact","level":2,"title":"<code>is_json_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is json.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_zarr_artifact","level":2,"title":"<code>is_zarr_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to Zarr format.</p> <p>Use this when you know an artifact should be Zarr and want type-safe loading:</p> <pre><code>if is_zarr_artifact(artifact):\n    ds = load(artifact)  # Type checker knows return is xarray.Dataset\n    ds.dims  # IDE autocomplete works!\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is zarr.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_hdf_artifact","level":2,"title":"<code>is_hdf_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to HDF5 format (h5 or hdf5).</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is h5 or hdf5.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_netcdf_artifact","level":2,"title":"<code>is_netcdf_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to NetCDF format.</p> <p>Use this when you know an artifact should be NetCDF and want type-safe loading:</p> <pre><code>if is_netcdf_artifact(artifact):\n    ds = load(artifact)  # Type checker knows return is xarray.Dataset\n    ds.dims  # IDE autocomplete works!\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is netcdf.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_openmatrix_artifact","level":2,"title":"<code>is_openmatrix_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to OpenMatrix format.</p> <p>Use this when you know an artifact should be OpenMatrix and want type-safe loading:</p> <pre><code>if is_openmatrix_artifact(artifact):\n    matrix_data = load(artifact)  # Type checker knows return is appropriate type\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is openmatrix.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/api_helpers/#consist.api.is_spatial_artifact","level":2,"title":"<code>is_spatial_artifact(artifact)</code>","text":"<p>Type guard: narrow artifact to spatial formats (GeoDataFrame outputs).</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>ArtifactLike</code> <p>Artifact to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if artifact driver is geojson, shapefile, or geopackage.</p>","path":["API Reference","API Helpers"],"tags":[]},{"location":"api/artifact/","level":1,"title":"Artifact","text":"<p>               Bases: <code>SQLModel</code></p> <p>Represents a physical data object in the Consist database.</p> <p>This table stores canonical metadata for any file/dataset Consist tracks. It is linked to runs via <code>run_artifact_link</code> to record whether an artifact was an input or output. The <code>run_id</code> field records the producing run (if any) and is often <code>None</code> for external inputs.</p> <p>Artifacts are the core building blocks of provenance and caching. Each artifact has a unique identity, a virtualized location, and rich metadata, supporting both \"hot\" (ingested) and \"cold\" (file-based) data strategies.</p> <p>Attributes:     id (uuid.UUID): A unique identifier for the artifact.     key (str): A semantic, human-readable name for the artifact (e.g., \"households\", \"parcels\").     container_uri (str): A portable, virtualized Uniform Resource Identifier (URI) for the                artifact's location (e.g., \"inputs://land_use.csv\").     table_path (Optional[str]): Optional path inside a container (e.g., \"/tables/households\").     array_path (Optional[str]): Optional path inside a container for array artifacts.     driver (str): The name of the format handler used to read or write the artifact                   (e.g., \"parquet\", \"csv\", \"zarr\").     hash (Optional[str]): SHA256 content hash of the artifact's data, enabling content-addressable                           lookups and deduplication.     run_id (Optional[str]): The ID of the run that generated this artifact. Null for inputs.     meta (Dict[str, Any]): A flexible JSON field for storing arbitrary metadata, such as                            schema signatures, or data dimensions.     created_at (datetime): The timestamp when the artifact was first logged.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.abs_path","level":2,"title":"<code>abs_path</code>  <code>property</code> <code>writable</code>","text":"<p>Runtime-only helper to access the absolute path of this artifact.</p> <p>This property provides the resolved absolute file system path for the artifact. It is not persisted to the database but is crucial for local file operations and for chaining Consist runs within the same script or environment.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The absolute file system path of the artifact, or <code>None</code> if it has not yet been resolved or set.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.path","level":2,"title":"<code>path</code>  <code>property</code>","text":"<p>Resolve this artifact to a filesystem Path.</p> <p>Uses the tracker when available to handle mount-aware URIs; otherwise falls back to the cached absolute path or the raw URI.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.is_matrix","level":2,"title":"<code>is_matrix</code>  <code>property</code>","text":"<p>Indicates if the artifact represents a multi-dimensional array or matrix-like data.</p> <p>This property helps in dispatching to appropriate data loaders or processing functions that handle array-based data structures, such as those typically found in scientific computing.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the artifact's driver is associated with matrix-like data formats (e.g., Zarr, HDF5, NetCDF, OpenMatrix), False otherwise.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.is_tabular","level":2,"title":"<code>is_tabular</code>  <code>property</code>","text":"<p>Indicates if the artifact represents tabular data (rows and columns).</p> <p>This property assists in identifying artifacts that can be loaded and processed using tools designed for structured, record-based data, such as Pandas DataFrames.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the artifact's driver is associated with tabular data formats (e.g., Parquet, CSV, SQL), False otherwise.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.created_at_iso","level":2,"title":"<code>created_at_iso</code>  <code>property</code>","text":"<p>Return created_at as an ISO 8601 formatted string.</p> <p>Useful for serialization to external systems that expect string timestamps.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The created_at timestamp as ISO 8601 string, or None if not set.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/artifact/#consist.models.artifact.Artifact.get_meta","level":2,"title":"<code>get_meta(key, default=None)</code>","text":"<p>Safely retrieves a value from the 'meta' dictionary.</p> <p>Args:     key (str): The key to look up in the metadata.     default (Any, optional): The default value to return if the key is not found.                              Defaults to None.</p> <p>Returns:     Any: The value associated with the key, or the default value if the key is not present.</p>","path":["API Reference","Artifact"],"tags":[]},{"location":"api/identity/","level":1,"title":"Identity Manager","text":"<p>Manages the cryptographic identity of a Run, which is fundamental to Consist's reproducibility and caching features.</p> <p>To achieve robust caching and \"run forking\", a Run's identity is defined as a composite SHA256 hash, ensuring that any change in code, configuration, or input provenance results in a unique run signature.</p> <p>H_run = SHA256( H_code + H_config + H_inputs )</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.canonical_json_str","level":2,"title":"<code>canonical_json_str(obj)</code>","text":"<p>Return a stable JSON string for hashing/IDs.</p> <p>Uses <code>_clean_structure</code> to normalize types and then dumps with deterministic key ordering and compact separators.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.canonical_json_sha256","level":2,"title":"<code>canonical_json_sha256(obj)</code>","text":"<p>SHA256 hex digest of <code>canonical_json_str(obj)</code>.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.normalize_json","level":2,"title":"<code>normalize_json(obj)</code>","text":"<p>Normalize Python structures into JSON-friendly types.</p> <p>This mirrors the canonical hashing cleanup but preserves the full structure without excluding any keys.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.calculate_run_signature","level":2,"title":"<code>calculate_run_signature(code_hash, config_hash, input_hash)</code>","text":"<p>Computes the final cryptographic signature (cache key) for a run.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.get_code_version","level":2,"title":"<code>get_code_version()</code>","text":"<p>Retrieves the global 'Code Identity' using the Git Commit SHA.</p> <p>This uses GitPython directly to avoid subprocess overhead and parsing fragility.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_callable_hash","level":2,"title":"<code>compute_callable_hash(func, strategy='module', extra_deps=None)</code>","text":"<p>Computes a hash for a specific Python function/callable.</p> <p>This allows for granular caching (ignoring global repo changes) by focusing on the relevant code.</p> Strategies: <p>'source':     Hashes ONLY the function's source code (via <code>inspect.getsource</code>).     Use this for pure functions with no external dependencies. 'module':     Hashes the entire file (.py) where the function is defined.     This is the robust \"in-between\": it captures helper functions and     constants in the same file, but ignores changes in unrelated files.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to hash.</p> required <code>strategy</code> <code>str</code> <p>The hashing strategy (\"source\" or \"module\").</p> <code>\"module\"</code> <code>extra_deps</code> <code>List[str]</code> <p>List of additional file paths (relative to project root) that this function depends on. Their content will be mixed into the hash.</p> <code>None</code>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_config_hash","level":2,"title":"<code>compute_config_hash(config, exclude_keys=None)</code>","text":"<p>Generates a deterministic SHA256 hash of the configuration dictionary.</p> <p>This method implements \"Canonical Config Hashing\" by: 1.  Removing specified <code>exclude_keys</code> (e.g., non-deterministic values like timestamps,     or sensitive information that should not affect reproducibility).</p> <ol> <li> <p>Converting any NumPy types to native Python types, addressing \"The NumPy Problem\"     to prevent serialization errors and ensure consistent hashing across different environments.</p> </li> <li> <p>Recursively sorting dictionary keys to guarantee a canonical JSON representation     regardless of the original insertion order.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary to hash.</p> required <code>exclude_keys</code> <code>Optional[List[str]]</code> <p>A list of keys whose values should be excluded from the hashing process. Defaults to an empty list.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A SHA256 hex digest of the canonicalized configuration.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_run_config_hash","level":2,"title":"<code>compute_run_config_hash(*, config, model, year=None, iteration=None)</code>","text":"<p>Compute a config hash for a run, mixing in identity-relevant run fields.</p> <p>Tracker persists <code>config</code> for human inspection, but caching identity needs to include some run context fields that are frequently semantically relevant.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_input_hash","level":2,"title":"<code>compute_input_hash(inputs, path_resolver=None, signature_lookup=None)</code>","text":"<p>Generates a deterministic hash representing the state of all input artifacts.</p> <p>This hash contributes to the Merkle DAG by incorporating the unique identities of all inputs, ensuring that a change in any upstream data source results in a new run signature. It handles two main scenarios for inputs:</p> <ol> <li> <p>Provenance Exists: If an <code>Artifact</code> object is linked to a previous run     (<code>artifact.run_id</code> is present), its <code>run_id</code> is used as its identity,     forming a direct link in the provenance graph. This enables \"run forking\"     and efficient cache invalidation.</p> </li> <li> <p>Raw File Input: If the input is a raw file not previously generated by     Consist (i.e., <code>artifact.run_id</code> is None), its content or metadata is hashed     (based on <code>hashing_strategy</code>) to establish its identity. A <code>path_resolver</code>     is required in this case to access the file on the local filesystem.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Artifact]</code> <p>A list of <code>Artifact</code> objects representing the inputs to the run.</p> required <code>path_resolver</code> <code>Optional[Callable[[str], str]]</code> <p>A function that takes an <code>Artifact</code> URI (a portable string like \"inputs://data.csv\") and returns an absolute file path on the local filesystem. This is required for hashing the content of raw files.</p> <code>None</code> <code>signature_lookup</code> <code>Optional[Callable[[str], str]]</code> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A SHA256 hex digest representing the combined and ordered identity of all inputs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a raw file artifact needs to be hashed but no <code>path_resolver</code> function is provided.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_file_checksum","level":2,"title":"<code>compute_file_checksum(file_path)</code>","text":"<p>Computes a cryptographic identifier for a given file or directory based on the configured hashing strategy.</p> <p>This method is critical for establishing the unique identity of raw file-based inputs to a Consist run. It supports two main strategies: 'full' (content-based) and 'fast' (metadata-based), and handles both single files and directories.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The absolute path to the file or directory for which to compute the checksum.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A SHA256 hex digest representing the checksum or identity of the file/directory.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified <code>file_path</code> does not exist on the filesystem.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If 'full' content hashing is performed on a directory, as this can be computationally expensive for large directories.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.label_for_hash_input","level":2,"title":"<code>label_for_hash_input(path)</code>","text":"<p>Create a stable, human-friendly label for a hash input path.</p> <p>This is used when recording inputs that are represented only by their hash (e.g., \"hash-only\" config inputs). The method prefers a path that is relative to <code>project_root</code> for readability and portability, and falls back to the original string if it cannot be made relative.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>A file or directory path used as a hash input.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string label suitable for logs and provenance records.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.digest_path","level":2,"title":"<code>digest_path(path, *, ignore_dotfiles=True, allowlist=None)</code>","text":"<p>Digest a file or directory with optional filtering.</p> <ul> <li>Files: delegated to <code>compute_file_checksum</code> (honors hashing_strategy).</li> <li>Directories: deterministic digest over relative paths + (content or metadata).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ignore_dotfiles</code> <code>bool</code> <p>If True, ignore any file whose relative path includes a component starting with '.'.</p> <code>True</code> <code>allowlist</code> <code>Optional[List[str]]</code> <p>If provided, only include files whose relative path matches at least one glob pattern.</p> <code>None</code>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/identity/#consist.core.identity.IdentityManager.compute_hash_inputs_digests","level":2,"title":"<code>compute_hash_inputs_digests(hash_inputs, *, ignore_dotfiles=True, allowlist=None)</code>","text":"<p>Compute digests for external \"hash-only\" config inputs (files or directories).</p> <p>Items may be: - A path (str/Path): label derived from project-relative path when possible. - A (label, path) tuple: explicit label.</p>","path":["API Reference","Identity Manager"],"tags":[]},{"location":"api/indexing/","level":1,"title":"Indexing Helpers","text":"","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.RunIndexField","level":2,"title":"<code>RunIndexField = Literal['id', 'model_name', 'status', 'year', 'iteration', 'parent_run_id', 'config_hash', 'input_hash', 'git_hash', 'signature', 'description', 'created_at', 'started_at', 'ended_at']</code>  <code>module-attribute</code>","text":"","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.RunFieldIndex","level":2,"title":"<code>RunFieldIndex</code>  <code>dataclass</code>","text":"<p>Index <code>Tracker.find_runs(..., index_by=...)</code> results by a <code>Run</code> attribute.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.FacetIndex","level":2,"title":"<code>FacetIndex</code>  <code>dataclass</code>","text":"<p>Index <code>Tracker.find_runs(..., index_by=...)</code> results by a persisted facet value.</p> <p>The facet must be indexed to <code>RunConfigKV</code> (default when <code>facet_index=True</code>) and requires a DB-backed tracker.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.index_by_field","level":2,"title":"<code>index_by_field(field)</code>","text":"<p>Typed helper for <code>index_by=...</code> keyed by a Run field.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/indexing/#consist.core.indexing.index_by_facet","level":2,"title":"<code>index_by_facet(key)</code>","text":"<p>Typed helper for <code>index_by=...</code> keyed by a facet key.</p>","path":["API Reference","Indexing Helpers"],"tags":[]},{"location":"api/materialize/","level":1,"title":"Materialization","text":"","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.materialize_artifacts","level":2,"title":"<code>materialize_artifacts(tracker, items, *, on_missing='warn')</code>","text":"<p>Copy cached artifact bytes onto the filesystem at caller-specified destinations.</p> <p>This is intentionally copy-only materialization: - If the artifact's resolved source path exists, it is copied to the destination. - If it does not exist, behavior depends on <code>on_missing</code>. - If the destination already exists, it is left untouched. - Destination paths cannot be symlinks and will not be overwritten if the type differs.</p> <p>This function does not attempt database-backed reconstruction. Higher-level code can decide when to call <code>consist.load(...)</code> or enable DB recovery.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Tracker</code> <p>Tracker used to resolve portable artifact URIs to host filesystem paths.</p> required <code>items</code> <code>Sequence[tuple[Artifact, Path]]</code> <p>A list of <code>(artifact, destination_path)</code> pairs to materialize.</p> required <code>on_missing</code> <code>('warn', 'raise')</code> <p>What to do when a resolved source path does not exist.</p> <code>\"warn\"</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Map of <code>artifact.key -&gt; destination_path</code> for successfully materialized items.</p>","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.materialize_artifacts_from_sources","level":2,"title":"<code>materialize_artifacts_from_sources(items, *, allowed_base, on_missing='warn')</code>","text":"<p>Copy artifact bytes from explicit source paths to caller-specified destinations.</p> <p>This is useful for rehydrating cached inputs from historical run directories when the current run directory is different from the original producer. Existing destination paths are left untouched. Destination paths cannot be symlinks and will not be overwritten if the type differs.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_base</code> <code>Path | None</code> <p>Base directory that destination paths must remain within. When None, no base containment check is performed.</p> required","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.build_materialize_items_for_keys","level":2,"title":"<code>build_materialize_items_for_keys(outputs, *, destinations_by_key)</code>","text":"<p>Convenience helper to construct <code>(artifact, destination)</code> pairs by artifact key.</p> <p>Any keys not present in <code>outputs</code> are ignored (caller decides whether to treat this as an error).</p>","path":["API Reference","Materialization"],"tags":[]},{"location":"api/materialize/#consist.core.materialize.materialize_ingested_artifact_from_db","level":2,"title":"<code>materialize_ingested_artifact_from_db(*, artifact, tracker, destination)</code>","text":"<p>Reconstruct a CSV/Parquet artifact from DuckDB and write it to disk.</p> <p>This is intended for <code>cache_hydration=\"inputs-missing\"</code> when the original on-disk source is missing but the artifact is ingested (<code>is_ingested=True</code>).</p> <p>Supported drivers: csv, parquet. All other drivers raise ValueError.</p>","path":["API Reference","Materialization"],"tags":[]},{"location":"api/matrix/","level":1,"title":"Matrix Views","text":"<p>A factory class responsible for creating virtual xarray Datasets from Consist's metadata.</p> <p>This factory allows users to query the Consist metadata catalog for multi-dimensional data (matrices) and lazily load the corresponding Zarr stores from disk. It consolidates data across different runs into a single, unified xarray Dataset, indexed by <code>run_id</code>, <code>year</code>, and <code>iteration</code>, facilitating comparative analysis and exploration.</p> <p>Attributes:</p> Name Type Description <code>tracker</code> <code>Tracker</code> <p>An instance of the Consist <code>Tracker</code>, which provides access to the database engine and artifact resolution necessary for identifying and loading matrix data.</p>","path":["API Reference","Matrix Views"],"tags":[]},{"location":"api/matrix/#consist.core.matrix.MatrixViewFactory.load_matrix_view","level":2,"title":"<code>load_matrix_view(concept_key, variables=None, *, run_ids=None, parent_id=None, model=None, status=None)</code>","text":"<p>Returns a lazy xarray Dataset containing all runs that match the <code>concept_key</code>.</p> <p>This method queries the Consist database for all matrix-type artifacts associated with the given <code>concept_key</code>. It then lazily opens each corresponding Zarr store using <code>xarray</code> and concatenates them into a single <code>xarray.Dataset</code> along a new <code>run_id</code> dimension. This allows for convenient analysis of multi-dimensional data across different experimental runs.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key (e.g., \"model_output_grid\", \"simulation_results\") identifying the collection of matrix artifacts to load.</p> required <code>variables</code> <code>Optional[List[str]]</code> <p>A list of variable names to load from each Zarr store. If <code>None</code>, all variables from each store will be loaded.</p> <code>None</code> <code>run_ids</code> <code>Optional[List[str]]</code> <p>Optional list of run IDs to include in the view.</p> <code>None</code> <code>parent_id</code> <code>Optional[str]</code> <p>Optional scenario/parent run ID to filter by.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional model name to filter by.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Optional run status to filter by (e.g., \"completed\").</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>A lazy-loaded <code>xarray.Dataset</code> containing the combined data from all matching matrix artifacts, with a new <code>run_id</code> dimension and <code>year</code>/<code>iteration</code> coordinates. Returns an empty <code>xr.Dataset</code> if no matching artifacts are found or can be loaded.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the <code>xarray</code> library is not installed.</p> <code>RuntimeError</code> <p>If the <code>Tracker</code> instance does not have a configured database connection.</p>","path":["API Reference","Matrix Views"],"tags":[]},{"location":"api/public_api/","level":1,"title":"Public API (v0.1)","text":"<p>This page defines Consist's public API for the <code>0.1.x</code> series. Items listed as Advanced are still public but are primarily targeted at advanced users and may be more verbose, lower-level, or easier to misuse.</p>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#stable-intended-for-external-users","level":2,"title":"Stable (intended for external users)","text":"<ul> <li><code>consist.Tracker</code></li> <li><code>consist.Run</code></li> <li><code>consist.Artifact</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#scenario-workflow-helpers","level":3,"title":"Scenario / workflow helpers","text":"<ul> <li><code>consist.scenario</code></li> <li><code>consist.single_step_scenario</code></li> <li><code>consist.run</code></li> <li><code>consist.trace</code></li> <li><code>consist.start_run</code></li> <li><code>consist.define_step</code></li> <li><code>consist.use_tracker</code></li> <li><code>ScenarioContext</code> (returned by <code>consist.scenario(...)</code>)</li> <li><code>run_id</code>, <code>config</code>, <code>inputs</code>, <code>add_input</code>, <code>declare_outputs</code>, <code>require_outputs</code>, <code>collect_by_keys</code>, <code>run</code>, <code>trace</code></li> <li><code>RunContext</code> (injected via <code>inject_context=True</code>)</li> <li><code>run_dir</code>, <code>inputs</code>, <code>load</code>, <code>log_artifact</code>, <code>log_artifacts</code>, <code>log_input</code>, <code>log_output</code>, <code>log_meta</code>, <code>capture_outputs</code></li> <li><code>Coupler</code> (available at <code>scenario.coupler</code>)</li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#artifact-logging-and-loading","level":3,"title":"Artifact logging and loading","text":"<ul> <li><code>consist.log_artifact</code></li> <li><code>consist.log_dataframe</code></li> <li><code>consist.load</code></li> <li><code>consist.capture_outputs</code></li> <li><code>consist.get_artifact</code></li> <li><code>consist.cached_output</code></li> <li><code>consist.cached_artifacts</code></li> <li><code>consist.log_meta</code></li> <li><code>consist.current_run</code></li> <li><code>consist.current_consist</code></li> <li><code>Tracker.define_step(...)</code> (explicit tracker form)</li> <li><code>Tracker.materialize(...)</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#querying-and-views","level":3,"title":"Querying and views","text":"<ul> <li><code>consist.view</code></li> <li><code>consist.register_views</code></li> <li><code>consist.find_run</code></li> <li><code>consist.find_runs</code></li> <li><code>Tracker.find_latest_run(...)</code></li> <li><code>Tracker.diff_runs(...)</code></li> <li><code>Tracker.get_run_inputs(...)</code> / <code>Tracker.get_run_outputs(...)</code></li> <li><code>Tracker.get_run_config(...)</code></li> <li><code>Tracker.print_lineage(...)</code></li> <li><code>consist.run_query</code></li> <li><code>consist.db_session</code></li> <li><code>consist.pivot_facets</code></li> <li>Indexing helpers: <code>consist.index_by_field</code>, <code>consist.index_by_facet</code>, plus <code>RunFieldIndex</code> / <code>FacetIndex</code></li> <li>Views registry: <code>tracker.views</code> (<code>ViewRegistry</code>)</li> <li>Matrix utilities: <code>Tracker.load_matrix(...)</code>, <code>MatrixViewFactory</code></li> <li>Schema export: <code>Tracker.export_schema_sqlmodel(...)</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#tracker-methods-complete-public-surface","level":3,"title":"Tracker methods (complete public surface)","text":"<p>This section enumerates all non-underscore <code>Tracker</code> methods. If you're new to Consist, start with the Core and Logging/Loading groups and reach for Advanced only as needed.</p>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#core-lifecycle","level":4,"title":"Core lifecycle","text":"<ul> <li><code>begin_run</code>, <code>start_run</code>, <code>run</code>, <code>trace</code>, <code>scenario</code>, <code>end_run</code></li> <li><code>define_step</code>, <code>last_run</code>, <code>is_cached</code>, <code>cached_artifacts</code>, <code>cached_output</code></li> <li><code>suspend_cache_options</code>, <code>restore_cache_options</code>, <code>capture_outputs</code>, <code>log_meta</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#logging-and-loading","level":4,"title":"Logging and loading","text":"<ul> <li><code>log_artifact</code>, <code>log_artifacts</code>, <code>log_input</code>, <code>log_output</code>, <code>log_dataframe</code></li> <li><code>load</code>, <code>materialize</code>, <code>ingest</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#querying-and-history","level":4,"title":"Querying and history","text":"<ul> <li><code>find_runs</code>, <code>find_run</code>, <code>find_latest_run</code>, <code>get_latest_run_id</code></li> <li><code>find_artifacts</code>, <code>get_artifact</code>, <code>get_artifacts_for_run</code></li> <li><code>get_run</code>, <code>get_run_config</code>, <code>get_run_inputs</code>, <code>get_run_outputs</code></li> <li><code>get_artifact_lineage</code>, <code>print_lineage</code>, <code>history</code></li> <li><code>diff_runs</code>, <code>get_config_facet</code>, <code>get_config_facets</code>, <code>get_run_config_kv</code></li> <li><code>get_config_values</code>, <code>get_config_value</code>, <code>find_runs_by_facet_kv</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#views-and-matrices","level":4,"title":"Views and matrices","text":"<ul> <li><code>view</code>, <code>create_view</code>, <code>load_matrix</code>, <code>export_schema_sqlmodel</code></li> <li><code>netcdf_metadata</code>, <code>openmatrix_metadata</code>, <code>spatial_metadata</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#config-canonicalization","level":4,"title":"Config canonicalization","text":"<ul> <li><code>canonicalize_config</code>, <code>prepare_config</code>, <code>apply_config_plan</code>, <code>identity_from_config_plan</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#format-specific-logging","level":4,"title":"Format-specific logging","text":"<ul> <li><code>log_h5_container</code>, <code>log_h5_table</code>, <code>log_netcdf_file</code>, <code>log_openmatrix_file</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#advanced-power-user-lower-level","level":3,"title":"Advanced (power-user / lower-level)","text":"<p>These methods are still public, but are more low-level or easier to misuse.</p> <ul> <li><code>engine</code>, <code>set_run_subdir_fn</code>, <code>run_artifact_dir</code>, <code>resolve_uri</code></li> <li><code>run_query</code>, <code>get_run_record</code>, <code>resolve_historical_path</code>, <code>load_input_bundle</code></li> <li><code>get_artifact_by_uri</code>, <code>get_run_artifact</code>, <code>load_run_output</code>, <code>find_matching_run</code></li> <li><code>on_run_start</code>, <code>on_run_complete</code>, <code>on_run_failed</code></li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/public_api/#stable-but-optional-extras","level":2,"title":"Stable, but optional extras","text":"<p>These APIs are part of the public surface, but require extra dependencies.</p> <ul> <li>Ingestion helpers: <code>consist.ingest</code> (install with <code>consist[ingest]</code>)</li> <li><code>consist.integrations.containers</code> (container execution + caching; requires Docker or Singularity)</li> <li><code>consist.integrations.dlt_loader</code> (low-level ingestion integration; requires <code>consist[ingest]</code>)</li> </ul>","path":["API Reference","Public API (v0.1)"],"tags":[]},{"location":"api/run/","level":1,"title":"Run","text":"<p>               Bases: <code>SQLModel</code></p> <p>Primary run table in the Consist database.</p> <p>Each run captures execution metadata (status, timing, identity hashes, tags) and links to artifacts through <code>run_artifact_link</code>. Full configuration details live in the JSON run snapshot on disk, while the database stores hashes and a queryable subset of metadata for caching and discovery.</p> <p>Attributes:     id (str): A unique identifier for the run, often combining model name, year, and a UUID.     parent_run_id (Optional[str]): The ID of the parent run, if this is a nested execution.     status (str): The current state of the run (e.g., \"running\", \"completed\", \"failed\").     model_name (str): The name of the model or workflow being executed.     description (Optional[str]): Human-readable description of the run's purpose or outcome.     year (Optional[int]): The simulation or data year, if applicable.     iteration (Optional[int]): The iteration number, if applicable.     tags (List[str]): A list of string labels for categorization and filtering (e.g., [\"production\", \"urbansim\"]).     config_hash (Optional[str]): A hash of the run's configuration, used for caching.     git_hash (Optional[str]): The Git commit hash of the code version used for the run.     meta (Dict[str, Any]): A flexible dictionary for storing arbitrary metadata (e.g., hostname).     started_at (datetime): The timestamp when the run execution began.     ended_at (Optional[datetime]): The timestamp when the run execution completed or failed.     created_at (datetime): The timestamp when the run record was created.     updated_at (datetime): The timestamp when the run record was last updated.</p>","path":["API Reference","Run"],"tags":[]},{"location":"api/run/#consist.models.run.Run.duration_seconds","level":2,"title":"<code>duration_seconds</code>  <code>property</code>","text":"<p>Calculate the duration of the run in seconds.</p> <p>Returns the elapsed time between <code>started_at</code> and <code>ended_at</code> if both are set, otherwise returns None (e.g., if the run is still in progress).</p> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>The duration in seconds, or None if the run hasn't ended.</p>","path":["API Reference","Run"],"tags":[]},{"location":"api/tracker/","level":1,"title":"Tracker","text":"<p>The central orchestrator for Consist, managing the lifecycle of a Run and its associated Artifacts.</p> <p>The Tracker is responsible for:</p> <ol> <li> <p>Initiating and managing the state of individual \"Runs\" (e.g., model executions, data processing steps).</p> </li> <li> <p>Logging \"Artifacts\" (input files, output data, etc.) and their relationships to runs.</p> </li> <li> <p>Implementing a dual-write mechanism, logging provenance to both human-readable JSON files (<code>consist.json</code>)     and an analytical DuckDB database (<code>provenance.duckdb</code>).</p> </li> <li> <p>Providing path virtualization to make runs portable across different environments,     as described in the \"Path Resolution &amp; Mounts\" architectural section.</p> </li> <li> <p>Facilitating smart caching based on a Merkle DAG strategy, enabling \"run forking\" and \"hydration\"     of previously computed results.</p> </li> </ol>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.last_run","level":2,"title":"<code>last_run</code>  <code>property</code>","text":"<p>Return the most recent run record observed by this tracker.</p> <p>Returns:</p> Type Description <code>Optional[ConsistRecord]</code> <p>The last completed/failed run record for this tracker instance, or <code>None</code> if no run has executed yet.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.is_cached","level":2,"title":"<code>is_cached</code>  <code>property</code>","text":"<p>Whether the currently active run is a cache hit.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current <code>start_run</code>/<code>run</code>/<code>trace</code> execution is reusing a cached run. Returns <code>False</code> if no run is active.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.engine","level":2,"title":"<code>engine</code>  <code>property</code>","text":"<p>Return the SQLAlchemy engine used by this tracker.</p> <p>Use this for advanced, low-level database access when the higher-level tracker/query helpers are insufficient.</p> <p>Returns:</p> Type Description <code>Optional[Engine]</code> <p>The SQLAlchemy engine if a database is configured, otherwise <code>None</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.begin_run","level":2,"title":"<code>begin_run(run_id, model, config=None, inputs=None, tags=None, description=None, cache_mode='reuse', *, artifact_dir=None, allow_external_paths=None, facet=None, facet_from=None, hash_inputs=None, facet_schema_version=None, facet_index=True, **kwargs)</code>","text":"<p>Start a run imperatively (without context manager).</p> <p>Use this when run start and end are in separate methods, or when integrating with frameworks that have their own lifecycle management. Returns the Run object. Call end_run() when complete.</p> <p>This provides an alternative to the context manager pattern when you need more control over the run lifecycle, such as in external model integrations where start_model_run() and complete_model_run() are separate method calls.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>A unique identifier for the current run.</p> required <code>model</code> <code>str</code> <p>A descriptive name for the model or process being executed (non-empty, length-limited).</p> required <code>config</code> <code>Union[Dict[str, Any], BaseModel, None]</code> <p>Configuration parameters for this run. Keys must be strings; extremely large string values are rejected.</p> <code>None</code> <code>inputs</code> <code>Optional[list[ArtifactRef]]</code> <p>A list of input paths (str/Path) or Artifact references.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>A list of string labels for categorization and filtering (non-empty, length-limited).</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>A human-readable description of the run's purpose.</p> <code>None</code> <code>cache_mode</code> <code>str</code> <p>Strategy for caching: \"reuse\", \"overwrite\", or \"readonly\".</p> <code>\"reuse\"</code> <code>artifact_dir</code> <code>Optional[Union[str, Path]]</code> <p>Override the per-run artifact directory. Relative paths are resolved under <code>&lt;run_dir&gt;/outputs</code>. Absolute paths must remain within <code>run_dir</code> unless allow_external_paths is enabled.</p> <code>None</code> <code>allow_external_paths</code> <code>Optional[bool]</code> <p>Allow artifact_dir and cached-output materialization outside <code>run_dir</code>. Defaults to the Tracker setting when unset.</p> <code>None</code> <code>facet</code> <code>Optional[FacetLike]</code> <p>Optional small, queryable configuration facet to persist alongside the run. This is distinct from <code>config</code> (which is hashed and stored in the JSON snapshot).</p> <code>None</code> <code>facet_from</code> <code>Optional[List[str]]</code> <p>List of config keys to extract into the facet. Extracted values are merged with any explicit <code>facet</code>, with explicit keys taking precedence.</p> <code>None</code> <code>hash_inputs</code> <code>HashInputs</code> <p>Extra inputs to include in the run identity hash without logging them as run inputs/outputs. Useful for config bundles or auxiliary files. Each entry is either a path (str/Path) or a named tuple <code>(name, path)</code>.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Optional schema version tag for the persisted facet.</p> <code>None</code> <code>facet_index</code> <code>bool</code> <p>Whether to flatten and index facet keys/values for DB querying.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata. Special keywords <code>year</code> and <code>iteration</code> can be used. Metadata keys/values are validated and size-limited; use CONSIST_MAX_METADATA_ITEMS/KEY_LENGTH/VALUE_LENGTH to override.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Run</code> <p>The Run object representing the started run.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is already an active run.</p> Example <pre><code>run = tracker.begin_run(\"run_001\", \"urbansim\", config={...})\ntry:\n    tracker.log_artifact(input_file, direction=\"input\")\n    # ... do work ...\n    tracker.log_artifact(output_file, direction=\"output\")\n    tracker.end_run(\"completed\")\nexcept Exception as e:\n    tracker.end_run(\"failed\", error=e)\n    raise\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.start_run","level":2,"title":"<code>start_run(run_id, model, **kwargs)</code>","text":"<p>Context manager to initiate and manage the lifecycle of a Consist run.</p> <p>This is the primary entry point for defining a reproducible and observable unit of work. It wraps the imperative <code>begin_run()</code>/<code>end_run()</code> methods to provide automatic cleanup and exception handling.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>A unique identifier for the current run.</p> required <code>model</code> <code>str</code> <p>A descriptive name for the model or process being executed.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments forwarded to <code>begin_run()</code>, including commonly used keys:</p> <ul> <li><code>config</code>: Union[Dict[str, Any], BaseModel, None]</li> <li><code>inputs</code>: Optional[list[ArtifactRef]]</li> <li><code>tags</code>: Optional[List[str]]</li> <li><code>description</code>: Optional[str]</li> <li><code>cache_mode</code>: str (\"reuse\", \"overwrite\", \"readonly\")</li> <li><code>facet</code>, <code>facet_from</code>, <code>hash_inputs</code>, <code>facet_schema_version</code>, <code>facet_index</code></li> <li><code>year</code>, <code>iteration</code></li> </ul> <code>{}</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The current <code>Tracker</code> instance for use within the <code>with</code> block.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any exception raised within the <code>with</code> block will be caught, the run marked as \"failed\", and then re-raised after cleanup.</p> See Also <p>begin_run : Imperative alternative for starting runs. end_run : Imperative alternative for ending runs.</p> Example <p>with tracker.start_run(\"run_1\", \"my_model\", config={\"p\": 1}): ...     tracker.log_artifact(\"data.csv\", \"input\") ...     # ... execution ... ...     tracker.log_artifact(\"results.parquet\", \"output\")</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.run","level":2,"title":"<code>run(fn=None, name=None, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', load_inputs=None, executor='python', container=None, runtime_kwargs=None, inject_context=False, output_mismatch='warn', output_missing='warn')</code>","text":"<p>Execute a function-shaped run with caching and output handling.</p> <p>This method executes a callable (or container) with automatic provenance tracking, intelligent caching based on code+config+inputs, and artifact logging.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Optional[Callable]</code> <p>The function to execute. Required for executor='python'. Can be None for executor='container'.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Human-readable name for the run. Defaults to function name if not provided.</p> <code>None</code> <code>run_id</code> <code>Optional[str]</code> <p>Unique identifier for this run. Auto-generated if not provided.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model/component name for categorizing runs. Defaults to the run name.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Human-readable description of the run.</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration parameters. Becomes part of the cache signature. Can be a dict or Pydantic model.</p> <code>None</code> <code>config_plan</code> <code>Optional[ConfigPlan]</code> <p>Precomputed config plan (e.g., from ActivitySim adapter). The plan's identity hash is folded into the run config hash and its artifacts/ingestables are applied on cache miss.</p> <code>None</code> <code>config_plan_ingest</code> <code>bool</code> <p>Whether to ingest tables from the config plan.</p> <code>True</code> <code>config_plan_profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas for the config plan.</p> <code>False</code> <code>inputs</code> <code>Optional[Mapping[str, ArtifactRef] | Iterable[ArtifactRef]]</code> <p>Input files or artifacts. - Dict: Maps names to paths/Artifacts. Auto-loads into function parameters (default load_inputs=True). - List/Iterable: Hashed for cache key but not auto-loaded (use load_inputs=False).</p> <code>None</code> <code>input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>optional_input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>depends_on</code> <code>Optional[List[ArtifactRef]]</code> <p>Additional file paths or artifacts to hash for the cache signature (e.g., config files).</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Labels for filtering and organizing runs (e.g., [\"production\", \"baseline\"]).</p> <code>None</code> <code>facet</code> <code>Optional[FacetLike]</code> <p>Queryable metadata facets (small config values) logged to the run.</p> <code>None</code> <code>facet_from</code> <code>Optional[List[str]]</code> <p>List of config keys to extract and log as facets.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Schema version for facet compatibility tracking.</p> <code>None</code> <code>facet_index</code> <code>Optional[bool]</code> <p>Whether to index facets for faster queries.</p> <code>None</code> <code>hash_inputs</code> <code>Optional[HashInputs]</code> <p>Strategy for hashing inputs: \"fast\" (mtime), \"full\" (content), or None (auto-detect).</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Year metadata (for multi-year simulations). Included in provenance.</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Iteration count (for iterative workflows). Included in provenance.</p> <code>None</code> <code>parent_run_id</code> <code>Optional[str]</code> <p>Parent run ID (for nested runs in scenarios).</p> <code>None</code> <code>outputs</code> <code>Optional[List[str]]</code> <p>Names of output artifacts to log (for executor='python' with auto-loaded DataFrames). Maps artifact key to ingested table name.</p> <code>None</code> <code>output_paths</code> <code>Optional[Mapping[str, ArtifactRef]]</code> <p>Output file paths to log. Dict maps artifact keys to host paths or Artifact refs.</p> <code>None</code> <code>capture_dir</code> <code>Optional[Path]</code> <p>Directory to scan for outputs (legacy tools that write to specific dirs).</p> <code>None</code> <code>capture_pattern</code> <code>str</code> <p>Glob pattern for capturing outputs (used with capture_dir).</p> <code>\"*\"</code> <code>cache_mode</code> <code>str</code> <p>Cache behavior: \"reuse\" (return cache hit), \"overwrite\" (always re-execute), or \"skip_check\".</p> <code>\"reuse\"</code> <code>cache_hydration</code> <code>Optional[str]</code> <p>Materialization strategy for cache hits: - \"outputs-requested\": Copy only output_paths to disk - \"outputs-all\": Copy all cached outputs to run_artifact_dir - \"inputs-missing\": Backfill missing inputs from prior runs before executing</p> <code>None</code> <code>validate_cached_outputs</code> <code>str</code> <p>Validation for cached outputs: \"lazy\" (check if files exist), \"strict\", or \"none\".</p> <code>\"lazy\"</code> <code>load_inputs</code> <code>Optional[bool]</code> <p>Whether to auto-load input artifacts into function parameters. Defaults to True if inputs is a dict, False if a list.</p> <code>None</code> <code>executor</code> <code>str</code> <p>Execution backend: \"python\" (call fn directly) or \"container\" (use Docker/Singularity).</p> <code>\"python\"</code> <code>container</code> <code>Optional[Mapping[str, Any]]</code> <p>Container spec (required if executor='container'). Must contain 'image' and 'command'.</p> <code>None</code> <code>runtime_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional kwargs to pass to fn at runtime (merged with auto-loaded inputs). These values are not part of the cache signature; use them for handles or runtime-only dependencies. Consider <code>consist.require_runtime_kwargs</code> to enforce required keys.</p> <code>None</code> <code>inject_context</code> <code>bool | str</code> <p>If True or a parameter name, inject a RunContext as that parameter (for file I/O, output logging).</p> <code>False</code> <code>output_mismatch</code> <code>str</code> <p>Behavior when output count doesn't match: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <code>output_missing</code> <code>str</code> <p>Behavior when expected outputs are missing: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <p>Returns:</p> Type Description <code>RunResult</code> <p>Contains: - <code>outputs</code>: Dict[str, Artifact] of logged output artifacts - <code>cache_hit</code>: bool indicating if this was a cache hit - <code>run_id</code>: The run's unique identifier</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fn is None (for executor='python'), or if container/output_paths not provided for executor='container'.</p> <code>RuntimeError</code> <p>If the function execution fails or container execution returns non-zero code.</p> <p>Examples:</p> <p>Simple data processing:</p> <pre><code>&gt;&gt;&gt; def clean_data(raw: pd.DataFrame) -&gt; pd.DataFrame:\n...     return raw[raw['value'] &gt; 0.5]\n&gt;&gt;&gt;\n&gt;&gt;&gt; result = tracker.run(\n...     fn=clean_data,\n...     inputs={\"raw\": Path(\"raw.csv\")},\n...     outputs=[\"cleaned\"],\n... )\n</code></pre> <p>With config for cache distinction:</p> <pre><code>&gt;&gt;&gt; result = tracker.run(\n...     fn=clean_data,\n...     inputs={\"raw\": Path(\"raw.csv\")},\n...     config={\"threshold\": 0.5},\n...     outputs=[\"cleaned\"],\n... )\n</code></pre> See Also <p>start_run : Manual run context management (more control) trace : Context manager alternative (always executes, even on cache hit)</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.trace","level":2,"title":"<code>trace(name, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', output_mismatch='warn', output_missing='warn')</code>","text":"<p>Context manager for inline tracing of a run with inline execution.</p> <p>This context manager allows you to define a run directly within a <code>with</code> block, with the Python code inside executing every time (even on cache hits). This differs from <code>tracker.run()</code>, which skips execution on cache hits.</p> <p>Use <code>trace()</code> when you need inline control: for data loading, file I/O, or integrations that require code execution regardless of cache state.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable name for the run. Also defaults the model name if not provided.</p> required <code>run_id</code> <code>Optional[str]</code> <p>Unique identifier for this run. Auto-generated if not provided.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model/component name for categorizing runs. Defaults to the run name.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Human-readable description of the run.</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration parameters. Becomes part of the cache signature. Can be a dict or Pydantic model.</p> <code>None</code> <code>config_plan</code> <code>Optional[ConfigPlan]</code> <p>Precomputed config plan (e.g., from ActivitySim adapter). The plan's identity hash is folded into the run config hash and its artifacts/ingestables are applied on cache miss.</p> <code>None</code> <code>config_plan_ingest</code> <code>bool</code> <p>Whether to ingest tables from the config plan.</p> <code>True</code> <code>config_plan_profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas for the config plan.</p> <code>False</code> <code>inputs</code> <code>Optional[Mapping[str, ArtifactRef] | Iterable[ArtifactRef]]</code> <p>Input files or artifacts. - Dict: Maps names to paths/Artifacts. Logged as inputs but not auto-loaded. - List/Iterable: Hashed for cache key but not auto-loaded.</p> <code>None</code> <code>input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>optional_input_keys</code> <code>Optional[Iterable[str] | str]</code> <p>Deprecated. Use <code>inputs</code> mapping instead.</p> <code>None</code> <code>depends_on</code> <code>Optional[List[ArtifactRef]]</code> <p>Additional file paths or artifacts to hash for the cache signature (e.g., config files).</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Labels for filtering and organizing runs (e.g., [\"production\", \"baseline\"]).</p> <code>None</code> <code>facet</code> <code>Optional[FacetLike]</code> <p>Queryable metadata facets (small config values) logged to the run.</p> <code>None</code> <code>facet_from</code> <code>Optional[List[str]]</code> <p>List of config keys to extract and log as facets.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Schema version for facet compatibility tracking.</p> <code>None</code> <code>facet_index</code> <code>Optional[bool]</code> <p>Whether to index facets for faster queries.</p> <code>None</code> <code>hash_inputs</code> <code>Optional[HashInputs]</code> <p>Strategy for hashing inputs: \"fast\" (mtime), \"full\" (content), or None (auto-detect).</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Year metadata (for multi-year simulations). Included in provenance.</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Iteration count (for iterative workflows). Included in provenance.</p> <code>None</code> <code>parent_run_id</code> <code>Optional[str]</code> <p>Parent run ID (for nested runs in scenarios).</p> <code>None</code> <code>outputs</code> <code>Optional[List[str]]</code> <p>Names of output artifacts to log. Each item is a key name for logged outputs.</p> <code>None</code> <code>output_paths</code> <code>Optional[Mapping[str, ArtifactRef]]</code> <p>Output file paths to log. Dict maps artifact keys to host paths or Artifact refs.</p> <code>None</code> <code>capture_dir</code> <code>Optional[Path]</code> <p>Directory to scan for outputs. New/modified files are auto-logged.</p> <code>None</code> <code>capture_pattern</code> <code>str</code> <p>Glob pattern for capturing outputs (used with capture_dir).</p> <code>\"*\"</code> <code>cache_mode</code> <code>str</code> <p>Cache behavior: \"reuse\" (return cache hit), \"overwrite\" (always re-execute), or \"skip_check\".</p> <code>\"reuse\"</code> <code>cache_hydration</code> <code>Optional[str]</code> <p>Materialization strategy for cache hits: - \"outputs-requested\": Copy only output_paths to disk - \"outputs-all\": Copy all cached outputs to run_artifact_dir - \"inputs-missing\": Backfill missing inputs from prior runs before executing</p> <code>None</code> <code>validate_cached_outputs</code> <code>str</code> <p>Validation for cached outputs: \"lazy\" (check if files exist), \"strict\", or \"none\".</p> <code>\"lazy\"</code> <code>output_mismatch</code> <code>str</code> <p>Behavior when output count doesn't match: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <code>output_missing</code> <code>str</code> <p>Behavior when expected outputs are missing: \"warn\", \"error\", or \"ignore\".</p> <code>\"warn\"</code> <p>Yields:</p> Type Description <code>Tracker</code> <p>The current <code>Tracker</code> instance for use within the <code>with</code> block.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If output_mismatch or output_missing are invalid values.</p> <code>RuntimeError</code> <p>If output validation fails based on validation settings.</p> Notes <p>Unlike <code>tracker.run()</code>, the Python code inside a <code>trace()</code> block ALWAYS executes, even on cache hits. This is useful for side effects, data loading, or code that should run regardless of cache state.</p> <p>If you want to skip execution on cache hits (like <code>tracker.run()</code>), consider using <code>tracker.run()</code> with a callable instead.</p> <p>Examples:</p> <p>Simple inline tracing with file capture:</p> <pre><code>&gt;&gt;&gt; with tracker.trace(\n...     \"my_analysis\",\n...     output_paths={\"results\": \"./results.csv\"}\n... ):\n...     df = pd.read_csv(\"raw.csv\")\n...     df[\"value\"] = df[\"value\"] * 2\n...     df.to_csv(\"./results.csv\", index=False)\n</code></pre> <p>Multi-year simulation:</p> <pre><code>&gt;&gt;&gt; with tracker.scenario(\"baseline\") as sc:\n...     for year in [2020, 2030, 2040]:\n...         with sc.trace(name=\"simulate\", year=year):\n...             results = run_model(year)\n...             tracker.log_artifact(results, key=\"output\")\n</code></pre> See Also <p>run : Function-shaped alternative (skips on cache hit) scenario : Multi-step workflow grouping start_run : Imperative alternative for run lifecycle management</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.scenario","level":2,"title":"<code>scenario(name, config=None, tags=None, model='scenario', step_cache_hydration=None, coupler=None, require_outputs=None, **kwargs)</code>","text":"<p>Create a ScenarioContext to manage a grouped workflow of steps.</p> <p>This method initializes a scenario context manager that acts as a \"header\" run. It allows defining multiple steps (runs) that are automatically linked to this header run via <code>parent_run_id</code>, without manual threading.</p> <p>The scenario run is started, then immediately suspended (allowing steps to run), and finally restored and completed when the context exits.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the scenario. This will become the Run ID.</p> required <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Scenario-level configuration. Stored on the header run but NOT automatically inherited by steps.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Tags for the scenario. \"scenario_header\" is automatically appended.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model name for the header run.</p> <code>\"scenario\"</code> <code>step_cache_hydration</code> <code>Optional[str]</code> <p>Default cache hydration policy for all scenario steps unless overridden in a specific <code>scenario.trace(...)</code> or <code>scenario.run(...)</code>.</p> <code>None</code> <code>coupler</code> <code>Optional[Coupler]</code> <p>Optional Coupler instance to use for the scenario.</p> <code>None</code> <code>require_outputs</code> <code>Optional[Iterable[str]]</code> <p>Declare required outputs at scenario creation time.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata or arguments for the header run (including <code>facet_from</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>ScenarioContext</code> <p>A context manager object that provides <code>.trace()</code> and <code>.add_input()</code> methods.</p> Example <pre><code>with tracker.scenario(\"baseline\", config={\"mode\": \"test\"}) as sc:\n    sc.add_input(\"data.csv\", key=\"data\")\n    with sc.step(\"init\"):\n        ...\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.end_run","level":2,"title":"<code>end_run(status='completed', error=None)</code>","text":"<p>End the current run started with begin_run().</p> <p>This method finalizes the run, persists the final state to JSON and database, and emits lifecycle hooks. It is idempotent - calling it multiple times on an already-ended run will log a warning but not raise an error.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>str</code> <p>The final status of the run. Typically \"completed\" or \"failed\".</p> <code>\"completed\"</code> <code>error</code> <code>Optional[Exception]</code> <p>The exception that caused the failure, if status is \"failed\". The error message will be stored in the run's metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>Run</code> <p>The completed Run object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is no active run to end.</p> Example <pre><code>run = tracker.begin_run(\"run_001\", \"urbansim\")\ntry:\n    # ... do work ...\n    tracker.end_run(\"completed\")\nexcept Exception as e:\n    tracker.end_run(\"failed\", error=e)\n    raise\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.define_step","level":2,"title":"<code>define_step(**kwargs)</code>","text":"<p>Attach metadata to a function without changing execution behavior.</p> <p>This decorator lets you attach defaults such as <code>outputs</code>, <code>tags</code>, or <code>cache_mode</code> to a function. <code>Tracker.run</code> and <code>ScenarioContext.run</code> read this metadata when executing the function.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Step metadata (e.g., <code>outputs</code>, <code>tags</code>, <code>cache_mode</code>, <code>inject_context</code>) to attach to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A decorator that returns the original function with attached metadata.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.cached_artifacts","level":2,"title":"<code>cached_artifacts(direction='output')</code>","text":"<p>Returns hydrated artifacts for the active run when it is a cache hit.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>str</code> <p>\"output\" or \"input\" to filter hydrated artifacts.</p> <code>\"output\"</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping of artifact key to Artifact for the specified direction. Returns an empty dict if no cache hit or no artifacts.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.cached_output","level":2,"title":"<code>cached_output(key=None)</code>","text":"<p>Convenience to fetch a hydrated cached output artifact for the current run.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[str]</code> <p>If provided, returns the artifact with this key; otherwise returns the first available cached output.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>The cached output artifact, or None if not cached / not found.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.suspend_cache_options","level":2,"title":"<code>suspend_cache_options()</code>","text":"<p>Suspend active-run cache options and reset them to defaults.</p> <p>This is useful for helper functions that want default cache behavior without mutating the caller's options.</p> <p>Returns:</p> Type Description <code>ActiveRunCacheOptions</code> <p>The previously active cache options, for later restoration.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.restore_cache_options","level":2,"title":"<code>restore_cache_options(options)</code>","text":"<p>Restore previously suspended active-run cache options.</p> <p>This should typically be paired with a prior <code>suspend_cache_options</code> call to restore the caller's cache behavior.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>ActiveRunCacheOptions</code> <p>Cache options to restore (usually returned by <code>suspend_cache_options</code>).</p> required","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.capture_outputs","level":2,"title":"<code>capture_outputs(directory, pattern='*', recursive=False)</code>","text":"<p>A context manager to automatically capture and log new or modified files in a directory.</p> <p>This context manager is used within a <code>tracker.run</code>/<code>tracker.trace</code> call or <code>start_run</code> block to monitor a specified directory. Any files created or modified within this directory during the execution of the <code>with</code> block will be automatically logged as output artifacts of the current run.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>The path to the directory to monitor for new or modified files.</p> required <code>pattern</code> <code>str</code> <p>A glob pattern (e.g., \".csv\", \"data_.parquet\") to filter which files are captured within the specified directory. Defaults to all files.</p> <code>\"*\"</code> <code>recursive</code> <code>bool</code> <p>If True, the capture will recursively scan subdirectories within <code>directory</code>.</p> <code>False</code> <p>Yields:</p> Type Description <code>OutputCapture</code> <p>An <code>OutputCapture</code> object containing a list of <code>Artifact</code> objects that were captured and logged after the <code>with</code> block finishes.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>capture_outputs</code> is used outside of an active <code>start_run</code> context.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_meta","level":2,"title":"<code>log_meta(**kwargs)</code>","text":"<p>Updates the metadata for the current run.</p> <p>This method allows logging additional key-value pairs to the <code>meta</code> field of the currently active <code>Run</code> object. This is particularly useful for recording runtime metrics (e.g., accuracy, loss, F1-score), tags, or any other arbitrary information generated during the run's execution. The metadata is immediately flushed to both the JSON log and the database.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Arbitrary key-value pairs to merge into the <code>meta</code> dictionary of the current run. Existing keys will be updated, and new keys will be added.</p> <code>{}</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_artifact","level":2,"title":"<code>log_artifact(path, key=None, direction='output', schema=None, driver=None, table_path=None, array_path=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', profile_file_schema=None, file_schema_sample_rows=None, **meta)</code>","text":"<p>Logs an artifact (file or data reference) within the current run context.</p> <p>This method supports:</p> <ul> <li> <p>Automatic Input Discovery: If an input <code>path</code> matches a previously     logged output artifact, Consist automatically links them, building the     provenance graph. This is a key part of \"Auto-Forking\".</p> </li> <li> <p>Path Virtualization: Converts absolute file system paths to portable URIs     (e.g., <code>inputs://data.csv</code>) using configured mounts, adhering to     \"Path Resolution &amp; Mounts\".</p> </li> <li> <p>Schema Metadata Injection: Embeds schema information (if provided) into the     artifact's metadata, useful for later \"Strict Mode\" validation or introspection.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged. Passing an <code>Artifact</code> is useful for explicitly linking an already-logged artifact as an input or output in the current run.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact (e.g., \"households\"). Required if <code>path</code> is a path-like (str/Path).</p> <code>None</code> <code>direction</code> <code>str</code> <p>Specifies whether the artifact is an \"input\" or \"output\" for the current run. Defaults to \"output\".</p> <code>\"output\"</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the artifact's data. Its name will be stored in artifact metadata.</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify the driver (e.g., 'h5_table'). If None, the driver is inferred from the file extension.</p> <code>None</code> <code>table_path</code> <code>Optional[str]</code> <p>Optional table path inside a container (e.g., HDF5).</p> <code>None</code> <code>array_path</code> <code>Optional[str]</code> <p>Optional array path inside a container (e.g., Zarr group).</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>reuse_if_unchanged</code> <code>bool</code> <p>If True and logging an output, reuse a prior artifact row when the content hash matches.</p> <code>False</code> <code>reuse_scope</code> <code>(same_uri, any_uri)</code> <p>Scope for output reuse checks. \"same_uri\" restricts reuse to the same URI, while \"any_uri\" allows reuse across different URIs with the same hash.</p> <code>\"same_uri\"</code> <code>profile_file_schema</code> <code>bool</code> <p>If True, profile a lightweight schema for file-based tabular artifacts. Use \"if_changed\" to skip profiling when a matching content hash already has a schema.</p> <code>False</code> <code>file_schema_sample_rows</code> <code>Optional[int]</code> <p>Maximum rows to sample when profiling file-based schemas.</p> <code>None</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's flexible <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If <code>key</code> is not provided when <code>path</code> is a path-like (str/Path).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_artifacts","level":2,"title":"<code>log_artifacts(outputs, direction='output', driver=None, metadata_by_key=None, reuse_if_unchanged=False, reuse_scope='same_uri', **shared_meta)</code>","text":"<p>Log multiple artifacts in a single call for efficiency.</p> <p>This is a convenience method for bulk artifact logging, particularly useful when a model produces many output files or when registering multiple inputs. This requires an explicit mapping so artifact keys are always deliberate.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>mapping</code> <p>Mapping of key -&gt; path/Artifact to log.</p> required <code>direction</code> <code>str</code> <p>Specifies whether the artifacts are \"input\" or \"output\" for the current run.</p> <code>\"output\"</code> <code>driver</code> <code>Optional[str]</code> <p>Explicitly specify the driver for all artifacts. If None, driver is inferred from each file's extension individually.</p> <code>None</code> <code>metadata_by_key</code> <code>Optional[Mapping[str, Dict[str, Any]]]</code> <p>Per-key metadata overrides applied on top of shared metadata.</p> <code>None</code> <code>**shared_meta</code> <code>Any</code> <p>Metadata key-value pairs to apply to ALL logged artifacts. Useful for tagging a batch of related files.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping of key -&gt; logged Artifact.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ValueError</code> <p>If metadata_by_key contains keys not present in outputs.</p> <code>TypeError</code> <p>If mapping keys are not strings.</p> Example <pre><code># Log explicit outputs\noutputs = tracker.log_artifacts(\n    {\"persons\": \"output/persons.parquet\", \"households\": \"output/households.parquet\"},\n    metadata_by_key={\"households\": {\"role\": \"primary\"}},\n    year=2030,\n)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_input","level":2,"title":"<code>log_input(path, key=None, content_hash=None, force_hash_override=False, validate_content_hash=False, **meta)</code>","text":"<p>Log an input artifact. Convenience wrapper for log_artifact(direction='input').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_output","level":2,"title":"<code>log_output(path, key=None, content_hash=None, force_hash_override=False, validate_content_hash=False, reuse_if_unchanged=False, reuse_scope='same_uri', **meta)</code>","text":"<p>Log an output artifact. Convenience wrapper for log_artifact(direction='output').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>A file path (str/Path) or an existing <code>Artifact</code> reference to be logged.</p> required <code>key</code> <code>Optional[str]</code> <p>A semantic, human-readable name for the artifact.</p> <code>None</code> <code>content_hash</code> <code>Optional[str]</code> <p>Precomputed content hash to use for the artifact instead of hashing the path on disk.</p> <code>None</code> <code>force_hash_override</code> <code>bool</code> <p>If True, overwrite an existing artifact hash when it differs from <code>content_hash</code>. By default, mismatched overrides are ignored with a warning.</p> <code>False</code> <code>validate_content_hash</code> <code>bool</code> <p>If True, verify <code>content_hash</code> against the on-disk data and raise on mismatch.</p> <code>False</code> <code>**meta</code> <code>Any</code> <p>Additional key-value pairs to store in the artifact's <code>meta</code> field.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created or updated <code>Artifact</code> object.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_dataframe","level":2,"title":"<code>log_dataframe(df, key, schema=None, direction='output', path=None, driver=None, meta=None, profile_file_schema=False, file_schema_sample_rows=1000, **to_file_kwargs)</code>","text":"<p>Serialize a DataFrame, log it as an artifact, and trigger optional ingestion.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data to persist.</p> required <code>key</code> <code>str</code> <p>Logical artifact key.</p> required <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>Schema used for ingestion, if provided.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Artifact direction relative to the run.</p> <code>\"output\"</code> <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Output path; defaults to <code>&lt;run_dir&gt;/outputs/&lt;run_subdir&gt;/&lt;key&gt;.&lt;driver&gt;</code> where <code>run_subdir</code> is derived from <code>run_subdir_fn</code> (or the default pattern).</p> <code>None</code> <code>driver</code> <code>Optional[str]</code> <p>File format driver (e.g., \"parquet\" or \"csv\").</p> <code>None</code> <code>meta</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the artifact.</p> <code>None</code> <code>profile_file_schema</code> <code>bool</code> <p>If True, profile a lightweight schema for file-based tabular artifacts.</p> <code>False</code> <code>file_schema_sample_rows</code> <code>Optional[int]</code> <p>Maximum rows to sample when profiling file-based schemas.</p> <code>1000</code> <code>**to_file_kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>pd.DataFrame.to_parquet</code> or <code>to_csv</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The artifact logged for the written dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested driver is unsupported.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load","level":2,"title":"<code>load(artifact, **kwargs)</code>","text":"<p>Load an artifact using the public API while binding this tracker context.</p> <p>This is equivalent to <code>consist.load(artifact, tracker=self, ...)</code> and uses the artifact driver to select the appropriate loader.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact to load.</p> required <code>**kwargs</code> <code>Any</code> <p>Loader-specific options forwarded to <code>consist.load</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The loaded data object (e.g., DuckDB Relation, xarray.Dataset, etc.).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.materialize","level":2,"title":"<code>materialize(artifact, destination_path, *, on_missing='warn')</code>","text":"<p>Materialize a cached artifact onto the filesystem.</p> <p>This copies bytes from the resolved artifact URI to <code>destination_path</code>. It does not perform database-backed reconstruction.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The destination path for the materialized artifact, or <code>None</code> if missing and <code>on_missing=\"warn\"</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.ingest","level":2,"title":"<code>ingest(artifact, data=None, schema=None, run=None, profile_schema=True)</code>","text":"<p>Ingests data associated with an <code>Artifact</code> into the Consist DuckDB database.</p> <p>This method is central to Consist's \"Hot Data Strategy\", where data is materialized into the database for faster query performance and easier sharing. It leverages the <code>dlt</code> (Data Load Tool) integration for efficient and robust data loading, including support for schema inference and evolution.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact object representing the data being ingested. If the artifact was logged with a schema (e.g., <code>log_artifact(path, schema=MySchema)</code>) and that schema was registered with the Tracker at initialization (e.g., <code>Tracker(..., schemas=[MySchema])</code>), it will be automatically looked up and used for ingestion.</p> required <code>data</code> <code>Optional[Union[Iterable[Dict[str, Any]], Any]]</code> <p>An iterable (e.g., list of dicts, generator) where each item represents a row of data to be ingested. If <code>data</code> is omitted, Consist attempts to stream it directly from the artifact's file URI, resolving the path. Can also be other data types that <code>dlt</code> can handle directly (e.g., Pandas DataFrame).</p> <code>None</code> <code>schema</code> <code>Optional[Type[SQLModel]]</code> <p>An optional SQLModel class that defines the expected schema for the ingested data. If provided, <code>dlt</code> will use this for strict validation and this parameter takes precedence over any auto-detected schema. If not provided, Consist will automatically look up the schema by name from schemas registered in Tracker.init (using artifact.meta[\"schema_name\"]).</p> <code>None</code> <code>run</code> <code>Optional[Run]</code> <p>If provided, tags data with this run's ID (Offline Mode). If None, uses the currently active run (Online Mode).</p> <code>None</code> <code>profile_schema</code> <code>bool</code> <p>If True, profile and persist a deduped schema record for the ingested table, writing <code>schema_id</code>/<code>schema_summary</code> (and optionally <code>schema_profile</code>) into <code>Artifact.meta</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result information from the <code>dlt</code> ingestion process.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no database is configured (<code>db_path</code> was not provided during Tracker initialization) or if <code>ingest</code> is called outside of an active run context.</p> <code>Exception</code> <p>Any exception raised by the underlying <code>dlt</code> ingestion process.</p> <p>Examples:</p> <p>Auto-detected schema workflow (register schemas at tracker init):</p> <pre><code>&gt;&gt;&gt; tracker = Tracker(..., schemas=[MyDataSchema])\n&gt;&gt;&gt; art = tracker.log_artifact(file.csv, schema=MyDataSchema)\n&gt;&gt;&gt; tracker.ingest(art, data=df)  # Automatically looks up and uses MyDataSchema\n</code></pre> <p>Cross-session workflow (schemas persist in metadata):</p> <pre><code>&gt;&gt;&gt; # Session 1:\n&gt;&gt;&gt; tracker = Tracker(..., schemas=[MyDataSchema])\n&gt;&gt;&gt; art = tracker.log_artifact(file.csv, schema=MyDataSchema)\n&gt;&gt;&gt; # Session 2:\n&gt;&gt;&gt; tracker2 = Tracker(..., schemas=[MyDataSchema])\n&gt;&gt;&gt; art2 = tracker2.get_artifact(\"mydata\")\n&gt;&gt;&gt; tracker2.ingest(art2, data=df)  # Looks up MyDataSchema by artifact's schema_name\n</code></pre> <p>Override auto-detection (explicit schema always wins):</p> <pre><code>&gt;&gt;&gt; tracker.ingest(art, data=df, schema=DifferentSchema)  # Uses DifferentSchema\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_runs","level":2,"title":"<code>find_runs(tags=None, year=None, iteration=None, model=None, status=None, parent_id=None, metadata=None, limit=100, index_by=None, name=None)</code>","text":"<p>Retrieve runs matching the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[List[str]]</code> <p>Filter runs that contain all provided tags.</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Filter by run year.</p> <code>None</code> <code>iteration</code> <code>Optional[int]</code> <p>Filter by run iteration.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by run model name.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Filter by run status (e.g., \"completed\", \"failed\").</p> <code>None</code> <code>parent_id</code> <code>Optional[str]</code> <p>Filter by scenario/header parent id.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Filter by exact matches in <code>Run.meta</code> (client-side filter).</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to return.</p> <code>100</code> <code>index_by</code> <code>Optional[Union[str, IndexBySpec]]</code> <p>If provided, returns a dict keyed by a run attribute or facet value. Supported forms: - <code>\"year\"</code> / <code>\"iteration\"</code> / any Run attribute name - <code>\"facet.&lt;key&gt;\"</code> or <code>\"facet:&lt;key&gt;\"</code> to key by a persisted facet value - <code>IndexBySpec</code> helpers like <code>index_by_field(...)</code> / <code>index_by_facet(...)</code></p> <p>Note: if multiple runs share the same key, the last one wins.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Filter by <code>Run.model_name</code>/name alias used by DatabaseManager.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[Run], Dict[Hashable, Run]]</code> <p>List of runs, or a dict keyed by <code>index_by</code> when requested.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>index_by</code> is an unsupported type.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_run","level":2,"title":"<code>find_run(**kwargs)</code>","text":"<p>Find exactly one run matching the criteria.</p> <p>This is a convenience wrapper around <code>find_runs(...)</code> that enforces uniqueness.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Filters forwarded to <code>find_runs(...)</code>. Special cases: - <code>id</code> or <code>run_id</code>: if provided, performs a direct primary-key lookup.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Run</code> <p>The matching run.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runs match, or more than one run matches.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_latest_run","level":2,"title":"<code>find_latest_run(*, parent_id=None, model=None, status=None, year=None, tags=None, metadata=None, limit=10000)</code>","text":"<p>Return the most recent run matching the filters.</p> <p>Selection priority: 1) Highest <code>iteration</code> (when present) 2) Newest <code>created_at</code> (fallback when no iteration is set)</p> <p>Parameters:</p> Name Type Description Default <code>parent_id</code> <code>Optional[str]</code> <p>Filter by scenario/parent run ID.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by model name.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Filter by run status.</p> <code>None</code> <code>year</code> <code>Optional[int]</code> <p>Filter by run year.</p> <code>None</code> <code>tags</code> <code>Optional[List[str]]</code> <p>Filter runs that contain all provided tags.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Filter by exact matches in <code>Run.meta</code> (client-side filter).</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to consider.</p> <code>10_000</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_latest_run_id","level":2,"title":"<code>get_latest_run_id(**kwargs)</code>","text":"<p>Convenience wrapper to return the latest run ID for the given filters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Filters forwarded to <code>find_latest_run</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The run ID of the latest matching run.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no runs match the provided filters.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_artifacts","level":2,"title":"<code>find_artifacts(*, creator=None, consumer=None, key=None, limit=100)</code>","text":"<p>Find artifacts by producing/consuming runs and key.</p> <p>Parameters:</p> Name Type Description Default <code>creator</code> <code>Optional[Union[str, Run]]</code> <p>Run ID (or Run) that logged the artifact as an output.</p> <code>None</code> <code>consumer</code> <code>Optional[Union[str, Run]]</code> <p>Run ID (or Run) that logged the artifact as an input.</p> <code>None</code> <code>key</code> <code>Optional[str]</code> <p>Exact artifact key to match.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of artifacts to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>list</code> <p>Matching artifact records (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifact","level":2,"title":"<code>get_artifact(key_or_id)</code>","text":"<p>Retrieves an Artifact by its semantic key or UUID.</p> <p>This method provides a flexible way to locate artifacts, first checking the in-memory context of the current run, and then querying the database for persistent records.</p> <p>Parameters:</p> Name Type Description Default <code>key_or_id</code> <code>Union[str, UUID]</code> <p>The artifact's 'key' (e.g., \"households\") or its unique UUID. When a string is provided, the most recently created artifact matching that key is returned.</p> required <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>The found <code>Artifact</code> object, or <code>None</code> if no matching artifact is found.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifacts_for_run","level":2,"title":"<code>get_artifacts_for_run(run_id)</code>","text":"<p>Retrieve inputs and outputs for a specific run, organized by key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <p>Returns:</p> Type Description <code>RunArtifacts</code> <p>Container with <code>inputs</code> and <code>outputs</code> dicts. Returns empty collections if the database is not configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run","level":2,"title":"<code>get_run(run_id)</code>","text":"<p>Retrieve a single Run by its ID from the database.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The unique identifier of the run to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The Run object if found, or <code>None</code> if missing or no database is configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_config","level":2,"title":"<code>get_run_config(run_id, *, allow_missing=False)</code>","text":"<p>Load the full config snapshot for a historical run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>allow_missing</code> <code>bool</code> <p>Return <code>None</code> if the snapshot is missing instead of raising.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>The stored config payload, or <code>None</code> if missing and <code>allow_missing</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_inputs","level":2,"title":"<code>get_run_inputs(run_id)</code>","text":"<p>Return input artifacts for a run, keyed by artifact key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Input artifacts keyed by artifact key. Returns an empty dict if the database is not configured or the run is unknown.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_outputs","level":2,"title":"<code>get_run_outputs(run_id)</code>","text":"<p>Return output artifacts for a run, keyed by artifact key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Output artifacts keyed by artifact key. Returns an empty dict if the database is not configured or the run is unknown.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifact_lineage","level":2,"title":"<code>get_artifact_lineage(artifact_key_or_id, *, max_depth=None)</code>","text":"<p>Recursively builds a lineage tree for a given artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_key_or_id</code> <code>Union[str, UUID]</code> <p>Artifact key or UUID.</p> required <code>max_depth</code> <code>Optional[int]</code> <p>Maximum depth to traverse (0 returns only the artifact). Useful for large graphs or iterative workflows.</p> <code>None</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.print_lineage","level":2,"title":"<code>print_lineage(artifact_key_or_id, *, max_depth=None, show_run_ids=False)</code>","text":"<p>Print a formatted lineage tree for an artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_key_or_id</code> <code>Union[str, UUID]</code> <p>Artifact key or UUID to print.</p> required <code>max_depth</code> <code>Optional[int]</code> <p>Maximum depth to traverse (0 prints only the artifact).</p> <code>None</code> <code>show_run_ids</code> <code>bool</code> <p>Include run IDs alongside artifact entries.</p> <code>False</code>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.history","level":2,"title":"<code>history(limit=10, tags=None)</code>","text":"<p>Return recent runs as a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of runs to include.</p> <code>10</code> <code>tags</code> <code>Optional[List[str]]</code> <p>If provided, filter runs to those containing any of the given tags.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame of recent runs (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.diff_runs","level":2,"title":"<code>diff_runs(run_id_a, run_id_b, *, namespace=None, prefix=None, keys=None, limit=10000, include_equal=False)</code>","text":"<p>Compare flattened config facets between two runs.</p> <p>Parameters:</p> Name Type Description Default <code>run_id_a</code> <code>str</code> <p>Baseline run identifier.</p> required <code>run_id_b</code> <code>str</code> <p>Comparison run identifier.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Namespace for facets. Defaults to each run's model name.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Filter keys by prefix (e.g. <code>\"inputs.\"</code>).</p> <code>None</code> <code>keys</code> <code>Optional[Iterable[str]]</code> <p>Only include specific keys when provided.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of entries to inspect per run.</p> <code>10_000</code> <code>include_equal</code> <code>bool</code> <p>If True, include keys whose values are unchanged.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with <code>namespace</code> metadata and <code>changes</code> mapping keys to values.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_facet","level":2,"title":"<code>get_config_facet(facet_id)</code>","text":"<p>Retrieve a single persisted config facet by ID.</p> <p>Parameters:</p> Name Type Description Default <code>facet_id</code> <code>str</code> <p>The facet identifier.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The facet record if present, otherwise <code>None</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_facets","level":2,"title":"<code>get_config_facets(*, namespace=None, schema_name=None, limit=100)</code>","text":"<p>List persisted config facets, optionally filtered.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Optional[str]</code> <p>Filter facets by namespace.</p> <code>None</code> <code>schema_name</code> <code>Optional[str]</code> <p>Filter facets by schema name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of facet records to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of facet records (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_config_kv","level":2,"title":"<code>get_run_config_kv(run_id, *, namespace=None, prefix=None, limit=10000)</code>","text":"<p>Retrieve flattened key/value config entries for a run.</p> <p>This is primarily used for querying and debugging indexed config facets.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Filter by namespace.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Filter keys by prefix (e.g. <code>\"inputs.\"</code>).</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of entries to return.</p> <code>10_000</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of key/value rows (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_values","level":2,"title":"<code>get_config_values(run_id, *, namespace=None, prefix=None, keys=None, limit=10000)</code>","text":"<p>Return a flattened config facet as a dict of key/value pairs.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Namespace for the facet. Defaults to the run's model name when available.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Filter keys by prefix (e.g. <code>\"inputs.\"</code>).</p> <code>None</code> <code>keys</code> <code>Optional[Iterable[str]]</code> <p>Only include specific keys when provided.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of entries to return.</p> <code>10_000</code> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping of flattened keys to typed values.</p> Notes <p>Keys are stored as flattened dotted paths. If an original key contains a literal dot, it is escaped as <code>\"\\.\"</code> in the stored key.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_config_value","level":2,"title":"<code>get_config_value(run_id, key, *, namespace=None, default=None)</code>","text":"<p>Retrieve a single config value from a flattened config facet.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>key</code> <code>str</code> <p>Flattened key to fetch.</p> required <code>namespace</code> <code>Optional[str]</code> <p>Namespace for the facet. Defaults to the run's model name when available.</p> <code>None</code> <code>default</code> <code>Any</code> <p>Value to return when the key is missing.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The typed value for the key, or <code>default</code> if missing.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_runs_by_facet_kv","level":2,"title":"<code>find_runs_by_facet_kv(*, namespace, key, value_type=None, value_str=None, value_num=None, value_bool=None, limit=100)</code>","text":"<p>Find runs by a flattened config facet key/value.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Facet namespace.</p> required <code>key</code> <code>str</code> <p>Flattened facet key.</p> required <code>value_type</code> <code>Optional[str]</code> <p>Optional discriminator for the value column (implementation dependent).</p> <code>None</code> <code>value_str</code> <code>Optional[str]</code> <p>String value to match.</p> <code>None</code> <code>value_num</code> <code>Optional[float]</code> <p>Numeric value to match.</p> <code>None</code> <code>value_bool</code> <code>Optional[bool]</code> <p>Boolean value to match.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>list</code> <p>Matching run records (empty if DB is not configured).</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.view","level":2,"title":"<code>view(model, key=None)</code>","text":"<p>Create/register a hybrid view for a given SQLModel schema.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[SQLModel]</code> <p>SQLModel schema defining the logical columns for the concept.</p> required <code>key</code> <code>Optional[str]</code> <p>Override the concept key (defaults to <code>model.__tablename__</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[SQLModel]</code> <p>The dynamic SQLModel view class exposed via <code>tracker.views</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the tracker has no database configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.create_view","level":2,"title":"<code>create_view(view_name, concept_key)</code>","text":"<p>Create a named hybrid view over a registered concept.</p> <p>This is a lower-level helper than <code>Tracker.view(...)</code>. It is useful when you want to create multiple named views over the same concept key, or when you want explicit control over the view name.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>The SQL view name to create in the database (e.g., <code>\"v_persons\"</code>).</p> required <code>concept_key</code> <code>str</code> <p>The registered concept key to materialize (typically a table/artifact key).</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Backend-specific result from <code>ViewFactory.create_hybrid_view</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load_matrix","level":2,"title":"<code>load_matrix(concept_key, variables=None, *, run_ids=None, parent_id=None, model=None, status=None)</code>","text":"<p>Convenience wrapper for loading a matrix view from tracked artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>Semantic key for the matrix artifacts.</p> required <code>variables</code> <code>Optional[List[str]]</code> <p>Variables to load from each Zarr store; defaults to all variables.</p> <code>None</code> <code>run_ids</code> <code>Optional[List[str]]</code> <p>Restrict to specific run IDs.</p> <code>None</code> <code>parent_id</code> <code>Optional[str]</code> <p>Filter by scenario/parent run ID.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by model name.</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Filter by run status.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>An <code>xarray.Dataset</code> containing the combined matrix data.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.export_schema_sqlmodel","level":2,"title":"<code>export_schema_sqlmodel(*, schema_id=None, artifact_id=None, out_path=None, table_name=None, class_name=None, abstract=True, include_system_cols=False, include_stats_comments=True, prefer_source=None)</code>","text":"<p>Export a captured artifact schema as a SQLModel stub for manual editing.</p> <p>Exactly one of <code>schema_id</code> or <code>artifact_id</code> must be provided. The generated Python source is returned and can optionally be written to <code>out_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>schema_id</code> <code>Optional[str]</code> <p>Schema identifier to export (from the schema registry). If provided, prefer_source is ignored and this specific schema is used.</p> <code>None</code> <code>artifact_id</code> <code>Optional[Union[str, UUID]]</code> <p>Artifact ID to export the associated schema. When used, the schema selection respects the prefer_source parameter.</p> <code>None</code> <code>out_path</code> <code>Optional[Path]</code> <p>If provided, write the stub to this path and return its contents.</p> <code>None</code> <code>table_name</code> <code>Optional[str]</code> <p>Override the SQL table name in the generated class.</p> <code>None</code> <code>class_name</code> <code>Optional[str]</code> <p>Override the Python class name in the generated class.</p> <code>None</code> <code>abstract</code> <code>bool</code> <p>Whether to mark the generated class as abstract.</p> <code>True</code> <code>include_system_cols</code> <code>bool</code> <p>Whether to include Consist system columns in the stub.</p> <code>False</code> <code>include_stats_comments</code> <code>bool</code> <p>Whether to include column-level stats as comments.</p> <code>True</code> <code>prefer_source</code> <code>(file, duckdb)</code> <p>Preference hint for when user_provided schema does not exist. This is useful when an artifact has both a file profile (pandas dtypes) and a duckdb profile (post-ingestion types). Ignored if schema_id is provided directly.</p> <p>IMPORTANT: User-provided schemas (manually curated with FK constraints, indexes, etc.) are ALWAYS preferred if they exist. This parameter does not override user_provided schemas.</p> <ul> <li>\"file\": Prefer the original file schema (CSV/Parquet with pandas dtypes)</li> <li>\"duckdb\": Prefer the post-ingestion schema from the DuckDB table</li> <li>None (default): Prefer file, as it preserves richer type information   (e.g., pandas category)</li> </ul> <code>\"file\"</code> <p>Returns:</p> Type Description <code>str</code> <p>The rendered SQLModel stub source.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the tracker has no database configured or if the selector is invalid.</p> <code>KeyError</code> <p>If no schema is found for the provided selector.</p> <p>Examples:</p> <p>Export file schema (original raw file dtypes):</p> <pre><code>&gt;&gt;&gt; tracker.export_schema_sqlmodel(artifact_id=art.id)\n</code></pre> <p>Export ingested table schema (after dlt normalization):</p> <pre><code>&gt;&gt;&gt; tracker.export_schema_sqlmodel(artifact_id=art.id, prefer_source=\"duckdb\")\n</code></pre> <p>Export a specific schema directly by ID:</p> <pre><code>&gt;&gt;&gt; tracker.export_schema_sqlmodel(schema_id=\"abc123xyz\")\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.netcdf_metadata","level":2,"title":"<code>netcdf_metadata(concept_key)</code>","text":"<p>Access NetCDF metadata views for a given artifact key.</p> <p>This provides convenient access to query and explore NetCDF file structures stored in Consist's metadata catalog.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key identifying the NetCDF artifact.</p> required <p>Returns:</p> Type Description <code>NetCdfMetadataView</code> <p>A view object with methods to explore variables, dimensions, and attributes.</p> Example <pre><code>view = tracker.netcdf_metadata(\"climate\")\nvariables = view.get_variables(year=2024)\nprint(view.summary(\"climate\"))\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.openmatrix_metadata","level":2,"title":"<code>openmatrix_metadata(concept_key)</code>","text":"<p>Access OpenMatrix metadata views for a given artifact key.</p> <p>This provides convenient access to query and explore OpenMatrix file structures stored in Consist's metadata catalog.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key identifying the OpenMatrix artifact.</p> required <p>Returns:</p> Type Description <code>OpenMatrixMetadataView</code> <p>A view object with methods to explore matrices, zones, and attributes.</p> Example <pre><code>view = tracker.openmatrix_metadata(\"demand\")\nmatrices = view.get_matrices(year=2024)\nzones = view.get_zone_counts()\nprint(view.summary(\"demand\"))\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.spatial_metadata","level":2,"title":"<code>spatial_metadata(concept_key)</code>","text":"<p>Access spatial metadata views for a given artifact key.</p> <p>Parameters:</p> Name Type Description Default <code>concept_key</code> <code>str</code> <p>The semantic key identifying the spatial artifact.</p> required <p>Returns:</p> Type Description <code>SpatialMetadataView</code> <p>A view object with methods to explore spatial metadata.</p> Example <pre><code>view = tracker.spatial_metadata(\"parcels\")\nbounds = view.get_bounds(\"parcels\")\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.canonicalize_config","level":2,"title":"<code>canonicalize_config(adapter, config_dirs, *, run=None, run_id=None, strict=False, ingest=True, profile_schema=False, options=None)</code>","text":"<p>Canonicalize a model-specific config directory and ingest queryable slices.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>ConfigAdapter</code> <p>Adapter implementation for the model (e.g., ActivitySim).</p> required <code>config_dirs</code> <code>Iterable[Union[str, Path]]</code> <p>Ordered config directories to canonicalize.</p> required <code>run</code> <code>Optional[Run]</code> <p>Run context to attach to; defaults to the active run.</p> <code>None</code> <code>run_id</code> <code>Optional[str]</code> <p>Run identifier; must match the active run when provided.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>If True, adapter should error on missing references.</p> <code>False</code> <code>ingest</code> <code>bool</code> <p>Whether to ingest any queryable tables produced by the adapter.</p> <code>True</code> <code>profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas.</p> <code>False</code> <code>options</code> <code>Optional[ConfigAdapterOptions]</code> <p>Shared adapter options that override strict/ingest defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigContribution</code> <p>Structured summary of logged artifacts and ingestables.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.prepare_config","level":2,"title":"<code>prepare_config(adapter, config_dirs, *, strict=False, options=None, validate_only=False, facet_spec=None, facet_schema_name=None, facet_schema_version=None, facet_index=None)</code>","text":"<p>Prepare a config plan without logging artifacts or ingesting data.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>ConfigAdapter</code> <p>Adapter implementation for the model (e.g., ActivitySim).</p> required <code>config_dirs</code> <code>Iterable[Union[str, Path]]</code> <p>Ordered config directories to canonicalize.</p> required <code>strict</code> <code>bool</code> <p>If True, adapter should error on missing references.</p> <code>False</code> <code>options</code> <code>Optional[ConfigAdapterOptions]</code> <p>Shared adapter options that override strict defaults.</p> <code>None</code> <code>validate_only</code> <code>bool</code> <p>If True, validate ingestables without logging or ingesting.</p> <code>False</code> <code>facet_spec</code> <code>Optional[Dict[str, Any]]</code> <p>Adapter-specific facet extraction spec.</p> <code>None</code> <code>facet_schema_name</code> <code>Optional[str]</code> <p>Optional facet schema name for persistence.</p> <code>None</code> <code>facet_schema_version</code> <code>Optional[Union[str, int]]</code> <p>Optional facet schema version for persistence.</p> <code>None</code> <code>facet_index</code> <code>Optional[bool]</code> <p>Optional flag controlling KV facet indexing.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigPlan</code> <p>Pre-run config plan containing artifacts and ingestables.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.apply_config_plan","level":2,"title":"<code>apply_config_plan(plan, *, run=None, ingest=True, profile_schema=False, adapter=None, options=None)</code>","text":"<p>Apply a pre-run config plan to the active run.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>ConfigPlan</code> <p>Plan produced by <code>prepare_config</code>.</p> required <code>run</code> <code>Optional[Run]</code> <p>Run context to attach to; defaults to the active run.</p> <code>None</code> <code>ingest</code> <code>bool</code> <p>Whether to ingest any queryable tables produced by the adapter.</p> <code>True</code> <code>profile_schema</code> <code>bool</code> <p>Whether to profile ingested schemas.</p> <code>False</code> <code>adapter</code> <code>Optional[ConfigAdapter]</code> <p>Adapter instance used to create run-scoped artifacts, if needed.</p> <code>None</code> <code>options</code> <code>Optional[ConfigAdapterOptions]</code> <p>Shared adapter options that override ingest defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>ConfigContribution</code> <p>Structured summary of logged artifacts and ingestables.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.identity_from_config_plan","level":2,"title":"<code>identity_from_config_plan(plan)</code>","text":"<p>Return the identity hash derived from a config plan.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>ConfigPlan</code> <p>Config plan produced by <code>prepare_config</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Stable hash representing the canonical config content.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_h5_container","level":2,"title":"<code>log_h5_container(path, key=None, direction='output', discover_tables=True, table_filter=None, hash_tables='if_unchanged', table_hash_chunk_rows=None, **meta)</code>","text":"<p>Log an HDF5 file and optionally discover its internal tables.</p> <p>This method provides first-class HDF5 container support, automatically discovering and logging internal tables as child artifacts. This is particularly useful for model pipelines that use HDF5 files containing multiple datasets or tables.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the HDF5 file.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the container. If not provided, uses the file stem.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether this is an \"input\" or \"output\" artifact.</p> <code>\"output\"</code> <code>discover_tables</code> <code>bool</code> <p>If True, scan the file and create child artifacts for each table/dataset.</p> <code>True</code> <code>table_filter</code> <code>Optional[Union[Callable[[str], bool], List[str]]]</code> <p>Filter which tables to log. Can be: - A callable that takes a table name and returns True to include - A list of table names to include (exact match) If None, all tables are included.</p> <code>None</code> <code>hash_tables</code> <code>Literal['always', 'if_unchanged', 'never']</code> <p>Whether to compute content hashes for discovered tables. \"if_unchanged\" skips hashing when a table appears unchanged based on lightweight checks.</p> <code>\"if_unchanged\"</code> <code>table_hash_chunk_rows</code> <code>Optional[int]</code> <p>Row chunk size to use when hashing large tables.</p> <code>None</code> <code>**meta</code> <code>Any</code> <p>Additional metadata for the container artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Artifact, List[Artifact]]</code> <p>A tuple of (container_artifact, list_of_table_artifacts).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ImportError</code> <p>If h5py is not installed and discover_tables is True.</p> Example <pre><code># Log HDF5 file with auto-discovery of all tables\ncontainer, tables = tracker.log_h5_container(\"data.h5\", key=\"urbansim_data\")\nprint(f\"Logged {len(tables)} tables from container\")\n\n# Filter tables by callable\ncontainer, tables = tracker.log_h5_container(\n    \"data.h5\",\n    key=\"urbansim_data\",\n    table_filter=lambda name: name.startswith(\"/2025/\")\n)\n\n# Filter tables by list of names\ncontainer, tables = tracker.log_h5_container(\n    \"data.h5\",\n    key=\"urbansim_data\",\n    table_filter=[\"households\", \"persons\", \"buildings\"]\n)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_h5_table","level":2,"title":"<code>log_h5_table(path, *, table_path, key=None, direction='output', parent=None, hash_table=True, table_hash_chunk_rows=None, profile_file_schema=False, file_schema_sample_rows=None, **meta)</code>","text":"<p>Log a single HDF5 table as an artifact without scanning the container.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the HDF5 file on disk.</p> required <code>table_path</code> <code>str</code> <p>Internal table/dataset path inside the HDF5 container.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic key for the table artifact. Defaults to the dataset name.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether the table is an \"input\" or \"output\".</p> <code>\"output\"</code> <code>parent</code> <code>Optional[Artifact]</code> <p>Optional parent container artifact to link this table to.</p> <code>None</code> <code>hash_table</code> <code>bool</code> <p>Whether to compute a content hash for the table.</p> <code>True</code> <code>table_hash_chunk_rows</code> <code>Optional[int]</code> <p>Chunk size for hashing large tables.</p> <code>None</code> <code>profile_file_schema</code> <code>bool | Literal['if_changed']</code> <p>Whether to profile table schema and store it as metadata.</p> <code>False</code> <code>file_schema_sample_rows</code> <code>Optional[int]</code> <p>Number of rows to sample when profiling schema.</p> <code>None</code> <code>**meta</code> <code>Any</code> <p>Additional metadata to store on the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The created table artifact.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_netcdf_file","level":2,"title":"<code>log_netcdf_file(path, key=None, direction='output', **meta)</code>","text":"<p>Log a NetCDF file as an artifact with metadata extraction.</p> <p>This method provides convenient logging for NetCDF files, automatically detecting the driver and storing structural metadata about variables, dimensions, and coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the NetCDF file.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact. If not provided, uses the file stem.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether this is an \"input\" or \"output\" artifact.</p> <code>\"output\"</code> <code>**meta</code> <code>Any</code> <p>Additional metadata for the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged artifact with metadata extracted from the NetCDF structure.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ImportError</code> <p>If xarray is not installed.</p> Example <pre><code># Log NetCDF file\nart = tracker.log_netcdf_file(\"climate_data.nc\", key=\"temperature\")\n# Optionally ingest metadata\ntracker.ingest(art)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.log_openmatrix_file","level":2,"title":"<code>log_openmatrix_file(path, key=None, direction='output', **meta)</code>","text":"<p>Log an OpenMatrix (OMX) file as an artifact with metadata extraction.</p> <p>This method provides convenient logging for OpenMatrix files, automatically detecting the driver and storing structural metadata about matrices, dimensions, and attributes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the OpenMatrix file.</p> required <code>key</code> <code>Optional[str]</code> <p>Semantic name for the artifact. If not provided, uses the file stem.</p> <code>None</code> <code>direction</code> <code>str</code> <p>Whether this is an \"input\" or \"output\" artifact.</p> <code>\"output\"</code> <code>**meta</code> <code>Any</code> <p>Additional metadata for the artifact.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged artifact with metadata extracted from the OpenMatrix structure.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called outside an active run context.</p> <code>ImportError</code> <p>If neither h5py nor openmatrix is installed.</p> Example <pre><code># Log OpenMatrix file (e.g., ActivitySim travel demand)\nart = tracker.log_openmatrix_file(\"demand.omx\", key=\"travel_demand\")\n# Optionally ingest metadata\ntracker.ingest(art)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.set_run_subdir_fn","level":2,"title":"<code>set_run_subdir_fn(fn)</code>","text":"<p>Set a callable that returns the per-run artifact subdirectory name.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Optional[Callable[[Run], str]]</code> <p>Callable that accepts a <code>Run</code> and returns a relative directory name. Set to <code>None</code> to disable the custom resolver.</p> required","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.run_artifact_dir","level":2,"title":"<code>run_artifact_dir(run=None)</code>","text":"<p>Resolve the run-specific artifact directory for the active run.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>Optional[Run]</code> <p>Run to resolve the directory for. Defaults to the current run if active.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Directory under <code>run_dir</code> where run artifacts should be written by default. Absolute artifact_dir values outside <code>run_dir</code> are only allowed when allow_external_paths is enabled.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.resolve_uri","level":2,"title":"<code>resolve_uri(uri)</code>","text":"<p>** Delegates to FileSystemManager. **</p> <p>Converts a portable Consist URI back into an absolute file system path.</p> <p>This is the inverse operation of <code>_virtualize_path</code>, crucial for \"Path Resolution &amp; Mounts\". It uses the configured <code>mounts</code> and the <code>run_dir</code> to reconstruct the local absolute path to an artifact, making runs portable across different environments.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The portable URI (e.g., \"inputs://file.csv\", \"./output/data.parquet\") to resolve.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute file system path corresponding to the given URI. If the URI cannot be fully resolved (e.g., scheme not mounted), it returns the most resolved path or the original URI after attempting to make it absolute. Mounted URIs are validated to prevent path traversal outside the mount root.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.run_query","level":2,"title":"<code>run_query(query)</code>","text":"<p>Execute a SQLModel/SQLAlchemy query via the tracker engine.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Executable</code> <p>Query object (<code>select</code>, <code>text</code>, etc.).</p> required <p>Returns:</p> Type Description <code>list</code> <p>Results of the executed query.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no database is configured for this tracker.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_record","level":2,"title":"<code>get_run_record(run_id, *, allow_missing=False)</code>","text":"<p>Load the full run record snapshot from disk.</p> <p>This reads the JSON snapshot produced at run time (<code>consist_runs/&lt;id&gt;.json</code>) and returns the parsed <code>ConsistRecord</code>.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>allow_missing</code> <code>bool</code> <p>Return <code>None</code> if the snapshot file is missing or unreadable instead of raising.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[ConsistRecord]</code> <p>The parsed run record, or <code>None</code> if missing and <code>allow_missing</code>.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.resolve_historical_path","level":2,"title":"<code>resolve_historical_path(artifact, run)</code>","text":"<p>Resolve the on-disk path for an artifact from a prior run.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The artifact whose historical location should be resolved.</p> required <code>run</code> <code>Run</code> <p>The run that originally produced/consumed the artifact.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The resolved filesystem path for the artifact in its original run workspace.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load_input_bundle","level":2,"title":"<code>load_input_bundle(run_id)</code>","text":"<p>Load a set of input artifacts from a prior \"bundle\" run by run_id.</p> <p>This is a convenience helper for shared DuckDB bundles where a dedicated run logs all required inputs as outputs. The returned dict can be passed directly to <code>inputs=[...]</code> on a new run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The run id that logged the bundle outputs.</p> required <p>Returns:</p> Type Description <code>dict[str, Artifact]</code> <p>Mapping of artifact key -&gt; Artifact from the bundle run.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the run does not exist or has no output artifacts.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_artifact_by_uri","level":2,"title":"<code>get_artifact_by_uri(uri, *, table_path=None, array_path=None)</code>","text":"<p>Find an artifact by its URI.</p> <p>Useful for checking if a specific file has been logged, or for retrieving artifact metadata by path.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The portable URI to search for (e.g., \"inputs://households.csv\").</p> required <code>table_path</code> <code>Optional[str]</code> <p>Optional table path to match.</p> <code>None</code> <code>array_path</code> <code>Optional[str]</code> <p>Optional array path to match.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Artifact]</code> <p>The found <code>Artifact</code> object, or <code>None</code> if no matching artifact is found.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.get_run_artifact","level":2,"title":"<code>get_run_artifact(run_id, key=None, key_contains=None, direction='output')</code>","text":"<p>Convenience helper to fetch a single artifact for a specific run.</p> <p>Args:     run_id: Run identifier.     key: Exact key to match (if present in logged artifacts).     key_contains: Optional substring to match when the exact key is unknown.     direction: \"output\" (default) or \"input\".</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.load_run_output","level":2,"title":"<code>load_run_output(run_id, key, **kwargs)</code>","text":"<p>Load a specific output artifact from a run by key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier.</p> required <code>key</code> <code>str</code> <p>Output artifact key to load.</p> required <code>**kwargs</code> <code>Any</code> <p>Forwarded to <code>Tracker.load(...)</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded artifact data.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.find_matching_run","level":2,"title":"<code>find_matching_run(config_hash, input_hash, git_hash)</code>","text":"<p>Find a previously completed run that matches the identity hashes.</p> <p>Parameters:</p> Name Type Description Default <code>config_hash</code> <code>str</code> <p>Hash of the canonicalized config for the run.</p> required <code>input_hash</code> <code>str</code> <p>Hash of the run inputs.</p> required <code>git_hash</code> <code>str</code> <p>Git commit hash captured with the run.</p> required <p>Returns:</p> Type Description <code>Optional[Run]</code> <p>The matching run, or <code>None</code> if not found or if no database is configured.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.on_run_start","level":2,"title":"<code>on_run_start(callback)</code>","text":"<p>Register a callback to be invoked when a run starts.</p> <p>The callback receives the <code>Run</code> object after it has been initialized but before any user code executes. This is useful for external integrations like OpenLineage event emission, logging, or notifications.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Run], None]</code> <p>A function that takes a <code>Run</code> object as its only argument.</p> required <p>Returns:</p> Type Description <code>Callable[[Run], None]</code> <p>The same callback, allowing use as a decorator.</p> Example <pre><code>@tracker.on_run_start\ndef log_start(run):\n    print(f\"Starting run: {run.id}\")\n\n# Or without decorator:\ntracker.on_run_start(my_callback_function)\n</code></pre>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.on_run_complete","level":2,"title":"<code>on_run_complete(callback)</code>","text":"<p>Register a callback to be invoked when a run completes successfully.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Run, List[Artifact]], None]</code> <p>Called with the completed <code>Run</code> and its output artifacts.</p> required <p>Returns:</p> Type Description <code>Callable[[Run, List[Artifact]], None]</code> <p>The same callback, allowing use as a decorator.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/tracker/#consist.core.tracker.Tracker.on_run_failed","level":2,"title":"<code>on_run_failed(callback)</code>","text":"<p>Register a callback to be invoked when a run fails.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[Run, Exception], None]</code> <p>Called with the failed <code>Run</code> and the raised exception.</p> required <p>Returns:</p> Type Description <code>Callable[[Run, Exception], None]</code> <p>The same callback, allowing use as a decorator.</p>","path":["API Reference","Tracker"],"tags":[]},{"location":"api/views/","level":1,"title":"Views","text":"","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#view-registry","level":2,"title":"View Registry","text":"<p>Registry for dynamic view classes. Accessing a view (e.g. registry.Person) automatically refreshes the underlying DuckDB SQL definition to include new files.</p> <p>Use <code>register(model, key=...)</code> to add SQLModel schemas. Accessing the attribute returns a dynamic SQLModel view class that can be queried via <code>select(...)</code>.</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#consist.core.views.ViewRegistry.register","level":2,"title":"<code>register(model, key=None)</code>","text":"","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#view-factory","level":2,"title":"View Factory","text":"<p>A factory class responsible for generating \"Hybrid Views\" in DuckDB, acting as Consist's \"The Virtualizer\" component.</p> <p>Hybrid Views combine data from materialized tables (often ingested via dlt) with data directly from file-based artifacts (e.g., Parquet, CSV), providing a unified SQL interface to query both \"hot\" and \"cold\" data transparently. This approach is central to Consist's flexible data access strategy.</p> <p>Attributes:</p> Name Type Description <code>tracker</code> <code>Tracker</code> <p>An instance of the Consist <code>Tracker</code>, which provides access to the database engine, artifact resolution, and other run-time context necessary for view creation.</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#consist.core.views.ViewFactory.create_view_from_model","level":2,"title":"<code>create_view_from_model(model, key=None)</code>","text":"<p>Creates both the SQL View and the Python SQLModel class for a given schema.</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/views/#consist.core.views.ViewFactory.create_hybrid_view","level":2,"title":"<code>create_hybrid_view(view_name, concept_key, driver_filter=None, schema_model=None)</code>","text":"<p>Creates or replaces a DuckDB SQL VIEW that combines \"hot\" and \"cold\" data for a given concept.</p> <p>This method generates a \"Hybrid View\" which allows transparent querying across different data storage types. It implements \"View Optimization\" by leveraging DuckDB's capabilities for vectorized reads from files. The resulting view uses <code>UNION ALL BY NAME</code> to gracefully handle \"Schema Evolution\" (different columns across runs or data sources) by nulling out missing columns.</p> <p>\"Hot\" data refers to records already materialized into a DuckDB table (e.g., via ingestion). \"Cold\" data refers to records still residing in file-based artifacts (e.g., Parquet, CSV). Identifiers are quoted for SQL safety; missing cold-file paths are skipped at view creation.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>The name to assign to the newly created or replaced SQL view. This is the name you will use in your SQL queries to access the combined data.</p> required <code>concept_key</code> <code>str</code> <p>The semantic key identifying the data concept (e.g., \"households\", \"transactions\"). Artifacts and materialized tables matching this key will be included in the view.</p> required <code>driver_filter</code> <code>Optional[List[str]]</code> <p>An optional list of artifact drivers (e.g., \"parquet\", \"csv\") to include when querying \"cold\" data. If <code>None</code>, \"parquet\" and \"csv\" drivers are considered by default.</p> <code>None</code> <code>schema_model</code> <code>Type[SQLModel]</code> <p>SQL table definition for underlying data</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the view creation was attempted (even if the view ends up empty), False otherwise.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the <code>Tracker</code>'s database engine is not configured (i.e., <code>db_path</code> was not provided during <code>Tracker</code> initialization).</p>","path":["API Reference","Views"],"tags":[]},{"location":"api/workflow/","level":1,"title":"Workflow Contexts","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#scenario-context","level":2,"title":"Scenario Context","text":"<p>A context manager for grouping multiple steps into a single \"scenario\".</p> <p>A scenario creates a parent run (the \"header\") that aggregates the results, metadata, and lineage of all steps executed within its block. It provides a <code>coupler</code> to pass artifacts between steps, making it ideal for multi-stage simulation workflows.</p> <p>Attributes:</p> Name Type Description <code>coupler</code> <code>Coupler</code> <p>Scenario-local artifact registry for passing outputs between steps. Supports runtime-declared output validation.</p> <p>Examples:</p> <pre><code>with tracker.scenario(\"base_case\") as sc:\n    # Step 1: Pre-process\n    sc.run(preprocess_fn, inputs={\"raw\": \"data.csv\"}, outputs=[\"clean\"])\n    # Step 2: Model (reads \"clean\" from the coupler automatically)\n    sc.run(model_fn, input_keys=[\"clean\"], outputs=[\"results\"])\n</code></pre>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.run_id","level":2,"title":"<code>run_id</code>  <code>property</code>","text":"<p>Run ID of the scenario header.</p> <p>Returns:</p> Type Description <code>str</code> <p>The run ID for the scenario header (or the scenario name if the header has not been created yet).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.config","level":2,"title":"<code>config</code>  <code>property</code>","text":"<p>Read-only view of the scenario configuration.</p> <p>Returns:</p> Type Description <code>MappingProxyType</code> <p>Immutable mapping of configuration values for the scenario. Updates are applied by changing inputs to the scenario, not by mutating this mapping.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.inputs","level":2,"title":"<code>inputs</code>  <code>property</code>","text":"<p>Read-only view of registered exogenous inputs.</p> <p>Returns:</p> Type Description <code>MappingProxyType</code> <p>Immutable mapping of input keys to artifacts added via <code>add_input</code>. This reflects only scenario-level inputs, not step inputs.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.add_input","level":2,"title":"<code>add_input(path, key, **kwargs)</code>","text":"<p>Log an external input artifact to the scenario header run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>ArtifactRef</code> <p>Path (or prebuilt <code>Artifact</code>) representing the input.</p> required <code>key</code> <code>str</code> <p>Semantic key for the artifact.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional metadata forwarded to <code>Tracker.log_artifact</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>Logged artifact associated with the scenario.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a step has already started or the scenario context is inactive.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.declare_outputs","level":2,"title":"<code>declare_outputs(*names, required=False, warn_undocumented=False, description=None)</code>","text":"<p>Declare outputs that should be present in the scenario coupler.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Output keys to declare.</p> <code>()</code> <code>required</code> <code>bool | Mapping[str, bool]</code> <p>Whether declared outputs are required. A mapping allows per-key overrides.</p> <code>False</code> <code>warn_undocumented</code> <code>bool</code> <p>If True, warn when outputs are logged that were not declared.</p> <code>False</code> <code>description</code> <code>Optional[Mapping[str, str]]</code> <p>Human-readable descriptions for declared outputs.</p> <code>None</code>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.require_outputs","level":2,"title":"<code>require_outputs(*names, required=True, warn_undocumented=False, description=None)</code>","text":"<p>Declare required outputs that must be present at scenario exit.</p> <p>This is a convenience wrapper around <code>declare_outputs</code> that defaults <code>required=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Output keys to require.</p> <code>()</code> <code>required</code> <code>bool | Mapping[str, bool]</code> <p>Whether required outputs are enforced. A mapping allows per-key overrides.</p> <code>True</code> <code>warn_undocumented</code> <code>bool</code> <p>If True, warn when outputs are logged that were not declared.</p> <code>False</code> <code>description</code> <code>Optional[Mapping[str, str]]</code> <p>Human-readable descriptions for required outputs.</p> <code>None</code>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.collect_by_keys","level":2,"title":"<code>collect_by_keys(artifacts, *keys, prefix='')</code>","text":"<p>Collect explicit artifacts into the scenario coupler by key.</p> <p>Parameters:</p> Name Type Description Default <code>artifacts</code> <code>Mapping[str, Artifact]</code> <p>Source artifacts mapping (usually outputs from a step).</p> required <code>*keys</code> <code>str</code> <p>Keys to collect from the mapping.</p> <code>()</code> <code>prefix</code> <code>str</code> <p>Optional prefix to apply to collected keys in the coupler.</p> <code>\"\"</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>The collected artifacts keyed by their (possibly prefixed) names.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.run","level":2,"title":"<code>run(fn=None, name=None, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', load_inputs=None, executor='python', container=None, runtime_kwargs=None, inject_context=False, output_mismatch='warn', output_missing='warn')</code>","text":"<p>Execute a cached scenario step and update the Coupler with outputs.</p> <p>This method wraps <code>Tracker.run</code> while ensuring the scenario header is updated with step metadata and artifacts. Use <code>runtime_kwargs</code> for runtime-only inputs and <code>consist.require_runtime_kwargs</code> to validate required keys.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.ScenarioContext.trace","level":2,"title":"<code>trace(name, *, run_id=None, model=None, description=None, config=None, config_plan=None, config_plan_ingest=True, config_plan_profile_schema=False, inputs=None, input_keys=None, optional_input_keys=None, depends_on=None, tags=None, facet=None, facet_from=None, facet_schema_version=None, facet_index=None, hash_inputs=None, year=None, iteration=None, parent_run_id=None, outputs=None, output_paths=None, capture_dir=None, capture_pattern='*', cache_mode='reuse', cache_hydration=None, validate_cached_outputs='lazy', output_mismatch='warn', output_missing='warn')</code>","text":"<p>Manual tracing context manager for scenario steps.</p> <p>This wraps <code>Tracker.trace</code> to log a step while allowing inline code blocks. Use <code>ScenarioContext.run</code> when you want function execution to be skipped on cache hits.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#run-context","level":2,"title":"Run Context","text":"<p>A lightweight helper object injected into user functions. When you execute a run with <code>inject_context=True</code>, Consist passes a <code>RunContext</code> to your function. This allows you to access run-aware helpers—like the run's dedicated artifact directory and artifact logging methods—without needing to reference a global tracker instance directly.</p> <p>Examples:</p> <pre><code>def my_step(ctx: RunContext):\n    # Access the run's dedicated directory\n    output_path = ctx.run_dir / \"results.csv\"\n    # ... generate file ...\n    ctx.log_artifact(output_path, \"results\")\n</code></pre>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.run_dir","level":2,"title":"<code>run_dir</code>  <code>property</code>","text":"<p>Run-specific output directory for the active step.</p> <p>Returns:</p> Type Description <code>Path</code> <p>The directory where this step should write outputs by default. This value is derived from the active run and respects any per-run artifact directory overrides.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.inputs","level":2,"title":"<code>inputs</code>  <code>property</code>","text":"<p>Mapping of input artifact keys to artifacts for the active step.</p> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Dictionary of input artifacts keyed by their semantic keys. Raises a <code>RuntimeError</code> if accessed outside an active run.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.load","level":2,"title":"<code>load(key_or_artifact)</code>","text":"<p>Load data from an input artifact by key or from an Artifact instance.</p> <p>Parameters:</p> Name Type Description Default <code>key_or_artifact</code> <code>Union[str, Artifact]</code> <p>Input artifact key from <code>inputs</code> or an Artifact object.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data (driver-dependent).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_artifact","level":2,"title":"<code>log_artifact(*args, **kwargs)</code>","text":"<p>Log an artifact within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_artifact</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_artifact</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_artifact</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged artifact.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_artifacts","level":2,"title":"<code>log_artifacts(*args, **kwargs)</code>","text":"<p>Log multiple artifacts within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_artifacts</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_artifacts</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_artifacts</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Artifact]</code> <p>Mapping of artifact keys to logged artifacts.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_input","level":2,"title":"<code>log_input(*args, **kwargs)</code>","text":"<p>Log an input artifact within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_input</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_input</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_input</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged input artifact.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_output","level":2,"title":"<code>log_output(*args, **kwargs)</code>","text":"<p>Log an output artifact within the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_output</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments forwarded to <code>Tracker.log_output</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments forwarded to <code>Tracker.log_output</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Artifact</code> <p>The logged output artifact.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.log_meta","level":2,"title":"<code>log_meta(**kwargs)</code>","text":"<p>Update metadata for the active run.</p> <p>This is a thin wrapper around <code>Tracker.log_meta</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Metadata key/value pairs to merge into the run record.</p> <code>{}</code>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.workflow.RunContext.capture_outputs","level":2,"title":"<code>capture_outputs(directory, pattern='*')</code>","text":"<p>Capture files written under <code>directory</code> and log them as outputs on exit.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Directory to monitor for new or modified files.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern for files to capture.</p> <code>\"*\"</code> <p>Yields:</p> Type Description <code>OutputCapture</code> <p>Container listing artifacts that were logged during the context.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#coupler","level":2,"title":"Coupler","text":"<p>Scenario-local helper to thread named artifacts between steps.</p> <p>Coupler is intentionally small: - It stores the \"latest Artifact for a semantic key\" in-memory.</p> <p>It does not log artifacts, infer inputs/outputs, or mutate Artifacts as a side effect of reads. Keep provenance operations on the Tracker.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.set","level":2,"title":"<code>set(key, artifact)</code>","text":"<p>Store an artifact under a validated key.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.set_from_artifact","level":2,"title":"<code>set_from_artifact(key, value)</code>","text":"<p>Set an artifact, accepting both Artifact objects and artifact-like values.</p> <p>This method is useful when integrating with optional dependencies (like noop mode) where you may receive either: - A real Artifact (when tracking is enabled) - An artifact-like object with .path and .container_uri properties (noop mode) - A Path or string (fallback)</p> <p>All three forms are stored in the coupler and can be retrieved with get() or require().</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The coupler key (e.g., \"persons\", \"skims\"). Must follow artifact-key rules.</p> required <code>value</code> <code>Artifact or artifact - like or Path or str</code> <p>The value to store. Can be a real Artifact, artifact-like object with .path/.container_uri properties, a Path, or a string path.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The value that was stored.</p> <p>Examples:</p> <p>Using with optional Consist dependency: <pre><code># Works whether log_output returns Artifact or NoopArtifact\nartifact = tracker.log_output(path, key=\"persons\")\ncoupler.set_from_artifact(\"persons\", artifact)\n</code></pre></p> <p>Mixed real and fallback artifacts: <pre><code>artifact = log_output(path) or path  # Fallback to path string\ncoupler.set_from_artifact(\"key\", artifact)  # Handles both\n</code></pre></p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.update","level":2,"title":"<code>update(artifacts=None, /, **kwargs)</code>","text":"<p>Bulk-update the coupler mapping.</p> <p>Examples:</p> <p><code>coupler.update({\"persons\": art})</code> or <code>coupler.update(persons=art)</code></p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.get","level":2,"title":"<code>get(key)</code>","text":"<p>Return the current artifact for <code>key</code>, or None if unset (key is validated).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.require","level":2,"title":"<code>require(key)</code>","text":"<p>Return the artifact for <code>key</code>, raising a clear error if unset.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.keys","level":2,"title":"<code>keys()</code>","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.items","level":2,"title":"<code>items()</code>","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.values","level":2,"title":"<code>values()</code>","text":"","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.path","level":2,"title":"<code>path(key, *, required=True)</code>","text":"<p>Resolve an artifact's URI to an absolute host path.</p> <p>This does not mutate the Artifact; it only returns a resolved Path.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.declare_outputs","level":2,"title":"<code>declare_outputs(*names, required=False, warn_undocumented=False, description=None)</code>","text":"<p>Declare expected coupler outputs for runtime validation and documentation.</p> <p>This method enables early detection of missing outputs by validating at scenario exit time that all required outputs have been set. Can be used in conjunction with a schema, or independently for dynamic output declarations.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Output names to declare.</p> <code>()</code> <code>required</code> <code>bool or Mapping[str, bool]</code> <p>If bool: applies to all declared outputs. If Mapping: per-key required status (e.g., {\"persons\": True, \"jobs\": False}).</p> <code>False</code> <code>warn_undocumented</code> <code>bool</code> <p>Warn when setting keys that were not declared.</p> <code>False</code> <code>description</code> <code>Mapping[str, str]</code> <p>Human-readable descriptions of outputs for documentation.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any name is not a string.</p> <code>ValueError</code> <p>If any name is empty after stripping whitespace.</p> <p>Examples:</p> <p>Declare all outputs as required:</p> <pre><code>coupler.declare_outputs(\"persons\", \"households\", required=True)\n</code></pre> <p>Mix required and optional:</p> <pre><code>coupler.declare_outputs(\n    \"persons\", \"households\", \"legacy_format\",\n    required={\"persons\": True, \"households\": True, \"legacy_format\": False}\n)\n</code></pre> <p>With descriptions:</p> <pre><code>coupler.declare_outputs(\n    \"skims\",\n    description={\"skims\": \"Zone-to-zone travel times in Zarr format\"}\n)\n</code></pre> Notes <p>Required status is \"sticky\": once an output is marked required, later declarations cannot downgrade it to optional.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.missing_declared_outputs","level":2,"title":"<code>missing_declared_outputs()</code>","text":"<p>Return required declared outputs that have not been set in the coupler.</p> <p>This checks only outputs declared via declare_outputs(), not schema keys. Use validate_all_schema_keys_set() to check schema-based validation.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of required declared outputs that are missing.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.require_outputs","level":2,"title":"<code>require_outputs(*names, required=True, warn_undocumented=False, description=None)</code>","text":"<p>Declare required outputs with optional undocumented key warnings.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.collect_by_keys","level":2,"title":"<code>collect_by_keys(artifacts, *keys, prefix='')</code>","text":"<p>Collect explicit artifacts into the coupler by key.</p> <p>This method selectively ingests artifacts from a mapping, storing them under optionally-prefixed keys. Useful when a step returns many outputs but you only want specific ones, or when you need to namespace outputs by scenario/year.</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"api/workflow/#consist.core.coupler.Coupler.describe_outputs","level":2,"title":"<code>describe_outputs()</code>","text":"<p>Return descriptions of declared outputs (for documentation/introspection).</p>","path":["API Reference","Workflow Contexts"],"tags":[]},{"location":"getting-started/installation/","level":1,"title":"Installation and Quickstart","text":"<p>Consist is not on PyPI yet. For now, install from source. When the first public release is published, this page will include the PyPI command as well.</p>","path":["Getting Started","Installation and Quickstart"],"tags":[]},{"location":"getting-started/installation/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>Python 3.11 or newer</li> <li>git</li> <li>Optional: Docker or Singularity for container workflows</li> <li>Optional: Jupyter for running the example notebooks</li> </ul>","path":["Getting Started","Installation and Quickstart"],"tags":[]},{"location":"getting-started/installation/#install-from-source","level":2,"title":"Install from source","text":"<pre><code>git clone https://github.com/LBNL-UCB-STI/consist.git\ncd consist\npip install -e .\n</code></pre> <p>Optional extras:</p> <ul> <li>Ingestion (DLT): <code>pip install -e \".[ingest]\"</code></li> <li>Notebooks: <code>pip install -e \".[examples]\"</code></li> </ul>","path":["Getting Started","Installation and Quickstart"],"tags":[]},{"location":"getting-started/installation/#install-from-pypi","level":2,"title":"Install from PyPI","text":"<p>NOT WORKING, DELETE WHEN PUBLIC: Consist is not published to PyPI yet.</p> <pre><code>pip install consist\n</code></pre> <p>Optional extras:</p> <ul> <li>Ingestion (DLT): <code>pip install \"consist[ingest]\"</code></li> <li>Notebooks: <code>pip install \"consist[examples]\"</code></li> </ul>","path":["Getting Started","Installation and Quickstart"],"tags":[]},{"location":"getting-started/installation/#5-minute-quickstart","level":2,"title":"5-minute quickstart","text":"<p>This creates a small CSV, runs a tracked transformation, and loads the cached result.</p> <pre><code>from pathlib import Path\n\nimport pandas as pd\nimport consist\nfrom consist import Tracker, use_tracker\n\n# 1) Create a tracker (database + run directory)\ntracker = Tracker(run_dir=\"./runs\", db_path=\"./provenance.duckdb\")\n\n# 2) Prepare a tiny input file\nPath(\"raw.csv\").write_text(\"category,value\\nA,1\\nA,2\\nB,3\\n\")\n\n# 3) Define a function to run\ndef summarize(raw: pd.DataFrame) -&gt; pd.DataFrame:\n    return raw.groupby(\"category\")[\"value\"].sum().reset_index()\n\n# 4) Run with Consist\nwith use_tracker(tracker):\n    result = consist.run(\n        fn=summarize,\n        inputs={\"raw\": Path(\"raw.csv\")},\n        outputs=[\"summary\"],\n    )\n\n# 5) Load the output\nsummary = consist.load_df(result.outputs[\"summary\"])\nprint(summary)\n</code></pre> <p>Run the same script again and you should see a cache hit (no re-execution).</p>","path":["Getting Started","Installation and Quickstart"],"tags":[]},{"location":"getting-started/installation/#next-steps","level":2,"title":"Next steps","text":"<ul> <li>Read the Concepts overview for the mental model.</li> <li>Follow the Usage Guide for scenarios, couplers, and workflows.</li> <li>Explore the Example Notebooks if you prefer a guided walkthrough.</li> </ul>","path":["Getting Started","Installation and Quickstart"],"tags":[]},{"location":"integrations/","level":1,"title":"Integrations","text":"<p>These integrations extend Consist's capabilities for container execution, data ingestion, and configuration management.</p>","path":["Integrations"],"tags":[]},{"location":"integrations/#core-integrations","level":2,"title":"Core Integrations","text":"","path":["Integrations"],"tags":[]},{"location":"integrations/#container-integration","level":3,"title":"Container Integration","text":"<p>Execute Docker and Singularity containers with provenance tracking.</p> <ul> <li>Run ActivitySim, SUMO, BEAM, and other existing tools</li> <li>Image digest-based caching for reproducibility</li> <li>Covers volume mounting, output handling, and debugging</li> <li>Full guide | API reference</li> </ul>","path":["Integrations"],"tags":[]},{"location":"integrations/#dlt-loader","level":3,"title":"DLT Loader","text":"<p>Schema-validated data ingestion with DuckDB.</p> <ul> <li>Ingest Parquet, CSV, DataFrames with type enforcement</li> <li>Automatic provenance column injection</li> <li>Cross-run SQL queries with registered schemas</li> <li>Full guide | API reference</li> </ul>","path":["Integrations"],"tags":[]},{"location":"integrations/#config-adapters","level":3,"title":"Config Adapters","text":"<p>Model-specific configuration discovery and tracking.</p> <ul> <li>ActivitySim: Track and query calibration parameters across runs</li> <li>Automatic discovery of YAML hierarchies and CSV references</li> <li>Queryable configuration tables for sensitivity analysis</li> <li>Full guide</li> <li>ActivitySim adapter</li> <li>BEAM adapter</li> </ul>","path":["Integrations"],"tags":[]},{"location":"integrations/config_adapters/","level":1,"title":"Config Adapters","text":"<p>Config adapters provide model-specific interfaces to discover, canonicalize, and ingest complex file-based configurations. They enable tracking of calibration-sensitive parameters as queryable database tables while preserving full config provenance via artifacts.</p>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#overview","level":2,"title":"Overview","text":"","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#when-to-use-config-adapters","level":3,"title":"When to Use Config Adapters","text":"<p>Use config adapters when your model configuration:</p> <ul> <li>Lives in multiple files across a directory hierarchy (YAML, CSV, HOCON, etc.)</li> <li>Has layered inheritance or cascade logic (e.g., <code>inherit_settings: true</code>)</li> <li>Contains calibration-sensitive parameters you want to query and compare across runs</li> <li>Needs content hashing for provenance and cache identity</li> <li>Should remain decoupled from Consist core (no hard dependency on your model's libraries)</li> </ul>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#when-to-use-in-memory-configs-instead","level":3,"title":"When to Use In-Memory Configs Instead","text":"<p>Use the standard config API if:</p> <ul> <li>Configuration fits in a Python dict or Pydantic model</li> <li>You don't need to track file-based config artifacts</li> <li>A simple facet extraction suffices for your use case</li> </ul>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#architecture","level":3,"title":"Architecture","text":"<p>Config adapters implement three phases:</p> <pre><code>Config Directory\n    ↓\ndiscover()           ← Locate files, compute content hash\n    ↓\nCanonicalConfig\n    ↓\ncanonicalize()       ← Generate artifact specs + ingest specs\n    ↓\nCanonicalizationResult (artifacts + ingestables)\n    ↓\ntracker.canonicalize_config()  ← Log artifacts, ingest to DB\n    ↓\nQueryable Tables + Full Provenance\n</code></pre>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#adapter-guides","level":2,"title":"Adapter Guides","text":"<p>Detailed, model-specific implementations live here:</p> <ul> <li>ActivitySim Config Adapter</li> <li>BEAM Config Adapter</li> </ul> <p>These guides include discovery/canonicalization behaviors, ingestion schemas, and query examples. The API patterns are shared: you construct an adapter, call <code>tracker.canonicalize_config(...)</code> or <code>tracker.prepare_config(...)</code>, and then query the resulting cache tables.</p>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#extensibility","level":3,"title":"Extensibility","text":"<p>Config adapters follow a Protocol interface, making it straightforward to add support for other models. Future adapters might include:</p> <ul> <li>Additional HOCON/YAML models: Layered config discovery + calibration-sensitive key extraction</li> <li>Custom Models: Any model with file-based config that needs parameterization tracking</li> </ul> <p>If you'd like to add an adapter for your own model, consult the ActivitySim adapter source as a reference implementation.</p>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters/#see-also","level":2,"title":"See Also","text":"<ul> <li>Configuration, Identity, and Facets — In-memory config and facets</li> <li>Ingestion and Hybrid Views — DLT-based data ingestion</li> <li>Caching and Hydration — Run signature and cache identity</li> </ul>","path":["Integrations","Config Adapters"],"tags":[]},{"location":"integrations/config_adapters_activitysim/","level":1,"title":"ActivitySim Config Adapter","text":"<p>The ActivitySim config adapter discovers and canonicalizes ActivitySim configuration directories (with support for YAML inheritance, CSV references, and config bundling).</p>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#overview","level":2,"title":"Overview","text":"<p>Features:</p> <ul> <li>YAML Discovery: Resolves <code>settings.yaml</code> and active model YAMLs via <code>inherit_settings</code> and <code>include_settings</code></li> <li>CSV Reference Detection: Finds coefficients, probabilities, and specification files via registry + heuristics</li> <li>Constants Extraction: Flattens YAML <code>CONSTANTS</code> sections into queryable rows</li> <li>Config Bundling: Creates tarball archives for full config provenance</li> <li>Config Materialization: Apply parameter overrides to base bundles for scenario-based runs</li> <li>Strict/Lenient Modes: Error on file integrity issues or gracefully skip</li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#use-cases","level":2,"title":"Use Cases","text":"<p>1. Track calibration parameters across scenarios</p> <p>Compare how constants and coefficients change between runs:</p> <pre><code>from consist.integrations.activitysim import ActivitySimConfigAdapter\n\nadapter = ActivitySimConfigAdapter()\n\n# Run baseline scenario\nrun_a = tracker.begin_run(\"baseline\", \"activitysim\", cache_mode=\"overwrite\")\ntracker.canonicalize_config(adapter, [config_dir], ingest=True)\ntracker.end_run()\n\n# Run adjusted scenario\nrun_b = tracker.begin_run(\"adjusted\", \"activitysim\", cache_mode=\"overwrite\")\ntracker.canonicalize_config(adapter, [config_dir_adjusted], ingest=True)\ntracker.end_run()\n\n# Query which runs used a specific sample_rate\nrows_by_run = adapter.constants_by_run(\n    key=\"sample_rate\",\n    collapse=\"first\",\n    tracker=tracker,\n)\n\n# Use in joins/facet analyses\nsample_rate_sq = adapter.constants_query(key=\"sample_rate\").subquery()\n</code></pre> <p>2. Precompute config plans for caching + orchestration</p> <p>Prepare config artifacts and ingestion specs before a run, then apply them inside <code>consist.run</code>/<code>consist.trace</code> (or <code>Tracker.run</code>/<code>Tracker.trace</code>):</p> <pre><code>import consist\nfrom consist import use_tracker\n\nadapter = ActivitySimConfigAdapter()\nplan = tracker.prepare_config(adapter, [overlay_dir, base_dir])\n\nwith use_tracker(tracker):\n    consist.run(\n        fn=run_activitysim,\n        name=\"activitysim\",\n        config={\"scenario\": \"baseline\"},\n        config_plan=plan,\n        cache_mode=\"auto\",\n    )\n</code></pre> <p>You can also pass shared adapter options and run validation without ingesting:</p> <pre><code>from consist.core.config_canonicalization import ConfigAdapterOptions\n\noptions = ConfigAdapterOptions(strict=True, bundle=False, ingest=False)\nplan = tracker.prepare_config(\n    adapter,\n    [overlay_dir, base_dir],\n    options=options,\n    validate_only=True,\n)\nif plan.diagnostics and not plan.diagnostics.ok:\n    raise ValueError(\"Config validation failed.\")\n</code></pre> <p>3. Apply parameter adjustments for sensitivity testing</p> <p>Use the <code>materialize()</code> method to apply overrides to a baseline config:</p> <pre><code>from consist.integrations.activitysim import ConfigOverrides\n\n# Load baseline bundle\nbaseline_bundle = Path(\"outputs/base_run/config_bundle_xxxxx.tar.gz\")\n\n# Create overrides\noverrides = ConfigOverrides(\n    constants={\n        (\"settings.yaml\", \"sample_rate\"): 0.1,\n        (\"accessibility.yaml\", \"CONSTANTS.AUTO_TIME\"): 60.0,\n    },\n    coefficients={\n        (\"tour_mode_coeffs.csv\", \"car_ASC\", \"\"): -0.5,  # \"\" for direct coefficients\n    }\n)\n\n# Materialize new config with overrides\nmaterialized = adapter.materialize(\n    baseline_bundle,\n    overrides,\n    output_dir=Path(\"temp/adjusted_config\"),\n    identity=tracker.identity,\n)\n\n# Canonicalize the adjusted config\nrun = tracker.begin_run(\"sensitivity_test\", \"activitysim\")\ntracker.canonicalize_config(adapter, materialized.root_dirs, ingest=True)\ntracker.end_run()\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#discovery-and-canonicalization-workflow","level":2,"title":"Discovery and Canonicalization Workflow","text":"","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#discovery-phase-adapterdiscover","level":3,"title":"Discovery Phase (<code>adapter.discover()</code>)","text":"<ol> <li>Locates <code>settings.yaml</code> and loads active <code>models</code> list</li> <li>Resolves model YAMLs via suffix stripping and alias mapping</li> <li>Supports YAML <code>include_settings</code> for file references</li> <li>Computes content hash of all config directories</li> <li>Returns <code>CanonicalConfig</code> with file inventory</li> </ol> <p>Configuration options:</p> <pre><code>adapter = ActivitySimConfigAdapter(\n    model_name=\"activitysim\",        # Metadata label\n    adapter_version=\"0.1\",           # For run.meta tracking\n    allow_heuristic_refs=True,       # Detect *_FILE, *_PATH keys\n    bundle_configs=True,             # Create tarball archive\n    bundle_cache_dir=None,           # Default: &lt;run_dir&gt;/config_bundles\n)\n\ncanonical = adapter.discover(\n    root_dirs=[config_dir],\n    identity=tracker.identity,\n    strict=False,  # Warn on missing settings.yaml\n)\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#canonicalization-phase-adaptercanonicalize","level":3,"title":"Canonicalization Phase (<code>adapter.canonicalize()</code>)","text":"<ol> <li>Loads effective settings (after merging inheritance chain)</li> <li>Logs active YAMLs and referenced CSVs as input artifacts</li> <li>Extracts <code>CONSTANTS</code> + allowlisted settings → ingestion spec</li> <li>Classifies CSVs as coefficients or probabilities → ingestion specs</li> <li>Creates config bundle tarball (cached by content hash)</li> <li>Returns specs for artifact logging and ingest</li> </ol> <p>Key behaviors:</p> <ul> <li>Constant Attribution: Constants attributed to effective YAML after inheritance</li> <li>CSV Classification: Files ending in <code>_coefficients.csv</code>, <code>_coeffs.csv</code>, <code>_coefficients_template.csv</code>, or <code>_probs.csv</code></li> <li>CSV Format Support: Both direct (<code>value</code> column) and template (segment columns) coefficient formats</li> <li>Gzip Support: Transparently handles <code>.csv.gz</code> files</li> <li>Lenient Mode (default): Missing referenced CSVs logged as warnings; ingestion proceeds</li> <li>Strict Mode: Raises on missing files or malformed CSVs</li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#queryable-schemas","level":2,"title":"Queryable Schemas","text":"<p>ActivitySim config tables are deduplicated by content hash. To map rows back to individual runs, join against <code>activitysim_config_ingest_run_link</code> on <code>content_hash</code> and <code>table_name</code>.</p>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#activitysim_constants_cache","level":3,"title":"<code>activitysim_constants_cache</code>","text":"<p>Flattened YAML constants and allowlisted settings, deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the source file <code>file_name</code> str Primary key: source YAML file name <code>key</code> str Primary key: dot-notation constant key (e.g., <code>CONSTANTS.AUTO_TIME</code>, <code>sample_rate</code>) <code>value_type</code> str Type tag: <code>null</code>, <code>bool</code>, <code>num</code>, <code>str</code>, <code>json</code> <code>value_str</code> str | NULL String value when <code>value_type == \"str\"</code> <code>value_num</code> float | NULL Numeric value when <code>value_type == \"num\"</code> <code>value_bool</code> bool | NULL Boolean value when <code>value_type == \"bool\"</code> <code>value_json</code> JSON | NULL Complex value when <code>value_type == \"json\"</code> <p>Query example: Find runs with specific constant values</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimConstantsCache\n\nwith Session(tracker.engine) as session:\n    # Find runs where AUTO_TIME &gt; 50\n    rows = session.exec(\n        select(\n            ActivitySimConfigIngestRunLink.run_id,\n            ActivitySimConstantsCache.value_num,\n        )\n        .join(\n            ActivitySimConstantsCache,\n            ActivitySimConstantsCache.content_hash\n            == ActivitySimConfigIngestRunLink.content_hash,\n        )\n        .where(ActivitySimConfigIngestRunLink.table_name == \"activitysim_constants_cache\")\n        .where(ActivitySimConstantsCache.key == \"CONSTANTS.AUTO_TIME\")\n        .where(ActivitySimConstantsCache.value_num &gt; 50)\n    ).all()\n\n    for run_id, value_num in rows:\n        print(f\"{run_id}: AUTO_TIME = {value_num}\")\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#activitysim_coefficients_cache","level":3,"title":"<code>activitysim_coefficients_cache</code>","text":"<p>Parsed coefficient rows from CSV files, deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the source file <code>file_name</code> str Primary key: source CSV file name <code>coefficient_name</code> str Primary key: coefficient identifier <code>segment</code> str Primary key: segment name (template) or \"\" (direct) <code>source_type</code> str \"direct\" (value column) or \"template\" (segment columns) <code>value_raw</code> str Raw CSV cell value <code>value_num</code> float | NULL Parsed numeric value <code>constrain</code> str | NULL Constraint flag from CSV (if present) <code>is_constrained</code> bool | NULL Parsed constraint boolean <p>Query example: Compare coefficients across runs</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimCoefficientsCache\n\nwith Session(tracker.engine) as session:\n    # Find how a specific coefficient changed\n    rows = session.exec(\n        select(\n            ActivitySimConfigIngestRunLink.run_id,\n            ActivitySimCoefficientsCache.coefficient_name,\n            ActivitySimCoefficientsCache.segment,\n            ActivitySimCoefficientsCache.value_raw,\n        )\n        .join(\n            ActivitySimCoefficientsCache,\n            ActivitySimCoefficientsCache.content_hash\n            == ActivitySimConfigIngestRunLink.content_hash,\n        )\n        .where(\n            ActivitySimConfigIngestRunLink.table_name\n            == \"activitysim_coefficients_cache\"\n        )\n        .where(ActivitySimCoefficientsCache.coefficient_name == \"car_ASC\")\n    ).all()\n\n    for run_id, coef_name, segment, value_raw in rows:\n        print(f\"{run_id}: {coef_name}[{segment}] = {value_raw}\")\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#activitysim_probabilities_cache","level":3,"title":"<code>activitysim_probabilities_cache</code>","text":"<p>Parsed probability table rows (dims and numeric probabilities separated), deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the source file <code>file_name</code> str Primary key: source CSV file name <code>row_index</code> int Primary key: row index in source file <code>dims</code> JSON Non-numeric dimension values (e.g., <code>{\"mode\": \"drive\", \"income\": \"high\"}</code>) <code>probs</code> JSON Numeric probability/weight values (e.g., <code>{\"prob_0\": 0.3, \"prob_1\": 0.7}</code>) <p>Query example: Inspect probability tables</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimProbabilitiesCache\n\nwith Session(tracker.engine) as session:\n    # Find probability rows for a specific file\n    rows = session.exec(\n        select(\n            ActivitySimConfigIngestRunLink.run_id,\n            ActivitySimProbabilitiesCache.row_index,\n            ActivitySimProbabilitiesCache.dims,\n            ActivitySimProbabilitiesCache.probs,\n        )\n        .join(\n            ActivitySimProbabilitiesCache,\n            ActivitySimProbabilitiesCache.content_hash\n            == ActivitySimConfigIngestRunLink.content_hash,\n        )\n        .where(\n            ActivitySimConfigIngestRunLink.table_name\n            == \"activitysim_probabilities_cache\"\n        )\n        .where(ActivitySimProbabilitiesCache.file_name == \"atwork_probs.csv\")\n    ).all()\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#additional-tables","level":4,"title":"Additional Tables","text":"<ul> <li><code>activitysim_probabilities_entries_cache</code></li> <li><code>activitysim_probabilities_meta_entries_cache</code></li> <li><code>activitysim_coefficients_template_refs_cache</code></li> <li><code>activitysim_config_ingest_run_link</code></li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#configuration-best-practices","level":2,"title":"Configuration Best Practices","text":"<p>1. Organize config directories with inheritance</p> <pre><code>configs/\n  base/\n    settings.yaml      # inherit_settings: true\n    accessibility.yaml\n    coefficients.csv\n  overlay/\n    settings.yaml      # inherits from base\n    local_overrides.yaml\n</code></pre> <p>Call with overlay first (takes precedence):</p> <pre><code>tracker.canonicalize_config(adapter, [overlay_dir, base_dir])\n</code></pre> <p>2. Use strict mode for validation</p> <p>In development/testing, catch config problems early:</p> <pre><code>tracker.canonicalize_config(adapter, config_dirs, strict=True)\n</code></pre> <p>3. Leverage config bundling for reconstruction</p> <p>The bundle tarball preserves full config state, enabling: - Auditing exact config used for a run - Reproducing runs via <code>materialize()</code> - Sharing configs across machines</p> <p>Bundles are cached by content hash, so repeated canonicalizations are efficient.</p> <p>4. Query constants before running</p> <p>Use <code>activitysim_constants_cache</code> + <code>activitysim_config_ingest_run_link</code> queries to validate assumptions:</p> <pre><code>from sqlmodel import Session, select\nfrom consist.models.activitysim import ActivitySimConstantsCache, ActivitySimConfigIngestRunLink\n\n# Verify sample_rate is set to expected value\nwith Session(tracker.engine) as session:\n    result = session.exec(\n        select(ActivitySimConstantsCache.value_num)\n        .join(\n            ActivitySimConfigIngestRunLink,\n            ActivitySimConfigIngestRunLink.content_hash\n            == ActivitySimConstantsCache.content_hash,\n        )\n        .where(ActivitySimConfigIngestRunLink.run_id == run.id)\n        .where(ActivitySimConfigIngestRunLink.table_name == \"activitysim_constants_cache\")\n        .where(ActivitySimConstantsCache.key == \"sample_rate\")\n    ).first()\n    value_num = result[0] if result else None\n    assert value_num == 0.25, f\"Expected sample_rate=0.25, got {value_num}\"\n</code></pre>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_activitysim/#api-reference","level":2,"title":"API Reference","text":"<p>For detailed method signatures, parameters, and return types, see:</p> <ul> <li><code>ActivitySimConfigAdapter</code></li> <li><code>ConfigOverrides</code></li> <li><code>Tracker.canonicalize_config()</code></li> </ul>","path":["Integrations","Config Adapters","ActivitySim Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/","level":1,"title":"BEAM Config Adapter","text":"<p>The BEAM config adapter canonicalizes HOCON <code>.conf</code> configurations, resolves includes, and ingests every resolved key/value for fast queries. Paths that exist on disk are logged as input artifacts; missing paths produce warnings.</p> <p>Dependencies: - Requires <code>pyhocon</code> for parsing <code>.conf</code> files. - Requires <code>pandas</code> only if you use tabular ingestion via <code>BeamIngestSpec</code>.</p>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#usage","level":2,"title":"Usage","text":"<pre><code>from pathlib import Path\n\nfrom consist.integrations.beam import BeamConfigAdapter\n\nconfig_root = Path(\"/path/to/beam/production/sfbay\")\nadapter = BeamConfigAdapter(\n    primary_config=config_root / \"sfbay-pilates-base.conf\",\n    env_overrides={\"PWD\": str(config_root.parent.parent)},\n)\n\nrun = tracker.begin_run(\"beam_baseline\", \"beam\")\ntracker.canonicalize_config(adapter, [config_root], ingest=True)\ntracker.end_run()\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#facets","level":2,"title":"Facets","text":"<pre><code>plan = tracker.prepare_config(\n    adapter,\n    [config_root],\n    facet_spec={\n        \"keys\": [\n            \"beam.agentsim.simulationName\",\n            {\"key\": \"beam.physsim.name\", \"alias\": \"physsim\"},\n        ],\n    },\n    facet_schema_name=\"beam_config\",\n)\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#tabular-ingestion-by-config-key","level":2,"title":"Tabular Ingestion by Config Key","text":"<pre><code>from sqlmodel import Field, SQLModel\n\nfrom consist.integrations.beam import BeamIngestSpec\n\n\nclass BeamVehicleTypesCache(SQLModel, table=True):\n    __tablename__ = \"beam_vehicletypes_cache\"\n    __table_args__ = {\"schema\": \"global_tables\"}\n\n    id: int = Field(primary_key=True)\n    value: str\n    content_hash: str = Field(index=True)\n\n\nadapter = BeamConfigAdapter(\n    primary_config=config_root / \"sfbay-pilates-base.conf\",\n    ingest_specs=[\n        BeamIngestSpec(\n            key=\"beam.agentsim.agents.vehicles.vehicleTypesFilePath\",\n            table_name=\"beam_vehicletypes_cache\",\n            schema=BeamVehicleTypesCache,\n        ),\n    ],\n)\n</code></pre> <p>Notes: - Schemas used with <code>BeamIngestSpec</code> should include a <code>content_hash</code> column for dedupe. - If your configs use optional env substitutions (e.g., <code>${?BEAM_OUTPUT}</code>), set them via <code>env_overrides</code> to avoid unresolved keys during canonicalization/materialization.</p>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#tables","level":2,"title":"Tables","text":"","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#beam_config_cache","level":3,"title":"<code>beam_config_cache</code>","text":"<p>Canonicalized config key/value rows, deduplicated by content hash.</p> Column Type Notes <code>content_hash</code> str Primary key: content hash for the config <code>key</code> str Primary key: dotted config path <code>value_type</code> str One of <code>str</code>, <code>num</code>, <code>bool</code>, <code>null</code>, <code>json</code> <code>value_str</code> str | NULL String values <code>value_num</code> float | NULL Numeric values <code>value_bool</code> bool | NULL Boolean values <code>value_json_str</code> str | NULL JSON-encoded values","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#beam_config_ingest_run_link","level":3,"title":"<code>beam_config_ingest_run_link</code>","text":"<p>Links runs to ingested config hashes for query joins.</p> Column Type Notes <code>run_id</code> str Primary key: Consist run id <code>table_name</code> str Primary key: cache table name <code>content_hash</code> str Primary key: config content hash <code>config_name</code> str Primary key: config file name <p>Query example: Compare a key across runs</p> <pre><code>from sqlmodel import Session, select\n\nfrom consist.models.beam import BeamConfigCache, BeamConfigIngestRunLink\n\nwith Session(tracker.engine) as session:\n    rows = session.exec(\n        select(\n            BeamConfigIngestRunLink.run_id,\n            BeamConfigCache.key,\n            BeamConfigCache.value_num,\n            BeamConfigCache.value_str,\n        )\n        .join(\n            BeamConfigCache,\n            BeamConfigCache.content_hash == BeamConfigIngestRunLink.content_hash,\n        )\n        .where(BeamConfigIngestRunLink.table_name == \"beam_config_cache\")\n        .where(BeamConfigCache.key == \"beam.agentsim.agentSampleSizeAsFractionOfPopulation\")\n    ).all()\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#behavior-notes","level":2,"title":"Behavior Notes","text":"<ul> <li><code>resolve_substitutions=True</code> resolves HOCON substitutions; set to False to keep raw expressions.</li> <li><code>env_overrides</code> supplies environment variables for optional substitutions (e.g., <code>${?BEAM_OUTPUT}</code>).</li> <li><code>strict=True</code> raises on missing referenced files; otherwise missing paths are logged as warnings.</li> </ul>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#materialize-overrides","level":2,"title":"Materialize Overrides","text":"<pre><code>from consist.integrations.beam import BeamConfigOverrides\n\noverrides = BeamConfigOverrides(\n    values={\n        \"beam.agentsim.agentSampleSizeAsFractionOfPopulation\": 0.75,\n        \"beam.agentsim.lastIteration\": 5,\n    }\n)\n\nmaterialized = adapter.materialize(\n    [config_root],\n    overrides,\n    output_dir=Path(\"tmp/beam_materialized\"),\n    identity=tracker.identity,\n)\n</code></pre> <p>If you already built a config plan (e.g., for caching), you can reuse its <code>config_dirs</code> metadata:</p> <pre><code>plan = tracker.prepare_config(adapter, [config_root])\nmaterialized = adapter.materialize_from_plan(\n    plan,\n    overrides,\n    output_dir=Path(\"tmp/beam_materialized\"),\n    identity=tracker.identity,\n)\n</code></pre>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/config_adapters_beam/#api-reference","level":2,"title":"API Reference","text":"<p>For detailed method signatures, parameters, and return types, see:</p> <ul> <li><code>BeamConfigAdapter</code></li> <li><code>BeamConfigOverrides</code></li> <li><code>Tracker.canonicalize_config()</code></li> </ul>","path":["Integrations","Config Adapters","BEAM Config Adapter"],"tags":[]},{"location":"integrations/containers/","level":1,"title":"Containers","text":"<p>Consist Container API Module</p> <p>This module provides a high-level API for executing containerized steps (e.g., Docker, Singularity/Apptainer) with automatic provenance tracking and caching through Consist. It abstracts away the complexities of interacting directly with container runtimes and integrates seamlessly with Consist's <code>Tracker</code> to log container execution details, input dependencies, and output artifacts.</p> <p>Key functionalities include: -   Container Execution with Provenance: Wraps container execution     within a Consist <code>start_run</code> context, ensuring that container image     identity, commands, environment, and file I/O are fully tracked. -   Backend Agnosticism: Supports different container runtimes     (Docker, Singularity/Apptainer) via a unified interface. -   Automated Input/Output Logging: Automatically logs host-side     files as inputs and scans specified paths for outputs, linking them     to the container run.</p> <p>               Bases: <code>BaseModel</code></p> <p>Represents the 'Configuration' of a container run for hashing purposes.</p> <p>This model captures all relevant parameters that define a containerized execution, allowing Consist to compute a canonical hash for the container's configuration. This hash is critical for determining cache hits and ensuring reproducibility.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>str</code> <p>The name or reference of the container image (e.g., \"ubuntu:latest\").</p> <code>image_digest</code> <code>Optional[str]</code> <p>A content-addressable SHA digest of the container image, used for precise reproducibility. If None, the image tag is used.</p> <code>command</code> <code>List[str]</code> <p>The command and its arguments to execute inside the container, represented as a list of strings (exec form).</p> <code>environment</code> <code>Dict[str, str]</code> <p>A dictionary of environment variables passed to the container.</p> <code>backend</code> <code>str</code> <p>The container backend used to execute this container (e.g., \"docker\", \"singularity\").</p> <code>extra_args</code> <code>Dict[str, Any]</code> <p>Additional arguments or configuration specific to the container backend that might influence the execution but are not part of the core identity (e.g., resource limits, specific volume options).</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.api.ContainerResult","level":2,"title":"<code>ContainerResult</code>  <code>dataclass</code>","text":"<p>Return value for run_container with cached output artifacts.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.api.ContainerResult.output","level":3,"title":"<code>output</code>  <code>property</code>","text":"<p>Convenience: return the first (or only) output artifact if present.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.api.run_container","level":2,"title":"<code>run_container(tracker, run_id, image, command, volumes, inputs, outputs, environment=None, working_dir=None, backend_type='docker', pull_latest=False, lineage_mode='full')</code>","text":"<p>Executes a containerized step with optional provenance tracking and caching via Consist.</p> <p>This function acts as a high-level wrapper that integrates container execution with Consist's <code>Tracker</code>. In lineage mode \"full\" it initiates a <code>Consist</code> run (or attaches to an active run), uses the container's image and command as part of the run's identity (code/config), and tracks host-side files as inputs and outputs. In lineage mode \"none\" it only executes the container and returns a stable manifest/hash for callers to incorporate into an enclosing step's identity.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>Tracker</code> <p>The active Consist <code>Tracker</code> instance to use for provenance logging.</p> required <code>run_id</code> <code>str</code> <p>A unique identifier for this container execution run within Consist.</p> required <code>image</code> <code>str</code> <p>The container image to use (e.g., \"ubuntu:latest\", \"my_repo/my_image:tag\").</p> required <code>command</code> <code>Union[str, List[str]]</code> <p>The command to execute inside the container. Can be a string or a list of strings (for exec form). Commands are validated for non-empty tokens and a maximum length.</p> required <code>volumes</code> <code>Dict[str, str]</code> <p>A dictionary mapping host paths to container paths for volume mounts. Example: <code>{\"/host/path\": \"/container/path\"}</code>. Host paths are resolved and validated against tracker mounts when present; relative paths are resolved against the first mount root.</p> required <code>inputs</code> <code>List[ArtifactRef]</code> <p>A list of paths (str/Path) or <code>Artifact</code> objects on the host machine that serve as inputs to the containerized process. These are logged as Consist inputs.</p> required <code>outputs</code> <code>List[str]</code> <p>A list of paths on the host machine that are expected to be generated or modified by the containerized process. These paths will be scanned and logged as Consist output artifacts. Host paths are validated against tracker mounts when present.</p> required <code>outputs</code> <code>Dict[str, str]</code> <p>Alternatively, pass a mapping of logical output keys to host paths. The artifact will be logged with the provided key instead of the filename. Host paths are validated against tracker mounts when present.</p> required <code>environment</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary of environment variables to set inside the container. Defaults to empty.</p> <code>None</code> <code>working_dir</code> <code>Optional[str]</code> <p>The working directory inside the container where the command will be executed. If None, the default working directory of the container image will be used.</p> <code>None</code> <code>backend_type</code> <code>str</code> <p>The container runtime backend to use. Currently supports \"docker\" and \"singularity\".</p> <code>\"docker\"</code> <code>pull_latest</code> <code>bool</code> <p>If True, the Docker backend will attempt to pull the latest image before execution. (Applicable only for 'docker' backend).</p> <code>False</code> <code>lineage_mode</code> <code>Literal['full', 'none']</code> <p>\"full\" performs Consist provenance tracking, caching, and output scanning. \"none\" skips Consist logging/caching and does not scan outputs.</p> <code>\"full\"</code> <p>Returns:</p> Type Description <code>ContainerResult</code> <p>Structured result containing logged output artifacts and cache metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown <code>backend_type</code> is specified.</p> <code>RuntimeError</code> <p>If the container execution itself fails (e.g., non-zero exit code). If the underlying backend fails to resolve image digest or run the container.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.backend","level":2,"title":"<code>backend</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.command","level":2,"title":"<code>command</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.declared_outputs","level":2,"title":"<code>declared_outputs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.environment","level":2,"title":"<code>environment</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.extra_args","level":2,"title":"<code>extra_args = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.image","level":2,"title":"<code>image</code>  <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.image_digest","level":2,"title":"<code>image_digest = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.volumes","level":2,"title":"<code>volumes = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.working_dir","level":2,"title":"<code>working_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/containers/#consist.integrations.containers.models.ContainerDefinition.to_hashable_config","level":2,"title":"<code>to_hashable_config()</code>","text":"<p>Returns a clean dictionary representation of the container configuration suitable for hashing.</p> <p>This method generates a dictionary that excludes <code>None</code> values, ensuring a canonical representation of the configuration for consistent hash computation. This is crucial for Consist's caching mechanism.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the essential configuration parameters of the container, stripped of any <code>None</code> values, ready for hashing.</p>","path":["Integrations","Containers"],"tags":[]},{"location":"integrations/dlt_loader/","level":1,"title":"DLT Loader","text":"<p>Requires the optional <code>ingest</code> extra: <code>pip install \"consist[ingest]\"</code>.</p> <p>Consist dlt (Data Load Tool) Integration Module</p> <p>This module provides the integration layer between Consist and the <code>dlt</code> library, facilitating the robust and efficient ingestion of artifact data into the DuckDB database. It is responsible for materializing various data formats (e.g., Pandas DataFrames, Parquet, CSV, Zarr metadata) and ensuring that Consist's system-level provenance columns (such as <code>consist_run_id</code>, <code>consist_artifact_id</code>) are correctly injected into the data.</p> <p>Key functionalities include: -   Dynamic Schema Extension: User-defined <code>SQLModel</code> schemas are dynamically extended     with Consist's provenance-tracking system columns. -   Flexible Ingestion Strategies: Supports different data ingestion mechanisms,     including vectorized loading (for Pandas DataFrames, PyArrow tables) and streaming     for large datasets. -   Format-Specific Handlers: Contains specialized functions for processing and     preparing data from common file formats like Parquet, CSV, and extracting     structural metadata from Zarr archives. -   dlt Pipeline Integration: Leverages the <code>dlt</code> pipeline for robust data loading,     automatic schema inference, and optional strict validation, ensuring data quality     and consistency.</p>","path":["Integrations","DLT Loader"],"tags":[]},{"location":"integrations/dlt_loader/#consist.integrations.dlt_loader.ingest_artifact","level":2,"title":"<code>ingest_artifact(artifact, run_context, db_path, data_iterable=None, schema_model=None)</code>","text":"<p>Ingests artifact data into a DuckDB database using the <code>dlt</code> (Data Load Tool) library.</p> <p>This function supports various data sources (file paths, Pandas DataFrames, iterables of dicts) and automatically injects Consist's provenance system columns (<code>consist_run_id</code>, <code>consist_artifact_id</code>, <code>consist_year</code>, <code>consist_iteration</code>) into the data. It leverages <code>dlt</code> for robust schema handling, including inference and optional strict validation based on a provided <code>SQLModel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>artifact</code> <code>Artifact</code> <p>The Consist <code>Artifact</code> object representing the data to be ingested. Its driver information is used to determine the appropriate data handler.</p> required <code>run_context</code> <code>Run</code> <p>The <code>Run</code> object providing the context (ID, year, iteration) for provenance tracking.</p> required <code>db_path</code> <code>str</code> <p>The file system path to the DuckDB database where the data will be loaded.</p> required <code>data_iterable</code> <code>Optional[Union[Iterable[Any], str, DataFrame]]</code> <p>The data to ingest. Can be: - A file path (str) to a Parquet, CSV, HDF5, JSON, or Zarr file. - A Pandas DataFrame (will be treated as a single batch). - An iterable (e.g., list of dicts, generator) where each item represents a row. If <code>None</code>, it implies the data should be read directly from the <code>artifact</code>'s URI.</p> <code>None</code> <code>schema_model</code> <code>Optional[Type[SQLModel]]</code> <p>An optional <code>SQLModel</code> class that defines the expected schema for the data. If provided, <code>dlt</code> will use this for strict validation and schema management. If <code>None</code>, <code>dlt</code> will infer the schema.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LoadInfo, str]</code> <p>A tuple containing: - <code>dlt.LoadInfo</code>: An object providing detailed information about the data loading process. - <code>str</code>: The actual normalized table name where the data was loaded in the database.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no data is provided for ingestion, if the artifact driver is unsupported, or if a <code>schema_model</code> is provided but a schema contract violation occurs (e.g., new columns found in strict mode).</p> <code>ImportError</code> <p>If a required library for a specific driver (e.g., <code>pyarrow</code> for Parquet, <code>tables</code> for HDF5, <code>xarray</code>/<code>zarr</code> for Zarr) is not installed.</p>","path":["Integrations","DLT Loader"],"tags":[]}]}