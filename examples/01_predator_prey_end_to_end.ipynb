{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad025a8",
   "metadata": {},
   "source": [
    "# Consist End-to-End (Monte Carlo Predator–Prey Sweep)\n",
    "\n",
    "This notebook is a **happy-path** walkthrough of Consist in a scientific simulation workflow:\n",
    "\n",
    "1. Create a sweep registry (parameters + seeds)\n",
    "2. Run one simulation per Consist step (so each sweep member has provenance)\n",
    "3. Aggregate per-sim outputs into canonical analysis tables\n",
    "4. Ingest into DuckDB and persist schema profiles\n",
    "5. Export SQLModel classes from the persisted schema\n",
    "6. Import the generated module and do typed joins/analysis\n",
    "7. Re-run to demonstrate caching / minimal recomputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff392fd",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "This notebook writes outputs under `examples/runs/`, a DuckDB file under `examples/`, and generated SQLModel code under `examples/generated/`."
   ]
  },
  {
   "cell_type": "code",
   "id": "520e36e7",
   "metadata": {},
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sqlmodel import select\n",
    "import uuid\n",
    "\n",
    "import consist\n",
    "from consist import Tracker\n",
    "from consist.models import Artifact, Run\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    for candidate in (start, *start.parents):\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    raise RuntimeError(\"Could not locate repo root (missing pyproject.toml)\")\n",
    "\n",
    "\n",
    "REPO_ROOT = _find_repo_root(Path.cwd())\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from examples.pipeline_steps import (\n",
    "    aggregate_parquet,\n",
    "    build_sweep_registry,\n",
    "    make_run_id,\n",
    "    run_one_simulation_with_raw,\n",
    "    write_npz,\n",
    "    write_parquet,\n",
    ")\n",
    "from examples.synth_simulation import PredatorPreyConfig\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "EXAMPLES_DIR = REPO_ROOT / \"examples\"\n",
    "RUN_DIR = EXAMPLES_DIR / \"runs\" / \"predator_prey_demo\"\n",
    "SESSION_ID = uuid.uuid4().hex[:8]\n",
    "DB_PATH = EXAMPLES_DIR / f\"predator_prey_demo_{SESSION_ID}.duckdb\"\n",
    "GENERATED_DIR = EXAMPLES_DIR / \"generated\"\n",
    "GENERATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tracker = Tracker(run_dir=RUN_DIR, db_path=DB_PATH)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74456c37",
   "metadata": {},
   "source": [
    "**Caching note:** Consist’s cache signature includes a code identity (Git SHA). If your repo has uncommitted *tracked* changes, Consist uses a stable hash of the diff so cache hits still work during iteration (it ignores untracked run outputs/DB files)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f32d0f",
   "metadata": {},
   "source": [
    "## 1) Define the sweep\n",
    "\n",
    "We sweep a small grid of parameters (and optionally replicates per setting). Each row gets a stable `sim_id` and per-run seed."
   ]
  },
  {
   "cell_type": "code",
   "id": "c5723fb4",
   "metadata": {},
   "source": [
    "registry = build_sweep_registry(\n",
    "    # Tuned grid to highlight a transition region (coexistence ↔ extinction).\n",
    "    # (The stochastic dynamics can be sensitive; this set usually shows mixed regimes.)\n",
    "    prey_birth_rates=[0.60, 0.75],\n",
    "    predation_rates=[0.008, 0.011, 0.014],\n",
    "    predator_death_rates=[0.15, 0.20],\n",
    "    predator_birth_efficiency=0.20,\n",
    "    replicates_per_setting=10,\n",
    "    seed=7,\n",
    ")\n",
    "print(f\"sims={len(registry)} settings={registry['setting_id'].nunique()}\")\n",
    "registry.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1272f97",
   "metadata": {},
   "source": [
    "## 2) Cold run: one simulation per Consist step\n",
    "\n",
    "We create a scenario header run, then execute each `sim_id` as a child step. Each step writes:\n",
    "\n",
    "- a one-row metrics table (`sim_*_metrics.parquet`)\n",
    "- a sampled time-series table (`sim_*_series.parquet`)\n",
    "- optionally raw arrays (`sim_*_raw.npz`)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee63d695",
   "metadata": {},
   "source": [
    "from dataclasses import asdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _py_scalar(v):\n",
    "    return v.item() if hasattr(v, \"item\") else v\n",
    "\n",
    "\n",
    "base_config = PredatorPreyConfig(\n",
    "    steps=250,\n",
    "    sample_every=1,\n",
    "    prey_init=80,\n",
    "    predator_init=25,\n",
    "    predator_birth_efficiency=0.20,\n",
    ")\n",
    "\n",
    "scenario_id = \"predator_prey_sweep\"\n",
    "artifact_root = RUN_DIR / \"artifacts\" / scenario_id\n",
    "\n",
    "registry_path = artifact_root / \"sweep_registry.parquet\"\n",
    "\n",
    "sample_metrics_artifact: Artifact | None = None\n",
    "sample_series_artifact: Artifact | None = None\n",
    "registry_artifact: Artifact | None = None\n",
    "summary_artifact: Artifact | None = None\n",
    "\n",
    "cache_hits = 0\n",
    "cache_misses = 0\n",
    "\n",
    "with tracker.scenario(\n",
    "    scenario_id,\n",
    "    config={\n",
    "        \"base\": asdict(base_config),\n",
    "        \"sweep\": {\n",
    "            \"prey_birth_rates\": sorted(registry[\"prey_birth_rate\"].unique().tolist()),\n",
    "            \"predation_rates\": sorted(registry[\"predation_rate\"].unique().tolist()),\n",
    "            \"predator_death_rates\": sorted(registry[\"predator_death_rate\"].unique().tolist()),\n",
    "            \"replicates_per_setting\": int(registry[\"replicate_id\"].max() + 1),\n",
    "        },\n",
    "    },\n",
    "    tags=[\"examples\", \"simulation\", \"predator_prey\"],\n",
    ") as scenario:\n",
    "    # Step: write a sweep registry artifact (parameters + seeds)\n",
    "    with scenario.step(\n",
    "        name=\"registry\",\n",
    "        model=\"registry\",\n",
    "        cache_mode=\"overwrite\",\n",
    "        facet={\"rows\": int(len(registry)), \"settings\": int(registry[\"setting_id\"].nunique())},\n",
    "    ) as t:\n",
    "        write_parquet(registry, registry_path)\n",
    "        registry_artifact = t.log_artifact(\n",
    "            registry_path,\n",
    "            key=\"sweep_registry\",\n",
    "            direction=\"output\",\n",
    "            rows=int(len(registry)),\n",
    "        )\n",
    "\n",
    "    assert registry_artifact is not None\n",
    "\n",
    "    # Steps: run one simulation per sweep row\n",
    "    for row in tqdm(registry.to_dict(orient=\"records\"), total=len(registry)):\n",
    "        sim_cfg = {k: _py_scalar(v) for k, v in dict(row).items()}\n",
    "        sim_id = int(sim_cfg[\"sim_id\"])\n",
    "        setting_id = int(sim_cfg[\"setting_id\"])\n",
    "        replicate_id = int(sim_cfg[\"replicate_id\"])\n",
    "\n",
    "        sim_dir = artifact_root / f\"sim_{sim_id:04d}\"\n",
    "        metrics_path = sim_dir / \"metrics.parquet\"\n",
    "        series_path = sim_dir / \"series.parquet\"\n",
    "        raw_path = sim_dir / \"raw.npz\"\n",
    "\n",
    "        run_id = make_run_id(scenario_id=scenario_id, sim_id=sim_id)\n",
    "        with scenario.step(\n",
    "            name=f\"sim_{sim_id:04d}\",\n",
    "            run_id=run_id,\n",
    "            model=\"simulate\",\n",
    "            cache_mode=\"reuse\",\n",
    "            inputs=[registry_artifact],\n",
    "            config={\n",
    "                **{\n",
    "                    \"steps\": int(base_config.steps),\n",
    "                    \"dt\": float(base_config.dt),\n",
    "                    \"prey_init\": int(base_config.prey_init),\n",
    "                    \"predator_init\": int(base_config.predator_init),\n",
    "                },\n",
    "                **sim_cfg,\n",
    "            },\n",
    "            facet={\n",
    "                \"sim_id\": sim_id,\n",
    "                \"setting_id\": setting_id,\n",
    "                \"replicate_id\": replicate_id,\n",
    "                \"prey_birth_rate\": float(sim_cfg[\"prey_birth_rate\"]),\n",
    "                \"predation_rate\": float(sim_cfg[\"predation_rate\"]),\n",
    "                \"predator_death_rate\": float(sim_cfg[\"predator_death_rate\"]),\n",
    "            },\n",
    "            tags=[\"sim\"],\n",
    "        ) as t:\n",
    "            if t.is_cached:\n",
    "                cache_hits += 1\n",
    "                continue\n",
    "\n",
    "            cache_misses += 1\n",
    "            raw_arrays, series_df, metrics_df = run_one_simulation_with_raw(\n",
    "                base_config=base_config,\n",
    "                registry_row=sim_cfg,\n",
    "            )\n",
    "            write_parquet(metrics_df, metrics_path)\n",
    "            write_parquet(series_df, series_path)\n",
    "            write_npz(raw_arrays, raw_path)\n",
    "\n",
    "            metrics_art = t.log_artifact(\n",
    "                metrics_path, key=\"sim_metrics\", direction=\"output\", rows=int(len(metrics_df))\n",
    "            )\n",
    "            series_art = t.log_artifact(\n",
    "                series_path, key=\"sim_series\", direction=\"output\", rows=int(len(series_df))\n",
    "            )\n",
    "            t.log_artifact(raw_path, key=\"sim_raw\", direction=\"output\")\n",
    "\n",
    "            if sample_metrics_artifact is None:\n",
    "                sample_metrics_artifact = metrics_art\n",
    "            if sample_series_artifact is None:\n",
    "                sample_series_artifact = series_art\n",
    "\n",
    "    # Step: summarize the sweep into a single results table\n",
    "    with scenario.step(\n",
    "        name=\"summarize\",\n",
    "        model=\"summarize\",\n",
    "        cache_mode=\"overwrite\",\n",
    "        inputs=[registry_artifact],\n",
    "    ) as t:\n",
    "        metrics_all = aggregate_parquet(paths=sorted(artifact_root.glob(\"sim_*/metrics.parquet\")))\n",
    "        summary = (\n",
    "            metrics_all.groupby(\n",
    "                [\"prey_birth_rate\", \"predation_rate\", \"predator_death_rate\"], dropna=False\n",
    "            )\n",
    "            .agg(\n",
    "                sims=(\"sim_id\", \"count\"),\n",
    "                prey_extinct_rate=(\"prey_extinct\", \"mean\"),\n",
    "                predator_extinct_rate=(\"predator_extinct\", \"mean\"),\n",
    "                mean_prey_final=(\"prey_final\", \"mean\"),\n",
    "                mean_predator_final=(\"predator_final\", \"mean\"),\n",
    "                mean_prey_std=(\"prey_std\", \"mean\"),\n",
    "                mean_predator_std=(\"predator_std\", \"mean\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        summary[\"oscillation_score\"] = summary[\"mean_prey_std\"] + summary[\"mean_predator_std\"]\n",
    "        summary_path = artifact_root / \"sweep_summary.parquet\"\n",
    "        write_parquet(summary, summary_path)\n",
    "        summary_artifact = t.log_artifact(\n",
    "            summary_path,\n",
    "            key=\"sweep_summary\",\n",
    "            direction=\"output\",\n",
    "            rows=int(len(summary)),\n",
    "        )\n",
    "\n",
    "print(f\"cache_misses={cache_misses} cache_hits={cache_hits}\")\n",
    "assert registry_artifact is not None\n",
    "assert sample_metrics_artifact is not None\n",
    "assert sample_series_artifact is not None\n",
    "assert summary_artifact is not None\n",
    "\n",
    "pd.read_parquet(artifact_root / \"sweep_summary.parquet\").sort_values(\n",
    "    [\"oscillation_score\"], ascending=False\n",
    ").head(8)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "db864144",
   "metadata": {},
   "source": [
    "## 3) Sweep summary\n",
    "\n",
    "A single summary artifact makes it easy to see “regimes” (stable coexistence vs extinction) without looking at every trajectory."
   ]
  },
  {
   "cell_type": "code",
   "id": "81efc0a2",
   "metadata": {},
   "source": [
    "summary_df = pd.read_parquet(artifact_root / \"sweep_summary.parquet\")\n",
    "\n",
    "# Example: visualize extinction risk as a heatmap (fix death rate to one slice)\n",
    "death_rate = float(sorted(summary_df[\"predator_death_rate\"].unique())[0])\n",
    "slice_df = summary_df[summary_df[\"predator_death_rate\"] == death_rate]\n",
    "pivot = slice_df.pivot_table(\n",
    "    index=\"prey_birth_rate\",\n",
    "    columns=\"predation_rate\",\n",
    "    values=\"predator_extinct_rate\",\n",
    ")\n",
    "\n",
    "ax = sns.heatmap(pivot.sort_index(), annot=True, fmt=\".2f\", cmap=\"mako\", vmin=0, vmax=1)\n",
    "ax.set_title(f\"Predator extinction rate (predator_death_rate={death_rate})\")\n",
    "ax.set_xlabel(\"predation_rate\")\n",
    "ax.set_ylabel(\"prey_birth_rate\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "745d2478",
   "metadata": {},
   "source": [
    "## 4) Ingest + schema export\n",
    "\n",
    "Ingest `all_metrics` into DuckDB, verify schema tracking in the DB, then export SQLModel code into `examples/generated/`."
   ]
  },
  {
   "cell_type": "code",
   "id": "57fcf003",
   "metadata": {},
   "source": [
    "# Ingest one representative sim output to persist a schema profile for export.\n",
    "# (We do NOT ingest every sim; we keep most results “cold” and query them via views.)\n",
    "tracker.ingest(registry_artifact)\n",
    "tracker.ingest(sample_metrics_artifact)\n",
    "tracker.ingest(sample_series_artifact)\n",
    "\n",
    "metrics_stub_path = GENERATED_DIR / \"predator_prey_metrics.py\"\n",
    "registry_stub_path = GENERATED_DIR / \"predator_prey_registry.py\"\n",
    "series_stub_path = GENERATED_DIR / \"predator_prey_series.py\"\n",
    "\n",
    "tracker.export_schema_sqlmodel(\n",
    "    artifact_id=sample_metrics_artifact.id,\n",
    "    out_path=metrics_stub_path,\n",
    "    class_name=\"PredatorPreyMetrics\",\n",
    "    table_name=\"sim_metrics\",\n",
    ")\n",
    "tracker.export_schema_sqlmodel(\n",
    "    artifact_id=registry_artifact.id,\n",
    "    out_path=registry_stub_path,\n",
    "    class_name=\"PredatorPreySweepRegistry\",\n",
    "    table_name=\"sweep_registry\",\n",
    ")\n",
    "tracker.export_schema_sqlmodel(\n",
    "    artifact_id=sample_series_artifact.id,\n",
    "    out_path=series_stub_path,\n",
    "    class_name=\"PredatorPreySeries\",\n",
    "    table_name=\"sim_series\",\n",
    ")\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\"-\", metrics_stub_path)\n",
    "print(\"-\", registry_stub_path)\n",
    "print(\"-\", series_stub_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20f70df3",
   "metadata": {},
   "source": [
    "## 5) Warm analysis (typed)\n",
    "\n",
    "Import the generated SQLModel class and run typed joins/analysis (e.g., extinction rates by parameter setting, representative trajectories)."
   ]
  },
  {
   "cell_type": "code",
   "id": "40b58118",
   "metadata": {},
   "source": [
    "import importlib\n",
    "\n",
    "# Compare: generated stubs vs checked-in “contract” models.\n",
    "# - Generated stubs are conservative and meant for editing.\n",
    "# - Checked-in models below demonstrate reviewed PK/FK/index choices.\n",
    "predator_prey_metrics = importlib.import_module(\"examples.generated.predator_prey_metrics\")\n",
    "predator_prey_registry = importlib.import_module(\"examples.generated.predator_prey_registry\")\n",
    "predator_prey_series = importlib.import_module(\"examples.generated.predator_prey_series\")\n",
    "\n",
    "importlib.reload(predator_prey_metrics)\n",
    "importlib.reload(predator_prey_registry)\n",
    "importlib.reload(predator_prey_series)\n",
    "\n",
    "GeneratedPredatorPreyMetrics = predator_prey_metrics.PredatorPreyMetrics\n",
    "GeneratedPredatorPreySweepRegistry = predator_prey_registry.PredatorPreySweepRegistry\n",
    "GeneratedPredatorPreySeries = predator_prey_series.PredatorPreySeries\n",
    "\n",
    "from examples.checked_models import (\n",
    "    PredatorPreyMetricsChecked,\n",
    "    PredatorPreySeriesChecked,\n",
    "    PredatorPreySweepRegistryChecked,\n",
    ")\n",
    "\n",
    "print(\"Generated (example):\", GeneratedPredatorPreyMetrics.__name__, \"abstract=\", getattr(GeneratedPredatorPreyMetrics, \"__abstract__\", False))\n",
    "print(\"Checked-in:\", PredatorPreyMetricsChecked.__name__)\n",
    "\n",
    "# Register typed hybrid views.\n",
    "# These views unify:\n",
    "# - “hot” rows (ingested tables)\n",
    "# - “cold” rows (Parquet artifacts across many runs)\n",
    "tracker.views.register(PredatorPreyMetricsChecked, key=\"sim_metrics\")\n",
    "tracker.views.register(PredatorPreySweepRegistryChecked, key=\"sweep_registry\")\n",
    "tracker.views.register(PredatorPreySeriesChecked, key=\"sim_series\")\n",
    "\n",
    "MetricsView = tracker.views.PredatorPreyMetricsChecked\n",
    "RegistryView = tracker.views.PredatorPreySweepRegistryChecked\n",
    "SeriesView = tracker.views.PredatorPreySeriesChecked\n",
    "\n",
    "with consist.db_session(tracker=tracker) as session:\n",
    "    # Typed join: per-sim metrics + sweep registry\n",
    "    stmt = (\n",
    "        select(MetricsView, RegistryView)\n",
    "        .join(RegistryView, MetricsView.sim_id == RegistryView.sim_id)\n",
    "        .where(MetricsView.consist_scenario_id == scenario_id)\n",
    "    )\n",
    "    joined = session.exec(stmt).all()\n",
    "\n",
    "metrics_rows = [m.model_dump() for m, _ in joined]\n",
    "registry_rows = [r.model_dump() for _, r in joined]\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "registry_df = pd.DataFrame(registry_rows)\n",
    "\n",
    "display(metrics_df.head(3))\n",
    "display(registry_df.head(3))\n",
    "\n",
    "# A small analysis: extinction rate by parameter setting (from the typed view)\n",
    "ext_summary = (\n",
    "    metrics_df.groupby([\"prey_birth_rate\", \"predation_rate\", \"predator_death_rate\"], dropna=False)\n",
    "    .agg(\n",
    "        sims=(\"sim_id\", \"count\"),\n",
    "        predator_extinct_rate=(\"predator_extinct\", \"mean\"),\n",
    "        prey_extinct_rate=(\"prey_extinct\", \"mean\"),\n",
    "        mean_oscillation=(\"predator_std\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "ext_summary.sort_values([\"predator_extinct_rate\", \"mean_oscillation\"], ascending=[False, False]).head(8)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcf13c08",
   "metadata": {},
   "source": [
    "## 6) Caching + recomputation demo\n",
    "\n",
    "We re-run the sweep under a new scenario id with `cache_mode=\"reuse\"`.\n",
    "\n",
    "- Most sims should be cache hits.\n",
    "- We intentionally delete one cached output file to force exactly one recomputation.\n",
    "- On cache hits, we also request **materialization** of cached outputs into a new folder."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5093c80",
   "metadata": {},
   "source": [
    "warm_scenario_id = f\"{scenario_id}_warm\"\n",
    "warm_root = RUN_DIR / \"artifacts\" / warm_scenario_id\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Force one cache miss by deleting a cached output file from the original run.\n",
    "# We delete `raw.npz` (not ingested) so Consist’s cache validation can detect it.\n",
    "victim_sim_id = int(registry[\"sim_id\"].max())\n",
    "victim_raw = artifact_root / f\"sim_{victim_sim_id:04d}\" / \"raw.npz\"\n",
    "if victim_raw.exists():\n",
    "    victim_raw.unlink()\n",
    "    print(\"Deleted cached output to force recomputation:\", victim_raw)\n",
    "else:\n",
    "    print(\"Victim file already missing:\", victim_raw)\n",
    "\n",
    "DEMO_N = 12\n",
    "MATERIALIZE_CACHED_OUTPUTS = False\n",
    "\n",
    "warm_hits = 0\n",
    "warm_misses = 0\n",
    "\n",
    "warm_registry = registry.sort_values(\"sim_id\").head(DEMO_N)\n",
    "print(f\"warm demo sims={len(warm_registry)} of total={len(registry)}\")\n",
    "\n",
    "with tracker.scenario(\n",
    "    warm_scenario_id,\n",
    "    config={\"reuse_from\": scenario_id},\n",
    "    tags=[\"examples\", \"cache_demo\"],\n",
    ") as scenario:\n",
    "    assert registry_artifact is not None\n",
    "    for row in tqdm(warm_registry.to_dict(orient=\"records\"), total=len(warm_registry)):\n",
    "        sim_cfg = {k: _py_scalar(v) for k, v in dict(row).items()}\n",
    "        sim_id = int(sim_cfg[\"sim_id\"])\n",
    "        \n",
    "        sim_dir = warm_root / f\"sim_{sim_id:04d}\"\n",
    "        metrics_path = sim_dir / \"metrics.parquet\"\n",
    "        series_path = sim_dir / \"series.parquet\"\n",
    "        raw_path = sim_dir / \"raw.npz\"\n",
    "\n",
    "        run_id = make_run_id(scenario_id=warm_scenario_id, sim_id=sim_id)\n",
    "\n",
    "        step_kwargs = {}\n",
    "        if MATERIALIZE_CACHED_OUTPUTS:\n",
    "            step_kwargs = {\n",
    "                \"materialize_cached_outputs\": \"requested\",\n",
    "                \"materialize_cached_output_paths\": {\n",
    "                    \"sim_metrics\": metrics_path,\n",
    "                    \"sim_series\": series_path,\n",
    "                    \"sim_raw\": raw_path,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        with scenario.step(\n",
    "            name=f\"sim_{sim_id:04d}\",\n",
    "            run_id=run_id,\n",
    "            model=\"simulate\",\n",
    "            cache_mode=\"reuse\",\n",
    "            inputs=[registry_artifact],\n",
    "            config={\n",
    "                **{\n",
    "                    \"steps\": int(base_config.steps),\n",
    "                    \"dt\": float(base_config.dt),\n",
    "                    \"prey_init\": int(base_config.prey_init),\n",
    "                    \"predator_init\": int(base_config.predator_init),\n",
    "                },\n",
    "                **sim_cfg,\n",
    "            },\n",
    "            **step_kwargs,\n",
    "        ) as t:\n",
    "            if t.is_cached:\n",
    "                warm_hits += 1\n",
    "                continue\n",
    "\n",
    "            warm_misses += 1\n",
    "            raw_arrays, series_df, metrics_df = run_one_simulation_with_raw(\n",
    "                base_config=base_config,\n",
    "                registry_row=sim_cfg,\n",
    "            )\n",
    "            write_parquet(metrics_df, metrics_path)\n",
    "            write_parquet(series_df, series_path)\n",
    "            write_npz(raw_arrays, raw_path)\n",
    "            t.log_artifact(metrics_path, key=\"sim_metrics\", direction=\"output\")\n",
    "            t.log_artifact(series_path, key=\"sim_series\", direction=\"output\")\n",
    "            t.log_artifact(raw_path, key=\"sim_raw\", direction=\"output\")\n",
    "\n",
    "print(f\"warm cache hits={warm_hits} misses={warm_misses}\")\n",
    "\n",
    "# Inspect one cached run’s materialized outputs\n",
    "with consist.db_session(tracker=tracker) as session:\n",
    "    warm_runs = (\n",
    "        session.exec(\n",
    "            select(Run)\n",
    "            .where(Run.parent_run_id == warm_scenario_id)\n",
    "            .where(Run.model_name == \"simulate\")\n",
    "        )\n",
    "        .all()\n",
    "    )\n",
    "    cached = [r for r in warm_runs if (r.meta or {}).get(\"cache_hit\")]\n",
    "    print(f\"warm simulate runs={len(warm_runs)} cached={len(cached)}\")\n",
    "    if cached:\n",
    "        print(\"example cached run meta keys:\", sorted((cached[0].meta or {}).keys()))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eefd9d67-2c78-4a15-afbb-d4a8b6486fdb",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
